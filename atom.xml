<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>CSRJTAN</title>
  <subtitle>Keep Moving</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://csrjtan.github.io/"/>
  <updated>2017-04-10T03:08:31.000Z</updated>
  <id>https://csrjtan.github.io/</id>
  
  <author>
    <name>CsrjTan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>VisualComputing_2</title>
    <link href="https://csrjtan.github.io/2017/04/08/VisualComputing-2/"/>
    <id>https://csrjtan.github.io/2017/04/08/VisualComputing-2/</id>
    <published>2017-04-08T07:13:57.000Z</published>
    <updated>2017-04-10T03:08:31.000Z</updated>
    
    <content type="html">&lt;p&gt;上一节讲了CV的介绍和Sparse Representation的内容，包括CV的概念、应用和难点；Sparse Representation的formulation, method以及步骤。当然还有为何Sparse Representation can work. 这一节讲一下Dictionary Learning和Representative works&lt;/p&gt;
&lt;h3 id=&quot;Dictionary-Learning&quot;&gt;&lt;a href=&quot;#Dictionary-Learning&quot; class=&quot;headerlink&quot; title=&quot;Dictionary Learning&quot;&gt;&lt;/a&gt;Dictionary Learning&lt;/h3&gt;&lt;h4 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h4&gt;&lt;p&gt;在稀疏编码之前，需要学习一组过完备的字典，从而使得编码向量是稀疏的。以下分为两种字典，Analytical and Learn;&lt;br&gt;Analytical包括DCT bases, Wavelets, Curvelets…; Learn dictionaries from natural images: K-SVD, Coordinate descent, Online dictionary learning;&lt;/p&gt;
&lt;p&gt;为什么需要字典学习？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Over-complete learned dictionary often work better than analytically&lt;/li&gt;
&lt;li&gt;More adaptive to specific task/data&lt;/li&gt;
&lt;li&gt;Less strict constraints on mathematical properties of bases&lt;/li&gt;
&lt;li&gt;More flexible to model data&lt;/li&gt;
&lt;li&gt;Tend to produce sparser solution&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;L0：K-SVD&quot;&gt;&lt;a href=&quot;#L0：K-SVD&quot; class=&quot;headerlink&quot; title=&quot;L0：K-SVD&quot;&gt;&lt;/a&gt;L0：K-SVD&lt;/h4&gt;&lt;p&gt;对于L0稀疏的字典学习，我们可以用K-SVD方法近似求解，其中可以看成是K-MEANS的一种扩展&lt;br&gt;字典学习的问题可以Modeling为：&lt;br&gt;$$min_{D,A}||Y-DA||_F^2 \ \ s.t.\ \ ||a_i||_0 &amp;lt;= T_0$$ 其中i为任意正数，$T_0$为稀疏值&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/vc_20170408_01.png&quot; alt=&quot;K-SVD&quot;&gt;&lt;br&gt;如图，对于字典学习：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先是稀疏编码，可以用Matching Pursuit来优化求解；然后用K-SVD方法更新字典。 &lt;/li&gt;
&lt;li&gt;然后将DA进行K次分片叠加得到$DA=\sum_{i=1}^K d_i a_i^T$, 这里便是一个可用词典；剥离第K条，寻找新的d,x来更新该条目&lt;/li&gt;
&lt;li&gt;最后，只抽取非零的a组成新的矩阵$\Omega$作为系数矩阵，对误差能量矩阵作SVD分解，d取U的第一行，x取$\sum V^T$的乘积第一列&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt;K-SVD的思想：K次分片，使得最后学得的字典over-complete; 选用第K个条目更新，每次只更新一个字典atom(one column in fat matrix); 对剥离后的‘空洞’做K-SVD， $E_k = U \sum V^T$, 新的d,a则取里面能量最大的元素， 这是对误差’空洞’的最佳逼近；只抽取非零系数组成新矩阵更新，有助于保持原来字典的稀疏性；&lt;/p&gt;
&lt;p&gt;对于L1字典，可以对D和A交替学习：当更新D时，这是Quadratic Programming; 当更新A时，这是LASSO Optimization (ADMM); &lt;/p&gt;
&lt;h4 id=&quot;Representative-Work&quot;&gt;&lt;a href=&quot;#Representative-Work&quot; class=&quot;headerlink&quot; title=&quot;Representative Work&quot;&gt;&lt;/a&gt;Representative Work&lt;/h4&gt;&lt;p&gt;Online learning: 考虑新来的样本，直接在原来基础上更新词典的策略以及收敛性&lt;/p&gt;
&lt;p&gt;Multi-scale Dictionary learning: 由于complexity increases exponentially with signal dimension,所以一般用较小的patch size; 而multi-scale可以自适应地融合不同scale字典编码&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/vc_20170408_02.png&quot; alt=&quot;Multi-Scale&quot;&gt;&lt;/p&gt;
&lt;p&gt;Double Sparsity: 可以针对高层次稀疏特征或者large patch再进行一次dictionary learning, 基于稀疏编码或着高维编码的再一次稀疏表示；&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/vc_20170408_03.png&quot; alt=&quot;Double Sparsity&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;Restoration-Methods&quot;&gt;&lt;a href=&quot;#Restoration-Methods&quot; class=&quot;headerlink&quot; title=&quot;Restoration Methods&quot;&gt;&lt;/a&gt;Restoration Methods&lt;/h3&gt;&lt;p&gt;Filtering-based methods: Isotropic method, Anisotropic method&lt;br&gt;Transformation methods: Motivation, find new representation where signal and noise can be better separated; Wavelet transform&lt;/p&gt;
&lt;h4 id=&quot;K-SVD-denoising&quot;&gt;&lt;a href=&quot;#K-SVD-denoising&quot; class=&quot;headerlink&quot; title=&quot;K-SVD denoising&quot;&gt;&lt;/a&gt;K-SVD denoising&lt;/h4&gt;&lt;p&gt;Basic Idea: 1.train over-complete dictionary 2.adopt trained dictionary to denoise patch in noisy image 3.Utilize the patch to reconstruct&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/vc_20170408_04.png&quot; alt=&quot;Modeling&quot;&gt;&lt;br&gt;Limitations: 1.Solving sparse coding not effective enough 2.L0 is not good choice &lt;/p&gt;
&lt;h4 id=&quot;BM3D&quot;&gt;&lt;a href=&quot;#BM3D&quot; class=&quot;headerlink&quot; title=&quot;BM3D&quot;&gt;&lt;/a&gt;BM3D&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/vc_20170408_05.png&quot; alt=&quot;BM3D&quot;&gt;&lt;br&gt;BM3D denoising算是业内最为经典的去噪算法了，其中结合了Nonlocal self-similarity和sparsity两个最重要的priors，效果非常不错，速度一般&lt;/p&gt;
&lt;p&gt;步骤：首先通过non-local matching找到一组图片块；组成tensor进行维纳滤波，之后进行阈值抑制（这里相当于稀疏去噪）；最后对新的tensor结合原来的tensor再重复做维纳滤波和阈值抑制；得到去噪patches reconstruct到图像即可&lt;/p&gt;
&lt;p&gt;优缺点：1.有效挖掘了nonlocal similarity和sparsity 2.在DWT（小波)做协同滤波并不能描述复杂的图像结构&lt;/p&gt;
&lt;h4 id=&quot;LSSC&quot;&gt;&lt;a href=&quot;#LSSC&quot; class=&quot;headerlink&quot; title=&quot;LSSC&quot;&gt;&lt;/a&gt;LSSC&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;Group Sparsity&lt;/strong&gt;&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/vc_20170408_06.png&quot; alt=&quot;Group SPARSITY&quot;&gt;&lt;br&gt;与普通的L1 sparsity不同，Marial提出系数矩阵满足group sparsity的L1，2范数；使得同样的patch，在字典下应该具有统一的稀疏编码，保持元组具有相同稀疏的特性；仔细看12范数的表达形式，j是行，i是列，使得系数尽可能在同一行；&lt;/p&gt;
&lt;p&gt;整个流程和BM3D相似，只是在协同滤波和阈值抑制上，改成用group sparsity的字典学习和稀疏编码去噪&lt;/p&gt;
&lt;p&gt;Adaptive Sparse Domain Selection： 由于大的词典使得稀疏编码过程非常耗时，而大的词典对于描述图像局部结构又是很有必要；这个方法提出从大字典中选择一个子集可以提速&lt;/p&gt;
&lt;p&gt;Piece-wise Linear Estimation (PLE), Motivations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sparse representation assumes &lt;strong&gt;Laplacian&lt;/strong&gt; prior on coefficients, lead to nonlinear sparse coding estimator&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;Mixture of Gaussians&lt;/strong&gt; to approximate Laplacian&lt;/li&gt;
&lt;li&gt;Select one appropriate Gaussian Prior to reconstruct&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;Coupled-Dictionary-Learning&quot;&gt;&lt;a href=&quot;#Coupled-Dictionary-Learning&quot; class=&quot;headerlink&quot; title=&quot;Coupled Dictionary Learning&quot;&gt;&lt;/a&gt;Coupled Dictionary Learning&lt;/h4&gt;&lt;p&gt;Motivations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Used coupled dictionary to model the relationship between degraded image and its corresponding images&lt;/li&gt;
&lt;li&gt;Build the corresponding in sparse domain(same code but different dictionary)&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/vc_20170408_07.png&quot; alt=&quot;SRSR&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Semi-coupled Dictionary Learning: flexible the relationship between two dictionary, the sparse code with a pre-learned mapping&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;上一节讲了CV的介绍和Sparse Representation的内容，包括CV的概念、应用和难点；Sparse Representation的formulation, method以及步骤。当然还有为何Sparse Representation can work. 这一节
    
    </summary>
    
      <category term="Read" scheme="https://csrjtan.github.io/categories/Read/"/>
    
    
      <category term="VisualComputing" scheme="https://csrjtan.github.io/tags/VisualComputing/"/>
    
  </entry>
  
  <entry>
    <title>Very Deep Super Resolution</title>
    <link href="https://csrjtan.github.io/2017/04/07/VDSR/"/>
    <id>https://csrjtan.github.io/2017/04/07/VDSR/</id>
    <published>2017-04-07T15:15:59.000Z</published>
    <updated>2017-04-07T15:43:16.000Z</updated>
    
    <content type="html">&lt;p&gt;这一篇是CVPR16 Kim的VDSR，通过VERY DEEP的简单模型，又快又好地解决了SR问题，成为暂时这个问题上的标杆模型。&lt;/p&gt;
&lt;h4 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h4&gt;&lt;p&gt;Our final model uses 20 weight layers. By cascading small filters many times in a deep network structure, contextual infor- mation over large image regions is exploited in an efficient way. With very deep networks, however, convergence speed becomes a critical issue during training. We propose a sim- ple yet effective training procedure. We learn residuals only and use extremely high learning rates (104 times higher than SRCNN [6]) enabled by adjustable gradient clipping.&lt;/p&gt;
&lt;h4 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h4&gt;&lt;p&gt;Single image super-resolution(SISR):upsampling方法，而后neighbor embedding,如今用CNN； SRCNN的limitation: 1.relies on context of small image regions; 2.only works for single scale； VDSR的主要优点有：1.通过small size kernel but very deep, to obtain a large context(receptive region) 2.Convergence very fast by residual-learning and BN high learning rate 3.Multi-Scale Factor,把多个scale的SR融合进一个网络模型 &lt;/p&gt;
&lt;h4 id=&quot;Methodology&quot;&gt;&lt;a href=&quot;#Methodology&quot; class=&quot;headerlink&quot; title=&quot;Methodology&quot;&gt;&lt;/a&gt;Methodology&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/paper_20170407_01.png&quot; alt=&quot;ARCHITECTURE&quot;&gt;&lt;br&gt;20层CONV+BN+RELU，L2 LOSS, HIGH LEARN RATE WITH RESIDUAL LEARNING AND ADJUSTABLE WEIGHT CLIPPING.&lt;/p&gt;
&lt;h4 id=&quot;Experiment&quot;&gt;&lt;a href=&quot;#Experiment&quot; class=&quot;headerlink&quot; title=&quot;Experiment&quot;&gt;&lt;/a&gt;Experiment&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;THE DEEPER THE BETTER ON PSNR/SSIM&lt;/li&gt;
&lt;li&gt;RESIDUAL LEARNING WORKS&lt;/li&gt;
&lt;li&gt;MULTI-SCALE MODEL BETTER THAN SINGLE SCALE ON LARGE SCALE&lt;/li&gt;
&lt;/ol&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;这一篇是CVPR16 Kim的VDSR，通过VERY DEEP的简单模型，又快又好地解决了SR问题，成为暂时这个问题上的标杆模型。&lt;/p&gt;
&lt;h4 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abs
    
    </summary>
    
      <category term="Tech" scheme="https://csrjtan.github.io/categories/Tech/"/>
    
    
      <category term="paper" scheme="https://csrjtan.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>VisualComputing_1</title>
    <link href="https://csrjtan.github.io/2017/04/07/VisualComputing-1/"/>
    <id>https://csrjtan.github.io/2017/04/07/VisualComputing-1/</id>
    <published>2017-04-07T13:30:44.000Z</published>
    <updated>2017-04-07T15:43:19.000Z</updated>
    
    <content type="html">&lt;p&gt;老板的CV课程，在期末前做一下相关笔记总结&lt;/p&gt;
&lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;h4 id=&quot;What-is-vision&quot;&gt;&lt;a href=&quot;#What-is-vision&quot; class=&quot;headerlink&quot; title=&quot;What is vision?&quot;&gt;&lt;/a&gt;What is vision?&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Perceive an integration of image data and prior knowledge in brain&lt;/li&gt;
&lt;li&gt;A field acquiring, processing, analyzing and understanding visual data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Computer Vision &amp;amp; Human Vision?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ill-posed problems&lt;/li&gt;
&lt;li&gt;mathematical models&lt;/li&gt;
&lt;li&gt;discrete vs. continuous&lt;/li&gt;
&lt;li&gt;local vs. global optimization&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;What-kinds-of-Topics&quot;&gt;&lt;a href=&quot;#What-kinds-of-Topics&quot; class=&quot;headerlink&quot; title=&quot;What kinds of Topics?&quot;&gt;&lt;/a&gt;What kinds of Topics?&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/vc_20170407_01.png&quot; alt=&quot;相关学科&quot;&gt;&lt;br&gt;Low Level: Image Denoising, Deblurring, Super-Resolution, photo-sketch synthesis, texture synthesis, optical flow, image matching&lt;/p&gt;
&lt;p&gt;Middle Level: image segmentation, motion capture, visual tracking, 3D reconstruction&lt;/p&gt;
&lt;p&gt;High Level: object detection, image understanding, video understanding&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/vc_20170407_02.png&quot; alt=&quot;具体应用问题&quot;&gt;&lt;/p&gt;
&lt;p&gt;Related Problems: medical imaging, optical character recognition (OCR), face detection, smile detection, vision-based biometrics, shape capture, automatic driving&lt;/p&gt;
&lt;h4 id=&quot;Why-image-restoration-challenging&quot;&gt;&lt;a href=&quot;#Why-image-restoration-challenging&quot; class=&quot;headerlink&quot; title=&quot;Why image restoration challenging?&quot;&gt;&lt;/a&gt;Why image restoration challenging?&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Real noise much more complex than additive white Gaussian&lt;/li&gt;
&lt;li&gt;Blur is non-uniform and complex to accurately estimate&lt;/li&gt;
&lt;li&gt;Space of image local structures is huge, inverse problem highly ill-posed&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Sparse-Representation-and-Dictionary-Learning-on-Restoration&quot;&gt;&lt;a href=&quot;#Sparse-Representation-and-Dictionary-Learning-on-Restoration&quot; class=&quot;headerlink&quot; title=&quot;Sparse Representation and Dictionary Learning on Restoration&quot;&gt;&lt;/a&gt;Sparse Representation and Dictionary Learning on Restoration&lt;/h3&gt;&lt;p&gt;Linear system $Ax=b$, if A full rank, $x = A^{-1}b$; if tall matrix(over-determined) than approximate solution by $minimize||Ax-b||_2^2$; if fat matrix(underdetermined), no solution in general and some constraint should be imposed&lt;/p&gt;
&lt;p&gt;假设estimation与observer的最小距离是L0,L1,L2或其他：L0，非凸优化； L1，tightest convex relaxation of L0, 稀疏解; L2有闭合Dense解。具体到一个优化问题的等高线逼近时，各个NORM BALL的图形如下。&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/vc_20170407_03.png&quot; alt=&quot;各个Norm Ball&quot;&gt;&lt;br&gt;尽管L1能逼近L0，但有时候L1也会出现非稀疏解，数学上已经证明，满足RIP性质的话，用L1近似L0能确保得到稀疏解。RIP又称有限等距性质,直观解释为从A矩阵中的部分列向量与任意向量x的乘积结果收敛在一个环形邻域，如下图&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/vc_20170407_04.png&quot; alt=&quot;RIP&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/vc_20170407_05.png&quot; alt=&quot;直观解释&quot;&gt;&lt;/p&gt;
&lt;p&gt;图像复原问题&lt;/p&gt;
&lt;h4 id=&quot;Modeling&quot;&gt;&lt;a href=&quot;#Modeling&quot; class=&quot;headerlink&quot; title=&quot;Modeling&quot;&gt;&lt;/a&gt;Modeling&lt;/h4&gt;&lt;p&gt;$y=Hx+v$, H:observation matrix, v:noise&lt;br&gt;Keys to solve ill-posed problems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Modeling the degradation process&lt;/li&gt;
&lt;li&gt;Good Prior knowledge about the clean image&lt;/li&gt;
&lt;li&gt;Good objective function for minimization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中H在denoise是identity matrix; deblurring为blurring matrix; supperresolution是compound matrix of blurring and downsampling matrix; Inpainting是indication matrix of damaged pixels; &lt;/p&gt;
&lt;h4 id=&quot;Methodology&quot;&gt;&lt;a href=&quot;#Methodology&quot; class=&quot;headerlink&quot; title=&quot;Methodology&quot;&gt;&lt;/a&gt;Methodology&lt;/h4&gt;&lt;p&gt;Filter based methods: Gaussian low-pass, PDE-based anisotropic diffusion, Bilateral filtering, Nonlocal means filtering; (local-&amp;gt;non local performance improve greatly)&lt;/p&gt;
&lt;p&gt;Transform based methods: Fourier(‘Global, Orthogonal’), Wavelet(‘local, small’), Ridgelet(‘more redundant’), Dictionary Learning(‘over-complete’)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Represent x over dictionary D, enforcing the new vector be sparse(robust)&lt;/li&gt;
&lt;li&gt;objective model $min_\alpha ||HD\alpha-y||_2^2+\lambda ||\alpha||_1$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;The-basic-procedure&quot;&gt;&lt;a href=&quot;#The-basic-procedure&quot; class=&quot;headerlink&quot; title=&quot;The basic procedure&quot;&gt;&lt;/a&gt;The basic procedure&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;Partition degraded image into overlapped patches(8*8)&lt;/li&gt;
&lt;li&gt;For each patch, solve the nonlinear L1-norm sparse coding problem:&lt;br&gt;$\hat{\alpha} = argmin_\alpha ||HD\alpha-y||_2^2+\lambda||\alpha||_1$&lt;/li&gt;
&lt;li&gt;Reconstruct each patch by $\hat{x}=D\hat{\alpha}$&lt;/li&gt;
&lt;li&gt;put the reconstructed patch back and average the overlapped pixels&lt;/li&gt;
&lt;li&gt;In practice, the 1~4 can be iterated for several rounds&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;why sparse?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Neuronscience&lt;/li&gt;
&lt;li&gt;Bayersian &lt;/li&gt;
&lt;li&gt;Compressive Sensing&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;How-to-solve&quot;&gt;&lt;a href=&quot;#How-to-solve&quot; class=&quot;headerlink&quot; title=&quot;How to solve?&quot;&gt;&lt;/a&gt;How to solve?&lt;/h4&gt;&lt;p&gt;L0: Greddy search(Matching pursuit, Orthogonal matching pursuit)&lt;br&gt;L1: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear programming&lt;/li&gt;
&lt;li&gt;Iteratively reweighted least squares&lt;/li&gt;
&lt;li&gt;Proximal gradient descent&lt;/li&gt;
&lt;li&gt;Augmented Lagrangian methods(Alternating Direction Method of Multipliers, ADMM)&lt;/li&gt;
&lt;/ul&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;老板的CV课程，在期末前做一下相关笔记总结&lt;/p&gt;
&lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;h4 
    
    </summary>
    
      <category term="Read" scheme="https://csrjtan.github.io/categories/Read/"/>
    
    
      <category term="VisualComputing" scheme="https://csrjtan.github.io/tags/VisualComputing/"/>
    
  </entry>
  
  <entry>
    <title>CDM_Review</title>
    <link href="https://csrjtan.github.io/2017/03/29/CDM-Review/"/>
    <id>https://csrjtan.github.io/2017/03/29/CDM-Review/</id>
    <published>2017-03-29T01:50:05.000Z</published>
    <updated>2017-03-29T04:44:40.000Z</updated>
    
    <content type="html">&lt;p&gt;今天来总结一下Color Demosaicking（CDM）里面的重要论文和方法。希望能囊括AP, AHD, SA, LDI-NAT, DLMMSE, LSSC, GBTF, RI等方法&lt;/p&gt;
&lt;h4 id=&quot;比较旧的：AP-AHD-SA等&quot;&gt;&lt;a href=&quot;#比较旧的：AP-AHD-SA等&quot; class=&quot;headerlink&quot; title=&quot;比较旧的：AP, AHD, SA等&quot;&gt;&lt;/a&gt;比较旧的：AP, AHD, SA等&lt;/h4&gt;&lt;p&gt;AP给出两个图像规律统计假设：1.自然图像在R,G,B通道间有较大的相关性（inter-color correlations) 2.G通道的采样率比R,B高一倍。则G通道的细节信息更丰富。它的方法包括两步：1.用了高低通滤波，然后构建inter-color恢复像素的公式 2.将结果投影到observed和label constrants sets上，进行fine-tune.（这些后来都有更好的方法）&lt;br&gt;Comment: AP这个方法效果已经不佳，但是它统计出来的inter-color correlation很重要&lt;/p&gt;
&lt;p&gt;SA让G和RB通道的像素估计进行一个交替循环地求解，类似于近似逼近的思想。迭代式求解涉及三个问题：1.从何开始（初始化方法） 2.该算法收敛吗（论文用AP的constrains set论证) 3.什么时候结束(更新不再提高，或到一个较少值)&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/SA_1.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;Comment: SA这个方法主要说明了一个Iterative求解CDM问题的可行性，但iterative问题需要说明清楚上述的三个问题。&lt;/p&gt;
&lt;p&gt;AHD这篇文章2005年提出了homogeneity概念，有效结合梯度较小的变化方向进行有效的像素估计，最后结合adaptive中值滤波的方法去除一下artifacts。效果比之前好，而且还快。&lt;/p&gt;
&lt;h4 id=&quot;接下来说一下速度比较慢，效果比较好的：DLMMSE-LDI-NAT-LSSC-Dict-Learning&quot;&gt;&lt;a href=&quot;#接下来说一下速度比较慢，效果比较好的：DLMMSE-LDI-NAT-LSSC-Dict-Learning&quot; class=&quot;headerlink&quot; title=&quot;接下来说一下速度比较慢，效果比较好的：DLMMSE, LDI-NAT, LSSC, Dict Learning&quot;&gt;&lt;/a&gt;接下来说一下速度比较慢，效果比较好的：DLMMSE, LDI-NAT, LSSC, Dict Learning&lt;/h4&gt;&lt;p&gt;DLMMSE: 基于G和R/B通道的primal difference signals是low pass的，提出了基于directional minimum mean square-error estimation的方法，这里用到了horizon和vertical两个方向。先恢复G通道，然后用G恢复R/B通道。&lt;br&gt;论文首先给出统计表，说明GR和GB的相关性比RB的强，所以用G-R和G-B作为通道相关性的信号，然后估计真实值与观察值的误差。为了方便求解，而且假设两个信号demosaick noise是i.i.d gaussian，则LMMSE的公式可以简化为&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/DLMMSE_1.png&quot; alt=&quot;&quot;&gt; 其中x是观察值，mu_x是x的均值，sigma为方差，y为估计值。&lt;/p&gt;
&lt;p&gt;LDI-NAT的方法在之前的博文说过，这里总结一下，相比DLMMSE，LDI-NAT用LDI做一个初始化方法，然后结合non-local similarity的方法，构建矩阵进行SVD去噪，从而达到去马赛克噪声的效果。具体数学部分挺多的，请看原文或者之前博文。&lt;/p&gt;
&lt;p&gt;LSSC是09年提出的nonlocal+dictionary learning的美妙融合，成为了领域的milestone，当时做image restoration是效果最好的。先学字典，然后稀疏编码求解。这个字典的学习是精髓，不同于BM3D直接使用小波字典，这里作者用了L1,2 norm来使得同样的信号尽量获得同样的编码，用group sparsity从而使得字典更紧凑。&lt;/p&gt;
&lt;p&gt;Regularization-based: 由于CDM是一个ill-posed problem,所以一般人们习惯于加入正则项来约束退化模型，从而得到原始的估计信号，这就使得正则项对于整个问题的重要性不言而喻了。这里说一篇《cdm using inter-channel correlation and nonlocal self-similarity》的TIP文章，作者提出了两个重要的term来做CDM restoration问题。首先是TV-term和inter-color channel的结合，在difference map上做tv效果会比单独TV更佳。 然后是nonlocal matrix的low rank constraint，由于高频纹理复杂以及噪声影响，这个nonlocal matrix可能不是低秩的，这里用一个低秩矩阵加上Outliers矩阵来近似，意思是总能来nonlocal matrix附近找到一个低秩矩阵满足低秩，从而将假设放宽了一点。最后优化这两个正则项。（当然这是基于MLRI的初始化之后再做的demosaicking denoise),接下来都是凸优化的数学问题求解了。&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/ISM_1.png&quot; alt=&quot;最后的优化问题&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;最后是又快又好的插值方法：GBTF，-RI-based，-CNN-based&quot;&gt;&lt;a href=&quot;#最后是又快又好的插值方法：GBTF，-RI-based，-CNN-based&quot; class=&quot;headerlink&quot; title=&quot;最后是又快又好的插值方法：GBTF， RI-based， CNN-based&quot;&gt;&lt;/a&gt;最后是又快又好的插值方法：GBTF， RI-based， CNN-based&lt;/h4&gt;&lt;p&gt;GBTF的论文可以理解为更细致的adaptive插值方法，先做一个LCC1的初始化，然后类似Total Variance的方法，构建一个difference map.在这个difference map上面做四个方法的加权插值，最后将这个estimate difference加回初始化图像，得到最终结果。特别地，对于R,B通道，提出用Laplacian filter加权拟合效果更佳。（这也是后来MLRI的思想）&lt;/p&gt;
&lt;p&gt;RI-based methods:&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/RI_1.png&quot; alt=&quot;RI流程&quot;&gt;&lt;br&gt;RI是最近比较热门的方法，传统流程是在估计G通道之后，用R-G的difference做R图的恢复估计。现在RI是不直接在R-G difference map上面做，而是让G做引导图像，R做被滤波图像，得到tentative R,我们用tentative R - R 的difference(residual map)做插值恢复。最后作者用residual map和difference map对比一下，说明了residual map的像素梯度变化更缓和，有利于梯度插值，减少了插值误差，所以work.&lt;/p&gt;
&lt;p&gt;之后延伸了MLRI：bilinear interpolation perform better for minimum laplacian energies. 在G和R通道上分别做sparse laplacian filter的卷积操作，其余跟RI一样。我的理解是，做完这个laplacian filter之后，residual image变得更加smooth,所以效果又提升了。&lt;/p&gt;
&lt;p&gt;IRI： 将Iterative和RI结合，交替做G和R/B的恢复&lt;br&gt;ARI: 将MLRI和IRI加权融合，因为有时候MLRI处理不佳，有时候IRI对对于强相关区域处理不佳，所以ADAPTIVELY结合两者，再做加权平均。&lt;/p&gt;
&lt;p&gt;Deep JDD: 将CDM和DNS一起做，它的网络没有用初始化方法，而是rearrange CFA,加入一个噪声参数层，end-to-end train,用了大量的数据，并且用目前的criterion来提取Hard-case建立了复杂库，并用这些库对网络进行fine-tune。最后得到较佳的JDD结果。&lt;br&gt;个人理解：由于没有初始化方法，所以需要大量的数据来学习CDM的初始化，网络需要学习的内容复杂（针对自己的实验，如果没有初始化效果会十分不佳），专注于hard-case的CDM可能会导致平滑区域的CDM效果不佳，其实不太有必要（我没有hard case效果还是不错,这也是它的网络在Kodak上面表现不佳的原因). 其实用CNN做JDD是十分好用的，但目前还存在的问题大概是：1.在处理CDM和DNS的流程上应该如何较佳（张老师认为DNS-&amp;gt;CDM会更好，而目前CFA DNS效果不佳，导致其后的CDM也不太好） 2.如何将CNN-JDD做得更快更好，拟合真实噪声的分布&lt;/p&gt;
&lt;p&gt;CNN-based的CDM，暂时我自己的网络用20层，64的kernel就已经把CDM效果做爆了，甚至比JDD要好。初始化方法是很有必要的，sequence end-to-end效果已经很好了，但这里面G channel细节信息更多error小，R/B channel细节信息较少，所以error大；如何用G CHANNEL来GUIDANCE成为问题。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;今天来总结一下Color Demosaicking（CDM）里面的重要论文和方法。希望能囊括AP, AHD, SA, LDI-NAT, DLMMSE, LSSC, GBTF, RI等方法&lt;/p&gt;
&lt;h4 id=&quot;比较旧的：AP-AHD-SA等&quot;&gt;&lt;a href=&quot;#比较旧的
    
    </summary>
    
      <category term="Tech" scheme="https://csrjtan.github.io/categories/Tech/"/>
    
    
      <category term="reivew CDM" scheme="https://csrjtan.github.io/tags/reivew-CDM/"/>
    
  </entry>
  
  <entry>
    <title>雅思笔记总结</title>
    <link href="https://csrjtan.github.io/2017/03/27/%E9%9B%85%E6%80%9D%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/"/>
    <id>https://csrjtan.github.io/2017/03/27/雅思笔记总结/</id>
    <published>2017-03-27T09:29:17.000Z</published>
    <updated>2017-03-28T14:49:22.000Z</updated>
    
    <content type="html">&lt;p&gt;还有1小时上Ethics，无聊之际将旧有的笔记本总结一下，然后‘清理’吧&lt;/p&gt;
&lt;h3 id=&quot;阅读真经&quot;&gt;&lt;a href=&quot;#阅读真经&quot; class=&quot;headerlink&quot; title=&quot;阅读真经&quot;&gt;&lt;/a&gt;阅读真经&lt;/h3&gt;&lt;figure class=&quot;highlight ada&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;阅读先看题，定位快寻觅&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;两种题后做，优先细节选&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;同义多替换，单词有灵犀&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;填词有规律，前后找痕迹&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;并列需查重，生词不用疑&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;难度为中等，变幻四种体&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;填表填图题，一见笑眯眯&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;顺藤能摸瓜，按图可索骥&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;答案常集中，原始送分题&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;段落选标题，连锁不简单&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;段中找亮点，中心藏后边&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;判断实不难，真假未提及&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;末题少驳斥，首题少NG&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;我有七种意，天下剑桥题&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;多选找并列，单选是&lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;题&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;如遇选标题，末段加大意&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;匹配乱序多，定位找同义&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;段落含信息，小心有NB&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;莫夸境界高，无招胜有招&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;三剑已合璧，笑看雅思谜&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;这是刘洪波雅思阅读真经，我感觉当年能阅读7.5这是对我整个雅思考试来说帮助最大的，所以每逢想起雅思就是这段经文了。同学们也要好好“查经”，他总结得特别好。&lt;/p&gt;
&lt;h4 id=&quot;英语语法&quot;&gt;&lt;a href=&quot;#英语语法&quot; class=&quot;headerlink&quot; title=&quot;英语语法&quot;&gt;&lt;/a&gt;英语语法&lt;/h4&gt;&lt;p&gt;这里有语法树的&lt;a href=&quot;http://www.cnblogs.com/memset/archive/2013/03/26/2981778.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;英语语法&lt;/a&gt;。分为词法和语法， 词法包括名词、代词、数词、冠词、动词、介词、连词、形容词、副词；其中动词包括谓语动词和非谓语动词，谓语动词包括情态动词，时态，语态（主动，被动）和虚拟语气，非谓语动词包括不定式，现在/过去分词以及动名词。&lt;/p&gt;
&lt;p&gt;然后是语法，分为一般规则和特殊规则。 一般的包括369，三类句子：简单句，并列句和复合句（定语/状语/名词性从句），六种简单句型，九种句子成分。 特殊规则又包括强调，倒装，语序，主谓一致，引语，平行和比较。&lt;/p&gt;
&lt;h4 id=&quot;丰富写作&quot;&gt;&lt;a href=&quot;#丰富写作&quot; class=&quot;headerlink&quot; title=&quot;丰富写作&quot;&gt;&lt;/a&gt;丰富写作&lt;/h4&gt;&lt;p&gt;句式：句首状语提前，句上植入短语，现在分词和过去分词，强调句，虚拟语气，-ly结尾的副词，主被动交替和平行结构&lt;/p&gt;
&lt;p&gt;连词：因：as, due to, owing to 果：thus, therefore, so, as a result 目的：thus+verbing 举例：for instance, such, this is confirmed by, the example of 对比：while, whereas, by contrast, unlike, one of the main differences between … and .. is 类比：similarly,  like …, 让步：in spite of, although, even if, even though, … as sb./sth. may be/seem … 假设：if…,(then) … 修饰定从： that, who, which, why(reason/reasons)&lt;/p&gt;
&lt;p&gt;限定范围：apart from, as well as, rather than, instead of&lt;br&gt;Admittedly,… .Nevertheless, …&lt;br&gt;e.g. Some experts believe that it is better for children to egin learning a foreign language at primary school rather than secondary school.&lt;/p&gt;
&lt;p&gt;下定义：means that, which means that, which is essentially&lt;/p&gt;
&lt;p&gt;写作常用短语搭配：achieve one’s full potential, work-life balance, acquire knowledge, gain better understand, 坚持做adhere/stick to, hold on, boosting profits, deepen understanding of, combat/address/tackle a problem&lt;/p&gt;
&lt;h4 id=&quot;经典句型&quot;&gt;&lt;a href=&quot;#经典句型&quot; class=&quot;headerlink&quot; title=&quot;经典句型&quot;&gt;&lt;/a&gt;经典句型&lt;/h4&gt;&lt;p&gt;1.According to a recent survey, millions people die each year from dieases linked to smoking&lt;br&gt;2.The last surveys show that quite a few children hae unpleasant association to homework.&lt;br&gt;&lt;a href=&quot;http://3.No&quot; class=&quot;test test-url&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;3.No&lt;/a&gt; invention has received more praise and abuse than Internet&lt;br&gt;4.People seem to fail to take into account the fact that education does not end with graduation.&lt;br&gt;5.An increasing number of people are beginning to realize that education is not complete with graduation.&lt;br&gt;6.When it comes to education, the majority of people believe that education is a lifetime study.&lt;br&gt;7.Many experts pont out that physical exercise contributes directly to a person’s physical fitness.&lt;br&gt;8.Proper measures must be taken to limit the number of foreign tourists and the great efforts should be made to protect local environment and history form the harmful effects of international tourism.&lt;br&gt;9.An increasing number of experts believe that migrants will exert positive effects on construction of city. However, this oppinion is now being questioned by more and more city residents, who complain that the migrants have bought many serious problems like crime and prostitution.&lt;br&gt;10.Many city residents complain that it’s so few buses in their city that they have to spend much more time waiting for a bus, which is usually crowed with a large number of passengers.&lt;br&gt;11.There is no denying the fact that air pollution is an extremely serious problem: the city authorities should take strong measures to deal with it.&lt;br&gt;12.An investigation shows that female workers tend to have a favorable attitude toward retirement.&lt;br&gt;13.A proper part-time job does not occupy students’ too much time. In fact, it is unhealthy for them to spend all of time on their study. As an old saying goes: All work and no play makes Jack a dull boy.&lt;br&gt;14.Any government, which is blind to this point, may pay a heavy price.&lt;br&gt;15.Nowadays, many students always go into raptures at the mere mention of the coming life of high school.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;还有1小时上Ethics，无聊之际将旧有的笔记本总结一下，然后‘清理’吧&lt;/p&gt;
&lt;h3 id=&quot;阅读真经&quot;&gt;&lt;a href=&quot;#阅读真经&quot; class=&quot;headerlink&quot; title=&quot;阅读真经&quot;&gt;&lt;/a&gt;阅读真经&lt;/h3&gt;&lt;figure class=&quot;highlight ada&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;阅读先看题，定位快寻觅&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;两种题后做，优先细节选&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;同义多替换，单词有灵犀&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;填词有规律，前后找痕迹&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;并列需查重，生词不用疑&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;难度为中等，变幻四种体&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;填表填图题，一见笑眯眯&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;顺藤能摸瓜，按图可索骥&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;答案常集中，原始送分题&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;段落选标题，连锁不简单&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;段中找亮点，中心藏后边&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;判断实不难，真假未提及&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;末题少驳斥，首题少NG&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;我有七种意，天下剑桥题&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;多选找并列，单选是&lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;题&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;如遇选标题，末段加大意&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;匹配乱序多，定位找同义&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;段落含信息，小心有NB&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;莫夸境界高，无招胜有招&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;三剑已合璧，笑看雅思谜&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Life" scheme="https://csrjtan.github.io/categories/Life/"/>
    
    
      <category term="IELTS" scheme="https://csrjtan.github.io/tags/IELTS/"/>
    
  </entry>
  
  <entry>
    <title>关于学习方法</title>
    <link href="https://csrjtan.github.io/2017/03/16/%E5%85%B3%E4%BA%8E%E5%AD%A6%E4%B9%A0/"/>
    <id>https://csrjtan.github.io/2017/03/16/关于学习/</id>
    <published>2017-03-16T03:56:26.000Z</published>
    <updated>2017-03-16T05:03:21.000Z</updated>
    
    <content type="html">&lt;p&gt;从WikiHow上面看到的生活经验，其实wikiHow上面的内容和方法论都很好，只是知易行难。&lt;/p&gt;
&lt;h3 id=&quot;如何成为学霸&quot;&gt;&lt;a href=&quot;#如何成为学霸&quot; class=&quot;headerlink&quot; title=&quot;如何成为学霸&quot;&gt;&lt;/a&gt;如何成为学霸&lt;/h3&gt;&lt;h4 id=&quot;保持良好的学习状态&quot;&gt;&lt;a href=&quot;#保持良好的学习状态&quot; class=&quot;headerlink&quot; title=&quot;保持良好的学习状态&quot;&gt;&lt;/a&gt;保持良好的学习状态&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;无论大脑还是身体，都需要保持最高效的状态&lt;/li&gt;
&lt;li&gt;充足的睡眠休息，外加适量的运动，最好是早睡早起&lt;/li&gt;
&lt;li&gt;健康的饮食规律，少吃多油多脂高糖的垃圾食品，多吃健身餐&lt;/li&gt;
&lt;li&gt;多喝水，保持充分的水分才能使得大脑良好地工作&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;找到适合自己的学习方法&quot;&gt;&lt;a href=&quot;#找到适合自己的学习方法&quot; class=&quot;headerlink&quot; title=&quot;找到适合自己的学习方法&quot;&gt;&lt;/a&gt;找到适合自己的学习方法&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;视觉学习者：通过图表、想象来帮助记忆&lt;/li&gt;
&lt;li&gt;听觉学习者：轻音乐帮助记忆，多听课，通过语音学习&lt;/li&gt;
&lt;li&gt;运动学习者：喜欢边学习边做动作，走来走去的，可以尝试橡皮泥&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;认真听讲&quot;&gt;&lt;a href=&quot;#认真听讲&quot; class=&quot;headerlink&quot; title=&quot;认真听讲&quot;&gt;&lt;/a&gt;认真听讲&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;让自己在前排听讲&lt;/li&gt;
&lt;li&gt;积极认真回答问题&lt;/li&gt;
&lt;li&gt;遇到问题与感兴趣的点举手提问&lt;/li&gt;
&lt;li&gt;学习记笔记（后面详述）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;提前认真完成作业&quot;&gt;&lt;a href=&quot;#提前认真完成作业&quot; class=&quot;headerlink&quot; title=&quot;提前认真完成作业&quot;&gt;&lt;/a&gt;提前认真完成作业&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;遇到难题与同学讨论&lt;/li&gt;
&lt;li&gt;可以求助老师与助教&lt;/li&gt;
&lt;li&gt;寻找安静专注的学习环境（图书馆，书房等）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;通过其它方式补充课堂的学习内容&quot;&gt;&lt;a href=&quot;#通过其它方式补充课堂的学习内容&quot; class=&quot;headerlink&quot; title=&quot;通过其它方式补充课堂的学习内容&quot;&gt;&lt;/a&gt;通过其它方式补充课堂的学习内容&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;兴趣是最好的老师，跟随自己的兴趣&lt;/li&gt;
&lt;li&gt;阅读相关的兴趣读物&lt;/li&gt;
&lt;li&gt;寻找该知识的实用性场景，学以致用&lt;/li&gt;
&lt;li&gt;例如在学习英语，可以通过影片纪录片来加强锻炼&lt;/li&gt;
&lt;li&gt;放假期间，适当地进行知识回顾总结，以及预习下一阶段的课程&lt;/li&gt;
&lt;li&gt;在备考时，提早开始复习（难度越大，复习越早）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;学会做人&quot;&gt;&lt;a href=&quot;#学会做人&quot; class=&quot;headerlink&quot; title=&quot;学会做人&quot;&gt;&lt;/a&gt;学会做人&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;懂得在别人做得好的时候，鼓励表扬他人，不要刻薄取笑&lt;/li&gt;
&lt;li&gt;助人为乐，帮助他人解决问题，分享知识，分享学习笔记&lt;/li&gt;
&lt;li&gt;尊重他人，多聆听别人的观点，学会理解&lt;/li&gt;
&lt;li&gt;保持冷静，坚持自我&lt;/li&gt;
&lt;li&gt;培养幽默感，保持学习的热情和积极乐观态度&lt;/li&gt;
&lt;li&gt;做自己，做真正让自己开心充实的事情，与人分享热爱的事情，与能让你成长的人交朋友，不要过分在意他人对你的看法&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;关于记笔记&quot;&gt;&lt;a href=&quot;#关于记笔记&quot; class=&quot;headerlink&quot; title=&quot;关于记笔记&quot;&gt;&lt;/a&gt;关于记笔记&lt;/h3&gt;&lt;p&gt;1.按学科特点，课程形式来记笔记，如讲座要求又快又好&lt;br&gt;2.记住自己的目标，留意你重视的信息，快速接受。如果是写作论文，需要关注论文的大纲，写下主旨性和关键性的想法&lt;br&gt;3.笔记方便思考记忆，加深了解信息，结构化思考知识&lt;br&gt;4.听讲为重，转述要点，学会概述&lt;br&gt;5.记下概念和术语，与正文分开&lt;br&gt;6.学会简写&lt;/p&gt;
&lt;h4 id=&quot;关于读书笔记&quot;&gt;&lt;a href=&quot;#关于读书笔记&quot; class=&quot;headerlink&quot; title=&quot;关于读书笔记&quot;&gt;&lt;/a&gt;关于读书笔记&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;为术语下定义，可以记录页码，帮助回到文中理解&lt;/li&gt;
&lt;li&gt;列出重要概念，帮助简化复杂的内容&lt;/li&gt;
&lt;li&gt;为纲要天上内容，写下学习目的的摄像，看到相关信息并记下页码&lt;/li&gt;
&lt;li&gt;标上不同的颜色，帮助突出重点，组织笔记结构&lt;/li&gt;
&lt;/ul&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;从WikiHow上面看到的生活经验，其实wikiHow上面的内容和方法论都很好，只是知易行难。&lt;/p&gt;
&lt;h3 id=&quot;如何成为学霸&quot;&gt;&lt;a href=&quot;#如何成为学霸&quot; class=&quot;headerlink&quot; title=&quot;如何成为学霸&quot;&gt;&lt;/a&gt;如何成为学霸&lt;/h3&gt;&lt;h
    
    </summary>
    
      <category term="Life" scheme="https://csrjtan.github.io/categories/Life/"/>
    
    
      <category term="高效 学习" scheme="https://csrjtan.github.io/tags/%E9%AB%98%E6%95%88-%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>matconvnet使用教学</title>
    <link href="https://csrjtan.github.io/2017/03/08/matconvnet%E4%BD%BF%E7%94%A8%E6%95%99%E5%AD%A6/"/>
    <id>https://csrjtan.github.io/2017/03/08/matconvnet使用教学/</id>
    <published>2017-03-08T13:41:00.000Z</published>
    <updated>2017-03-16T03:53:25.000Z</updated>
    
    <content type="html">&lt;p&gt;研究生期间，一直都在使用MatConvNet这个框架，感觉需要来个小总结以及小教学。&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h3 id=&quot;关于部署&quot;&gt;&lt;a href=&quot;#关于部署&quot; class=&quot;headerlink&quot; title=&quot;关于部署&quot;&gt;&lt;/a&gt;关于部署&lt;/h3&gt;&lt;p&gt;从官网下载最新版，当前是1.0-beta-23，matlab中用到mex,所以部署一下VS环境即可，按照官网来，先CPU模式vl_nncomplie()一下，然后再根据GPU命令编译一道。注意的是：1.cuda tool kit的目录，默认在c:program/toolkit…/cuda/v7.5/之类的，然后cudnn用cudnn5，放在matconvnet的matlab下，新建local的文件夹。还需要把cudnn5/bin/cudnn5.dll放到mex文件夹下面。如果遇到round function的问题，可以根据网上教的，把round function写在matconvnet的cuda源代码里面。&lt;/p&gt;
&lt;h3 id=&quot;关于matconvnet使用&quot;&gt;&lt;a href=&quot;#关于matconvnet使用&quot; class=&quot;headerlink&quot; title=&quot;关于matconvnet使用&quot;&gt;&lt;/a&gt;关于matconvnet使用&lt;/h3&gt;&lt;p&gt;大部分可以参照官方网站还有一个Manuel pdf,剩下的主要阅读项目的matlab代码，cuda代码暂时不需要涉及。主要分为simplenn net and DAG net， simple的是单向线性网，而DAG是多向图，从而方便设计更复杂的网络框架结构。当然暂时来说，matconvnet对于RNN的支持不太够，但在拟合deep learning来说还是很有用的。&lt;/p&gt;
&lt;p&gt;设计一个深度学习的程序包括：1.生成训练集 2.设置程序的默认参数 3.初始化网络的结构以及参数 4.训练数据导入和预处理 5.训练部署和优化过程 6.网络测试实验&lt;/p&gt;
&lt;h4 id=&quot;关于线性网络simplenn&quot;&gt;&lt;a href=&quot;#关于线性网络simplenn&quot; class=&quot;headerlink&quot; title=&quot;关于线性网络simplenn&quot;&gt;&lt;/a&gt;关于线性网络simplenn&lt;/h4&gt;&lt;figure class=&quot;highlight cs&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;初始化网络&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;number&quot;&gt;1.&lt;/span&gt;导入matconvnet: run(fullfile(fileparts(mfilename(&lt;span class=&quot;string&quot;&gt;&#39;fullpath&#39;&lt;/span&gt;)),&lt;span class=&quot;string&quot;&gt;&#39;..&#39;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&#39;matconvnet-master&#39;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&#39;matlab&#39;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&#39;vl_setupnn.m&#39;&lt;/span&gt;));&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;number&quot;&gt;2.&lt;/span&gt;初始化网络: net.layers=&amp;#123;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;number&quot;&gt;3.&lt;/span&gt;加层：net.layers&amp;#123;end+&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&amp;#125; = &lt;span class=&quot;keyword&quot;&gt;struct&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;&#39;type&#39;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&#39;conv&#39;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&#39;weights&#39;&lt;/span&gt;,&amp;#123;&amp;#125;,&lt;span class=&quot;string&quot;&gt;&#39;params&#39;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&#39;&#39;&lt;/span&gt;,&amp;#123;&amp;#125;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;number&quot;&gt;4.&lt;/span&gt;放入simplenn网络,并添加默认参数: net = vl_simplenn_tidy(net)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;训练网络&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[net,info] = cnn_train(net,imdb,getBatch(),&lt;span class=&quot;string&quot;&gt;&#39;expDir&#39;&lt;/span&gt;,expDir,&lt;span class=&quot;string&quot;&gt;&#39;train&#39;&lt;/span&gt;,find(imdb.images.&lt;span class=&quot;keyword&quot;&gt;set&lt;/span&gt;==&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;),&lt;span class=&quot;string&quot;&gt;&#39;val&#39;&lt;/span&gt;,find(imdb.images.&lt;span class=&quot;keyword&quot;&gt;set&lt;/span&gt;==&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;),&lt;span class=&quot;string&quot;&gt;&#39;learningRate&#39;&lt;/span&gt;,lr,&lt;span class=&quot;string&quot;&gt;&#39;theta&#39;&lt;/span&gt;,weight_theta,&lt;span class=&quot;string&quot;&gt;&#39;batchSize&#39;&lt;/span&gt;,bz,&lt;span class=&quot;string&quot;&gt;&#39;modelname&#39;&lt;/span&gt;,modelname,&lt;span class=&quot;string&quot;&gt;&#39;gpus&#39;&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;梯度更新 accumulate_gradients(state,net,res,opts,batchSize,mmap)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;定义损失函数（如L2） Y = vl_nnloss(X,c,dzdy,varargin)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; nargin&amp;lt;=&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt; || isempty(dzdy)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  t=((X-c).^&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)/&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  Y=t(:);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  Y = bsxfun(@minus,X,c).*dzdy;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;end&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;线性测试过程： load(&lt;span class=&quot;string&quot;&gt;&#39;mynet.mat&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;net = vl_simplenn_tidy(net);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;net.layers = net.layers(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;:end&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;res = vl_simplenn(net,input,[],[],&lt;span class=&quot;string&quot;&gt;&#39;mode&#39;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&#39;test&#39;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;关于训练数据生成与格式&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;一般是cell,只要在输入层和objective function处（最后层）对应维度即可。meta分为train,val,test。用cell images包括images.input images.label images.&lt;span class=&quot;keyword&quot;&gt;set&lt;/span&gt;，其中&lt;span class=&quot;keyword&quot;&gt;set&lt;/span&gt;的来区分训练集和验证集。&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#### 关于有向图DAG net&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;初始化 net.layers=&amp;#123;&amp;#125;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;net=dagnn.DagNN.fromSimpleNN(net,&lt;span class=&quot;string&quot;&gt;&#39;canonicalNames&#39;&lt;/span&gt;,&lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;net.addLayer(name,block,input,output,&lt;span class=&quot;keyword&quot;&gt;params&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;由于Multi-input and multi-output，需要定义好网络的variables和params，让params连接起来.&lt;/p&gt;
&lt;p&gt;在数据导入和getBatch函数时，也需要注意名字对上。&lt;/p&gt;
&lt;h3 id=&quot;MatConvNet-Manual&quot;&gt;&lt;a href=&quot;#MatConvNet-Manual&quot; class=&quot;headerlink&quot; title=&quot;MatConvNet Manual&quot;&gt;&lt;/a&gt;MatConvNet Manual&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/matconvnet_1.png&quot; alt=&quot;经典DAG&quot;&gt;&lt;br&gt;由于最近研究上，一般使用复杂的网络结构比较多，如VGG Net，ResNet，RNN等，传统的AlexNet, Linear Net已经在表达能力上有所欠缺。其中x为输入输出的变量variables(feature maps)，f为layer name, w为该层的参数（params),这些元素组成了网络，而DAG的指向是有向任意的，只要输入输出的维度对应上的话。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/matconvnet_2.png&quot; alt=&quot;SimpleNN BP&quot;&gt;&lt;br&gt;在网络训练过程中的BP，会创建一个反向的Net用于记录误差导数的值，但结构是完全逆向的，这样完整一次正向和反向才能最后更新网络的参数。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/matconvnet_3.png&quot; alt=&quot;DAG BP&quot;&gt;&lt;br&gt;上图是DAG BP的算法：1.将variables x按DAG顺序排列 2.对网络forward pass 3.按照对应x的大小初始化dx 4.遍历每一层：a.根据index找到x,找不到则下一个循环 b.用残差的导数算式更新derivative of x&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/matconvnet_4.png&quot; alt=&quot;DAG BP structure&quot;&gt;&lt;br&gt;类似simplenn BP, DAG BP过程中也需要完整逆向构建DAG网络，当完成一次正传和反传以后再进行梯度的更新（这时候可以适应多种梯度更新策略）。&lt;/p&gt;
&lt;p&gt;针对实现细节，可以直接去研究代码，卷积操作类似CAFFE，先把image通过im2row转化成tensor来加速卷积操作的。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;研究生期间，一直都在使用MatConvNet这个框架，感觉需要来个小总结以及小教学。&lt;/p&gt;
    
    </summary>
    
      <category term="Tech" scheme="https://csrjtan.github.io/categories/Tech/"/>
    
    
      <category term="matconvnet" scheme="https://csrjtan.github.io/tags/matconvnet/"/>
    
  </entry>
  
  <entry>
    <title>deep_compress ICLR2016 BEST PAPER</title>
    <link href="https://csrjtan.github.io/2017/03/08/deep-compress/"/>
    <id>https://csrjtan.github.io/2017/03/08/deep-compress/</id>
    <published>2017-03-08T03:14:28.000Z</published>
    <updated>2017-03-16T03:53:42.000Z</updated>
    
    <content type="html">&lt;p&gt;最近中了ICME2017的会议，抽空扫了一下有趣的论文。这篇是ICLR2016的best paper,作者是standford的song han。看了在微软的讲座，主要讲了Deep compress和Dense Sparse Dense training，最后是efficient inference engine的设计。&lt;/p&gt;
&lt;h3 id=&quot;Deep-Compress&quot;&gt;&lt;a href=&quot;#Deep-Compress&quot; class=&quot;headerlink&quot; title=&quot;Deep Compress&quot;&gt;&lt;/a&gt;Deep Compress&lt;/h3&gt;&lt;p&gt;摘要：该文章的压缩思路很清晰也简单，实验做得很充分。主要用了三个技巧：1.weights pruning,对网络的系数进行thresholding的截断，导致接近0的系数为0，从而增加网络的稀疏性，减少网络的连接和系数。(reduce ~10X) 2.Quantization,用更少的bits（32bits to 5bits)来表示系数，这里面用到了聚类方法来建立code book的技巧。  3.对code book进行哈夫曼编码，进一步压缩存储空间。&lt;br&gt;  VGG从5MB 到11.3MB。&lt;/p&gt;
&lt;h4 id=&quot;整个pipeline&quot;&gt;&lt;a href=&quot;#整个pipeline&quot; class=&quot;headerlink&quot; title=&quot;整个pipeline&quot;&gt;&lt;/a&gt;整个pipeline&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/deep_compress_1.png&quot; alt=&quot;Three stage compression&quot;&gt;&lt;br&gt;关于&lt;strong&gt;weights pruning&lt;/strong&gt;:这是网络压缩的常用技巧，对于原始网络首先正常训练得到最终weights，然后将少于某个阈值的weights从网络移除，最后再retrain稀疏网络的参数&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;关于&lt;strong&gt;Quantization&lt;/strong&gt;:在上一步pruning之后，用compressed sparse row &amp;amp; compressed sparse column的数据结构来存储网络，然后用code bokk 压缩8bits来存conv参数，5bits代表FC参数。以下用inNeurons=4,outNeurons=4的一层作为例子，而对于网络是针对整个网络做的。&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/deep_compress_2.png&quot; alt=&quot;training codebooks&quot;&gt;&lt;br&gt;首先对参数聚类，得到k个聚类中心（k根据压缩的比特来选择，比如5bits就是2^5=32),然后在每次更新的gradients中做group by and reduce操作得到code gradient,用当前网络的学习率更新code book.&lt;br&gt;最后再用huffman对code book编码。&lt;/p&gt;
&lt;p&gt;在用了以上strategy以后，不损失精度情况下，AlexNet from 240MB to 6.9MB&lt;/p&gt;
&lt;h3 id=&quot;Dense-Sparse-Dense-Training&quot;&gt;&lt;a href=&quot;#Dense-Sparse-Dense-Training&quot; class=&quot;headerlink&quot; title=&quot;Dense Sparse Dense Training&quot;&gt;&lt;/a&gt;Dense Sparse Dense Training&lt;/h3&gt;&lt;p&gt;思想跟dropout有点像，也是需要保持网络结构的稀疏性，但是这篇文章的正则项在training procedure上起作用，可以看做training regularization. 只是简单地在中间添加sparse connection（将较少的weights直接移除）的training，然后交替做training，而不是整个training procedure都用dropout来做，在不调整网络结构的基础下，DSD的训练模式使得效果提升。（但没有正面跟dropout比较）。但可以与dropout technique一起使用&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/dense_sparse_dense_train.png&quot; alt=&quot;DSD Training&quot;&gt;&lt;/p&gt;
&lt;p&gt;最后分析了一下DSD过程中的weights的变化，retrain会令pruning的weigths恢复，获得一种dense的weights分布。&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/dsd_weights.png&quot; alt=&quot;training codebooks&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;总结展望&quot;&gt;&lt;a href=&quot;#总结展望&quot; class=&quot;headerlink&quot; title=&quot;总结展望&quot;&gt;&lt;/a&gt;总结展望&lt;/h4&gt;&lt;p&gt;ICLR2016 BEST PAPER介绍了一种通用的压缩技巧：1.weights pruning 2.quantization 3.huffman coding. 而Dense sparse dense training则对训练过程加入regularizer使用更robusted.&lt;br&gt;Pruning和DSD的成功，主要还是sparse(L1)结构在起作用，在描述图像先验的时候，sparse prior确实是一种通用而且有效的先验。针对CDM或者JDD甚至V-CDM来说，这样的sparse tricks其实也是可用的。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;最近中了ICME2017的会议，抽空扫了一下有趣的论文。这篇是ICLR2016的best paper,作者是standford的song han。看了在微软的讲座，主要讲了Deep compress和Dense Sparse Dense training，最后是efficient inference engine的设计。&lt;/p&gt;
&lt;h3 id=&quot;Deep-Compress&quot;&gt;&lt;a href=&quot;#Deep-Compress&quot; class=&quot;headerlink&quot; title=&quot;Deep Compress&quot;&gt;&lt;/a&gt;Deep Compress&lt;/h3&gt;&lt;p&gt;摘要：该文章的压缩思路很清晰也简单，实验做得很充分。主要用了三个技巧：1.weights pruning,对网络的系数进行thresholding的截断，导致接近0的系数为0，从而增加网络的稀疏性，减少网络的连接和系数。(reduce ~10X) 2.Quantization,用更少的bits（32bits to 5bits)来表示系数，这里面用到了聚类方法来建立code book的技巧。  3.对code book进行哈夫曼编码，进一步压缩存储空间。&lt;br&gt;  VGG从5MB 到11.3MB。&lt;/p&gt;
&lt;h4 id=&quot;整个pipeline&quot;&gt;&lt;a href=&quot;#整个pipeline&quot; class=&quot;headerlink&quot; title=&quot;整个pipeline&quot;&gt;&lt;/a&gt;整个pipeline&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/deep_compress_1.png&quot; alt=&quot;Three stage compression&quot;&gt;&lt;br&gt;关于&lt;strong&gt;weights pruning&lt;/strong&gt;:这是网络压缩的常用技巧，对于原始网络首先正常训练得到最终weights，然后将少于某个阈值的weights从网络移除，最后再retrain稀疏网络的参数&lt;/p&gt;
    
    </summary>
    
      <category term="Tech" scheme="https://csrjtan.github.io/categories/Tech/"/>
    
    
      <category term="papers" scheme="https://csrjtan.github.io/tags/papers/"/>
    
  </entry>
  
  <entry>
    <title>2016读书总结</title>
    <link href="https://csrjtan.github.io/2016/11/13/diary20161112/"/>
    <id>https://csrjtan.github.io/2016/11/13/diary20161112/</id>
    <published>2016-11-12T17:21:33.000Z</published>
    <updated>2017-03-16T03:56:00.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/20161112_02.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;最近自制了日程本，为了合理清晰管理自己的时间，也让自己落实制定每天的计划，不然怕自己的思绪总是走偏，过于八卦而漫游。于是这个小小的日程本，可以让我每次迷茫或者不愿前行的时候，翻出来记住自己该前进的目标以及去积极行动的事情。这里定下了一些定量或者频率的事情，希望先将定量的一个一个划掉，然后年底能将频率的事情圆满完成，这里比较难的事情是需要完成52本书的阅读，算上上半年确实也有20多本，但最近三个月好像没有具体完成一本书的阅读，所以这里列举看过的书，以及制定余下时间看书的计划。谨记：年轻人的迷茫，一方面是见识浅薄，做得太少，见过太少，另一方面就是读书太少了。&lt;/p&gt;
&lt;h4 id=&quot;已读：&quot;&gt;&lt;a href=&quot;#已读：&quot; class=&quot;headerlink&quot; title=&quot;已读：&quot;&gt;&lt;/a&gt;已读：&lt;/h4&gt;&lt;p&gt;1.《自制力》：心理学类，自我管理，具体地认识自制力，然后如何去约束自我，养成良好的习惯，从而改变个人性格。  我的感悟：有助于理解自制力，对自己工作和生活的节奏调整，和克制有一定的帮助，值得有时间重温。&lt;br&gt;2.《如何高效学习》：成长类，作者是1年完成MIT 33门课程的高效能人士，讲述了他如何高效学习，用符合大脑结构的方法，突破自我思维，提高个人记忆。在认识到好的笔记或者思考模式后，需要刻意培训锻炼，从而养成真正有用的思考能力。  我的感悟：可以看到某些聪明人的思考方式，然而一方面用在自己身上只具有部分的适用性，另外培养这种思考模式需要不懈的毅力。&lt;br&gt;3.《此生未完成》：于娟，一位得了癌症的年轻博士毕业讲师，用诙谐幽默的言语和笔调，讲述自己对自已一生的回顾，以及抗癌历程。 我的感悟：珍惜生命，珍惜岁月，时刻谨记自己拥有的，感恩生活，感恩他人，认真工作，好好生活。&lt;br&gt;4.《围城》：钱老的经典，讲述镀金小伙方鸿渐回国当讲师的历程，遇到的女子，走过的经历，反应当时社会的风气。 我的感悟：不要像方鸿渐一样，自己应该做一个有主见，坚强的男人。&lt;br&gt;5.《黄金时代》：暂时唯一看过的王小波的作品，纯粹出于青年人的血气方刚来看这书，描写王二在文革期间个性开放的经历，也是能深刻反映人性和当时时代背景的特点。 我的感悟：果然王小波来写色情小说后，其他的色情小说都显得很不入流。&lt;br&gt;6.《棋王》：阿城的三王之一，讲述一个棋痴在文革时候的故事，两大爱好：吃和下棋，大概是小时候穷掼了，影射出棋王的世界观和性格。 个人感悟：阿城的描写手法细腻而栩栩如生，而且中短篇小说为主，很是值得去看，题材主要还是文革时期。&lt;br&gt;7.《幸福之路》：罗素的哲学类范本，从好几个大的话题来展开罗素的人生观，具有一定的借鉴意义。  个人感悟：我阅读的时候觉得太过于文绉绉了，以致于沉睡了无数个晚上终于完成，不然我可以一年都看不完一本书了。&lt;br&gt;8.《人性的弱点》： 仅次于圣经的伟大一书，从多个故事和例子上缓缓道来为人之道，如何建立良好的人际关系，以及Leader Ship等等，是一个入世和为人处事的一本良目。 个人感悟：每一次看都觉得似曾相识，估计这辈子可以阅读不下10次了。&lt;br&gt;9.《白夜行》：东野圭吾的佳作，从一个悬案开始，主人公以警长的身份和视角描写整个事件的走势，这是两个小孩人心叵测的推理小说吧，主要是作者的构思和叙事手法十分的精彩。 个人感悟：感动于男的工具人残忍但又默默地付出，震撼于一个聪明美丽小女孩的城府。&lt;br&gt;10.《一个人的朝圣》：哈罗德是一个生活的失败者，但由于远方朋友的一封信，使得他决定徒步北上，像一个朝圣者一样打破了自己原本破烂不堪的生活，作者最后揭开了他们儿子的真相，使读者有心灵上的震撼和冲击。  个人感悟：我认为哈罗德是一个橡皮人，从某些角度看与自己多少有些相似，畏惧不前，不敢付出但又勤勤勉勉，希望自己能成为一个决断的人，勇敢认真。&lt;br&gt;11.《解忧杂货铺》：东野圭吾的经典，利用杂货铺信箱可以穿越时空的设定，讲述三个混混通过对陌生人寄来的来信进行回复答疑，在帮助了别人的人生的过程中，也帮助了他们自己。 个人感悟：每个人在遇到人生选择和难题的时候，最清楚选择的其实往往还是他们自己，他们需要咨询的只是朋友给予的肯定或者否定，然而答案其实早就藏于他们的心中。&lt;br&gt;12.《统计学习方法》：李航写的关于统计学习，机器学习的书本，里面主要涉及基本的原理，还有一些重要公式的推导论证，深入浅出。  个人感悟：在中文教材十分稀缺的年代，这确实是不可多得的一本好书，目前在看周志华老师的《机器学习》，生动有趣，希望里面获得的干货会更多！&lt;br&gt;13.《如何阅读一本书》：成长工具类，首先这是一个高效学习的好书，它讲述了如何分层次去阅读一本好书，从目录标题，章节分层，主题阅读灯；另外可以关联到如何去判断一本好书，经典。里面包含了如何阅读各类书籍的技巧，希望重温三次，并付诸实践。  个人感悟：先学会层次化阅读，然后对编程主题的书进行快速的主题阅读。&lt;br&gt;14.《从0到1》：创业类，讲述在商业社会中，在互联网潮流中，真正有意义和有价值产品的概念，蓝海都是需要从0到1的，这里面是伟大而富有意义的。 个人感悟：多做一些创新，改变世界的事情，而不是重复和量产的事情。&lt;br&gt;15.《35岁前要做的33件事》：鸡汤类，三十而立，确定自己人生格局和定位，哲学的思想，多参与世界；培养自我的能力，增长见识，学会投资理财；理解世界，理解他人，理解自我；&lt;br&gt;16.《当我谈跑步时，我谈些什么》：村上春树的作品，作为一个作家，他是一个比较专业的长跑跑手，从碎碎念中，更多得到的是他在坚持中收获到的瑰宝。 个人感悟：在自己的人生中，我们应该学会坚持，特别是坚持一些宝贵而不容易做到的事情，它使得我们与众不同。&lt;br&gt;17.《站在两个世界的边缘》：程浩，一个残疾的小伙，收录了他在豆瓣和知乎上的一些解答，可以理解到这其实是一个卑微的灵魂，但丰富的阅读让他拥有高尚的人格，而又显得那么的伟大。 个人感悟：阅读使人成长，特别的心灵上。&lt;br&gt;18.《这就是搜索引擎》：一本将搜索引擎庖丁解牛的书，里面涉及一些架构，以及搜索引擎用到的关键技术，主要是爬虫，清洗，索引，RANKING，云等重要结构。  个人感觉：将部块分解，搜索引擎主要面对的难点只是大数据，所以用分布式堆砌可以有效的解决问题。&lt;br&gt;19.《未来在现实的第几层》：一本简介未来10年可以工业化的重要科学技术，主要偏向工业界的，包括生活和材料屏幕等。 个人感觉：可以让人开开科学的眼界。&lt;br&gt;20.《你一定爱读的极简欧洲史》： 不，我不爱读，里面将复杂的欧洲史压缩成小书，各种人名让我各种懵。不过把欧洲史的重点刻画出来了，中世纪，日耳曼民族和教皇。 个人感悟：我渴望去一次欧洲旅行，希望带上这本书重温欧洲文化，希腊神话，基督文明。&lt;br&gt;21.《再穷也要去旅行》：流水账的旅行小册，感觉我自己去一次整理写出来比这个好看，主要这位东南亚女孩也比较大胆和个性，旅行的时间也比我早多了，还是佩服这样有勇气的背包客。 个人感悟：我也想当一次背包客，说走就走，走万里路，见识这个世界！&lt;br&gt;22.《30而励》：芮成钢，作为一个外国交流大使，写一些敏感的政治时间，并用国际化的视角解读。  个人感悟：可能我没细看，觉得不太感兴趣。&lt;br&gt;23.《活着》：余华的作品，我最爱看的中文名著，没有之一，我觉得主角的命运很惨，但在这种基调下的人生反而显出了生的伟大。 个人感悟：可能我只是希望看看别人的生活比我惨，然后我庆幸和感恩自己所拥有的生活，每次我灰心丧气，只要想起福贵，我就知道自己不是绝境，还应该去努力一把。&lt;br&gt;24.《C++ Primer》：C语言是面向硬件，指针化，struct和宏比较多；C++是面向对象。 个人感悟：学好C和C++，走遍天下也不怕。&lt;br&gt;25.《小王子》：很短的反思性小说，童心的纯真让我们反省起作为社会和成年人的复杂和勾心斗角。 个人感悟：保持一颗美好的童心，会让你更感恩和热爱生活。&lt;br&gt;26.《黑客与画家》：Paul Graham是一个前辈黑客，现在是硅谷的投资者；讲述他认为的黑客的特点，从他的角度可以很好地理解黑客和技术是什么。&lt;br&gt;27.《苏菲的世界》：第一本西方哲学的入门书，从教授给小女孩的书信说起，一步一步地讲述西方哲学的发展和理论的构建，通俗易懂的理解了哲学的发展。 个人感悟：要攻读PHD，先理解哲学的思考方式和人生观是首当其冲的。&lt;br&gt;28.《我与地坛》：史铁生老师的书都是写入读者深处灵魂的，他对生与死的看法也是超乎常人，在这系列小说了，对人生、母爱、生死都有了深刻的描写和展现。 个人感悟：看得很精彩和感动。&lt;br&gt;29.《把时间当作朋友》：成长管理类，如何更好的管理自我，管理时间，首先改变认识，然后制定有效的计划措施。 个人感悟：有空值得重温，对时间宝贵的我们具有借鉴意义。&lt;br&gt;30.《倾城之恋》：讲述留学潮的女性求爱故事，作为一个大家闺秀在爱情里面的赌博，也反应着张爱玲老师的恋爱观。 个人感悟：乱世佳人，言情师祖。&lt;br&gt;31.《向左走，向右走》：这不算书，算是插画，讲述两个孤独城市里的陌生人，一次大雨的邂逅，各怀好感的两人却再也相遇不上对方；留下的只有漫长的等待，缘分就是始终没有出现在这对暧昧的恋人未满身上。&lt;br&gt;32.《别为小事抓狂》：心理学类，教会如何学习情绪管理。 个人感悟：太久之前阅读，比较鸡汤。&lt;br&gt;33.《拆掉思维里的墙》：鸡汤类，讲述思维的重要性，如何做成一件事和一个人的发展，主要还是看其思维和行事方式，这是每个人需要去突破自我，突破固定的思维，会发现更多。 &lt;/p&gt;
&lt;p&gt;剩下要读的：&lt;br&gt;19本&lt;br&gt;《当下的力量》、《机器学习》、《我是你爸爸》、《杀死一只知更鸟》、《The element of style》、《尽管去做-无压力工作法》、《娱乐至死》、《潜规则》、《沟通的艺术》、《影响力》、《书读完了》、《追风筝的人》、。。。（待补充）&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h4 id=&quot;今天日记&quot;&gt;&lt;a href=&quot;#今天日记&quot; class=&quot;headerlink&quot; title=&quot;今天日记&quot;&gt;&lt;/a&gt;今天日记&lt;/h4&gt;&lt;p&gt;阅读了一系列关于SR的文章，到了港岛和小珊宝贝吃了大快活的火锅，很满足快乐，还买了点手工皂回家用，既有学习，也有娱乐的收获，幸福感满满。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/20161112_01.png&quot; alt=&quot;珊珊馋嘴图&quot;&gt;&lt;/p&gt;
&lt;p&gt;今天是中山大学的诞辰92周年，更是中山爷爷诞辰150周年，香港博物馆有纪念展览仪式，必须过去参观礼拜。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/20161112_03.png&quot; alt=&quot;中山爷爷诞辰展览&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/20161112_02.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;最近自制了日程本，为了合理清晰管理自己的时间，也让自己落实制定每天的计划，不然怕自己的思绪总是走偏，过于八卦而漫游。于是这个小小的日程本，可以让我每次迷茫或者不愿前行的时候，翻出来记住自己该前进的目标以及去积极行动的事情。这里定下了一些定量或者频率的事情，希望先将定量的一个一个划掉，然后年底能将频率的事情圆满完成，这里比较难的事情是需要完成52本书的阅读，算上上半年确实也有20多本，但最近三个月好像没有具体完成一本书的阅读，所以这里列举看过的书，以及制定余下时间看书的计划。谨记：年轻人的迷茫，一方面是见识浅薄，做得太少，见过太少，另一方面就是读书太少了。&lt;/p&gt;
&lt;h4 id=&quot;已读：&quot;&gt;&lt;a href=&quot;#已读：&quot; class=&quot;headerlink&quot; title=&quot;已读：&quot;&gt;&lt;/a&gt;已读：&lt;/h4&gt;&lt;p&gt;1.《自制力》：心理学类，自我管理，具体地认识自制力，然后如何去约束自我，养成良好的习惯，从而改变个人性格。  我的感悟：有助于理解自制力，对自己工作和生活的节奏调整，和克制有一定的帮助，值得有时间重温。&lt;br&gt;2.《如何高效学习》：成长类，作者是1年完成MIT 33门课程的高效能人士，讲述了他如何高效学习，用符合大脑结构的方法，突破自我思维，提高个人记忆。在认识到好的笔记或者思考模式后，需要刻意培训锻炼，从而养成真正有用的思考能力。  我的感悟：可以看到某些聪明人的思考方式，然而一方面用在自己身上只具有部分的适用性，另外培养这种思考模式需要不懈的毅力。&lt;br&gt;3.《此生未完成》：于娟，一位得了癌症的年轻博士毕业讲师，用诙谐幽默的言语和笔调，讲述自己对自已一生的回顾，以及抗癌历程。 我的感悟：珍惜生命，珍惜岁月，时刻谨记自己拥有的，感恩生活，感恩他人，认真工作，好好生活。&lt;br&gt;4.《围城》：钱老的经典，讲述镀金小伙方鸿渐回国当讲师的历程，遇到的女子，走过的经历，反应当时社会的风气。 我的感悟：不要像方鸿渐一样，自己应该做一个有主见，坚强的男人。&lt;br&gt;5.《黄金时代》：暂时唯一看过的王小波的作品，纯粹出于青年人的血气方刚来看这书，描写王二在文革期间个性开放的经历，也是能深刻反映人性和当时时代背景的特点。 我的感悟：果然王小波来写色情小说后，其他的色情小说都显得很不入流。&lt;br&gt;6.《棋王》：阿城的三王之一，讲述一个棋痴在文革时候的故事，两大爱好：吃和下棋，大概是小时候穷掼了，影射出棋王的世界观和性格。 个人感悟：阿城的描写手法细腻而栩栩如生，而且中短篇小说为主，很是值得去看，题材主要还是文革时期。&lt;br&gt;7.《幸福之路》：罗素的哲学类范本，从好几个大的话题来展开罗素的人生观，具有一定的借鉴意义。  个人感悟：我阅读的时候觉得太过于文绉绉了，以致于沉睡了无数个晚上终于完成，不然我可以一年都看不完一本书了。&lt;br&gt;8.《人性的弱点》： 仅次于圣经的伟大一书，从多个故事和例子上缓缓道来为人之道，如何建立良好的人际关系，以及Leader Ship等等，是一个入世和为人处事的一本良目。 个人感悟：每一次看都觉得似曾相识，估计这辈子可以阅读不下10次了。&lt;br&gt;9.《白夜行》：东野圭吾的佳作，从一个悬案开始，主人公以警长的身份和视角描写整个事件的走势，这是两个小孩人心叵测的推理小说吧，主要是作者的构思和叙事手法十分的精彩。 个人感悟：感动于男的工具人残忍但又默默地付出，震撼于一个聪明美丽小女孩的城府。&lt;br&gt;10.《一个人的朝圣》：哈罗德是一个生活的失败者，但由于远方朋友的一封信，使得他决定徒步北上，像一个朝圣者一样打破了自己原本破烂不堪的生活，作者最后揭开了他们儿子的真相，使读者有心灵上的震撼和冲击。  个人感悟：我认为哈罗德是一个橡皮人，从某些角度看与自己多少有些相似，畏惧不前，不敢付出但又勤勤勉勉，希望自己能成为一个决断的人，勇敢认真。&lt;br&gt;11.《解忧杂货铺》：东野圭吾的经典，利用杂货铺信箱可以穿越时空的设定，讲述三个混混通过对陌生人寄来的来信进行回复答疑，在帮助了别人的人生的过程中，也帮助了他们自己。 个人感悟：每个人在遇到人生选择和难题的时候，最清楚选择的其实往往还是他们自己，他们需要咨询的只是朋友给予的肯定或者否定，然而答案其实早就藏于他们的心中。&lt;br&gt;12.《统计学习方法》：李航写的关于统计学习，机器学习的书本，里面主要涉及基本的原理，还有一些重要公式的推导论证，深入浅出。  个人感悟：在中文教材十分稀缺的年代，这确实是不可多得的一本好书，目前在看周志华老师的《机器学习》，生动有趣，希望里面获得的干货会更多！&lt;br&gt;13.《如何阅读一本书》：成长工具类，首先这是一个高效学习的好书，它讲述了如何分层次去阅读一本好书，从目录标题，章节分层，主题阅读灯；另外可以关联到如何去判断一本好书，经典。里面包含了如何阅读各类书籍的技巧，希望重温三次，并付诸实践。  个人感悟：先学会层次化阅读，然后对编程主题的书进行快速的主题阅读。&lt;br&gt;14.《从0到1》：创业类，讲述在商业社会中，在互联网潮流中，真正有意义和有价值产品的概念，蓝海都是需要从0到1的，这里面是伟大而富有意义的。 个人感悟：多做一些创新，改变世界的事情，而不是重复和量产的事情。&lt;br&gt;15.《35岁前要做的33件事》：鸡汤类，三十而立，确定自己人生格局和定位，哲学的思想，多参与世界；培养自我的能力，增长见识，学会投资理财；理解世界，理解他人，理解自我；&lt;br&gt;16.《当我谈跑步时，我谈些什么》：村上春树的作品，作为一个作家，他是一个比较专业的长跑跑手，从碎碎念中，更多得到的是他在坚持中收获到的瑰宝。 个人感悟：在自己的人生中，我们应该学会坚持，特别是坚持一些宝贵而不容易做到的事情，它使得我们与众不同。&lt;br&gt;17.《站在两个世界的边缘》：程浩，一个残疾的小伙，收录了他在豆瓣和知乎上的一些解答，可以理解到这其实是一个卑微的灵魂，但丰富的阅读让他拥有高尚的人格，而又显得那么的伟大。 个人感悟：阅读使人成长，特别的心灵上。&lt;br&gt;18.《这就是搜索引擎》：一本将搜索引擎庖丁解牛的书，里面涉及一些架构，以及搜索引擎用到的关键技术，主要是爬虫，清洗，索引，RANKING，云等重要结构。  个人感觉：将部块分解，搜索引擎主要面对的难点只是大数据，所以用分布式堆砌可以有效的解决问题。&lt;br&gt;19.《未来在现实的第几层》：一本简介未来10年可以工业化的重要科学技术，主要偏向工业界的，包括生活和材料屏幕等。 个人感觉：可以让人开开科学的眼界。&lt;br&gt;20.《你一定爱读的极简欧洲史》： 不，我不爱读，里面将复杂的欧洲史压缩成小书，各种人名让我各种懵。不过把欧洲史的重点刻画出来了，中世纪，日耳曼民族和教皇。 个人感悟：我渴望去一次欧洲旅行，希望带上这本书重温欧洲文化，希腊神话，基督文明。&lt;br&gt;21.《再穷也要去旅行》：流水账的旅行小册，感觉我自己去一次整理写出来比这个好看，主要这位东南亚女孩也比较大胆和个性，旅行的时间也比我早多了，还是佩服这样有勇气的背包客。 个人感悟：我也想当一次背包客，说走就走，走万里路，见识这个世界！&lt;br&gt;22.《30而励》：芮成钢，作为一个外国交流大使，写一些敏感的政治时间，并用国际化的视角解读。  个人感悟：可能我没细看，觉得不太感兴趣。&lt;br&gt;23.《活着》：余华的作品，我最爱看的中文名著，没有之一，我觉得主角的命运很惨，但在这种基调下的人生反而显出了生的伟大。 个人感悟：可能我只是希望看看别人的生活比我惨，然后我庆幸和感恩自己所拥有的生活，每次我灰心丧气，只要想起福贵，我就知道自己不是绝境，还应该去努力一把。&lt;br&gt;24.《C++ Primer》：C语言是面向硬件，指针化，struct和宏比较多；C++是面向对象。 个人感悟：学好C和C++，走遍天下也不怕。&lt;br&gt;25.《小王子》：很短的反思性小说，童心的纯真让我们反省起作为社会和成年人的复杂和勾心斗角。 个人感悟：保持一颗美好的童心，会让你更感恩和热爱生活。&lt;br&gt;26.《黑客与画家》：Paul Graham是一个前辈黑客，现在是硅谷的投资者；讲述他认为的黑客的特点，从他的角度可以很好地理解黑客和技术是什么。&lt;br&gt;27.《苏菲的世界》：第一本西方哲学的入门书，从教授给小女孩的书信说起，一步一步地讲述西方哲学的发展和理论的构建，通俗易懂的理解了哲学的发展。 个人感悟：要攻读PHD，先理解哲学的思考方式和人生观是首当其冲的。&lt;br&gt;28.《我与地坛》：史铁生老师的书都是写入读者深处灵魂的，他对生与死的看法也是超乎常人，在这系列小说了，对人生、母爱、生死都有了深刻的描写和展现。 个人感悟：看得很精彩和感动。&lt;br&gt;29.《把时间当作朋友》：成长管理类，如何更好的管理自我，管理时间，首先改变认识，然后制定有效的计划措施。 个人感悟：有空值得重温，对时间宝贵的我们具有借鉴意义。&lt;br&gt;30.《倾城之恋》：讲述留学潮的女性求爱故事，作为一个大家闺秀在爱情里面的赌博，也反应着张爱玲老师的恋爱观。 个人感悟：乱世佳人，言情师祖。&lt;br&gt;31.《向左走，向右走》：这不算书，算是插画，讲述两个孤独城市里的陌生人，一次大雨的邂逅，各怀好感的两人却再也相遇不上对方；留下的只有漫长的等待，缘分就是始终没有出现在这对暧昧的恋人未满身上。&lt;br&gt;32.《别为小事抓狂》：心理学类，教会如何学习情绪管理。 个人感悟：太久之前阅读，比较鸡汤。&lt;br&gt;33.《拆掉思维里的墙》：鸡汤类，讲述思维的重要性，如何做成一件事和一个人的发展，主要还是看其思维和行事方式，这是每个人需要去突破自我，突破固定的思维，会发现更多。 &lt;/p&gt;
&lt;p&gt;剩下要读的：&lt;br&gt;19本&lt;br&gt;《当下的力量》、《机器学习》、《我是你爸爸》、《杀死一只知更鸟》、《The element of style》、《尽管去做-无压力工作法》、《娱乐至死》、《潜规则》、《沟通的艺术》、《影响力》、《书读完了》、《追风筝的人》、。。。（待补充）&lt;/p&gt;
    
    </summary>
    
      <category term="Life" scheme="https://csrjtan.github.io/categories/Life/"/>
    
    
      <category term="随笔" scheme="https://csrjtan.github.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>认识教会的朋友</title>
    <link href="https://csrjtan.github.io/2016/11/08/%E8%AE%A4%E8%AF%86%E6%95%99%E4%BC%9A%E7%9A%84%E6%9C%8B%E5%8F%8B/"/>
    <id>https://csrjtan.github.io/2016/11/08/认识教会的朋友/</id>
    <published>2016-11-08T12:21:20.000Z</published>
    <updated>2017-03-16T03:55:13.000Z</updated>
    
    <content type="html">&lt;p&gt;这一段时间生活确实比较枯燥，感觉自己把自己给困住了，总结一下最近三个月的生活日常。&lt;/p&gt;
&lt;p&gt;首先今年宪源小朋友来科大读Msc了，也是学习机器学习内容，可以充实忙碌地学习工作了。来了之后组织女朋友和他情侣俩一块来到城门水塘，由于起床比较晚，中间不大会路，去到大概已经3点多了。从西铁线去到荃湾站，然后坐82号巴士到公园。&lt;/p&gt;
&lt;p&gt;城门水塘整个是一个麦理浩径的一段，所以十分的长，要走完感觉一天也勉强，一般就直接进去看看猴子，到了水塘附近找个美景拍拍照。这里确实风景优美，空气清新，是在香港这个钢铁森林里一个极好的郊野公园，以后有机会还要再来拍风景和人物美照。&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/2016110801.png&quot; alt=&quot;离开水塘前的合照&quot;&gt;&lt;/p&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;之后，珊珊突然周末希望去教会参加崇拜。受到了Windy的邀请，我们来到了九龙维景酒店对面的窝福堂，认识了普通话团契的伙伴们，大家都热切殷勤地欢迎了我俩。第一次参加他们的团契活动就是Outing了，我们先坐尖沙咀的邮轮来到中环，然后步行去到中环的一个比较文艺的旅游景点，有情侣石阶，壁画街，还参观了香港旧警署宿舍改造的设计工坊。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/2016110802.png&quot; alt=&quot;一起坐邮轮&quot;&gt;&lt;/p&gt;
&lt;p&gt;结果，我们渐渐地周末从没有活动而去一起学习，变成了参加团契的活动，虽然有时候开支比较大，但也很珍惜和热心的朋友在一起的光阴。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/2016110803.png&quot; alt=&quot;壁画街的幸福照&quot;&gt;&lt;/p&gt;
&lt;p&gt;后来的一周参加桌游活动，在观塘附近找到一家很赞的酒楼，性价比也是很好的。只是感概在香港开个桌游房都人均100，确实略贵，广州同类甚至娱乐性更强的才人均25~。~，玩完之后大家去了一个那边超级大的商场对面的草坪，参加了跳蚤市场。我俩在商场里的一家书局买了好久的手账，不过看到珊爷满载而归，还是十分值得的。&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/2016110804.png&quot; alt=&quot;桌游活动&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;这一段时间生活确实比较枯燥，感觉自己把自己给困住了，总结一下最近三个月的生活日常。&lt;/p&gt;
&lt;p&gt;首先今年宪源小朋友来科大读Msc了，也是学习机器学习内容，可以充实忙碌地学习工作了。来了之后组织女朋友和他情侣俩一块来到城门水塘，由于起床比较晚，中间不大会路，去到大概已经3点多了。从西铁线去到荃湾站，然后坐82号巴士到公园。&lt;/p&gt;
&lt;p&gt;城门水塘整个是一个麦理浩径的一段，所以十分的长，要走完感觉一天也勉强，一般就直接进去看看猴子，到了水塘附近找个美景拍拍照。这里确实风景优美，空气清新，是在香港这个钢铁森林里一个极好的郊野公园，以后有机会还要再来拍风景和人物美照。&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/2016110801.png&quot; alt=&quot;离开水塘前的合照&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Life" scheme="https://csrjtan.github.io/categories/Life/"/>
    
    
      <category term="教会 朋友" scheme="https://csrjtan.github.io/tags/%E6%95%99%E4%BC%9A-%E6%9C%8B%E5%8F%8B/"/>
    
  </entry>
  
</feed>
