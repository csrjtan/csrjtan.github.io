<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[CSRJTAN]]></title>
  <subtitle><![CDATA[Keep Moving]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="https://csrjtan.github.io/"/>
  <updated>2015-12-23T07:07:37.000Z</updated>
  <id>https://csrjtan.github.io/</id>
  
  <author>
    <name><![CDATA[CsrjTan]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[《爱的五种语言》]]></title>
    <link href="https://csrjtan.github.io/2015/12/23/%E3%80%8A%E7%88%B1%E7%9A%84%E4%BA%94%E7%A7%8D%E8%AF%AD%E8%A8%80%E3%80%8B/"/>
    <id>https://csrjtan.github.io/2015/12/23/《爱的五种语言》/</id>
    <published>2015-12-23T06:51:03.000Z</published>
    <updated>2015-12-23T07:07:37.000Z</updated>
    <content type="html"><![CDATA[<p>有一段时间没有更新博客了，觉得不写还是会颓，应该积极向上的去写更多东西，关于自己，关于读书，关于学习，每天应该在不断的总结和反省中前进。</p>
<h3 id="u300A_u7231_u7684_u4E94_u79CD_u8BED_u8A00_u300B"><a href="#u300A_u7231_u7684_u4E94_u79CD_u8BED_u8A00_u300B" class="headerlink" title="《爱的五种语言》"></a>《爱的五种语言》</h3><p>这里提出使用五种爱语，我们应该抓住伴侣的主要爱语，这样可以有效地提高爱箱，对于人与人之间的相处事半功半，实际可以以此延伸到亲人和友情，这是一种让人感受爱意最直接高效的方法。<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/爱的五种语言.png" alt=""></p>
<h3 id="u5173_u4E8E_u4E2A_u4EBA_u7BA1_u7406"><a href="#u5173_u4E8E_u4E2A_u4EBA_u7BA1_u7406" class="headerlink" title="关于个人管理"></a>关于个人管理</h3><p>坚持每晚11点休息，早上6点半起床吧，早起的一天才能正能量满满，积极向上呢，项目最近整体复杂度已经有点高，希望自己尽快使它们有效完成吧，腾出手来关注一些真正有用的事情。<br>接下来是个人管理的小图<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/每日个人管理.png" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>有一段时间没有更新博客了，觉得不写还是会颓，应该积极向上的去写更多东西，关于自己，关于读书，关于学习，每天应该在不断的总结和反省中前进。</p>
<h3 id="u300A_u7231_u7684_u4E94_u79CD_u8BED_u8A00_u300B"><a href]]>
    </summary>
    
      <category term="读书笔记" scheme="https://csrjtan.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[paper-reading-20151125]]></title>
    <link href="https://csrjtan.github.io/2015/11/25/paper-reading-20151125/"/>
    <id>https://csrjtan.github.io/2015/11/25/paper-reading-20151125/</id>
    <published>2015-11-25T06:13:16.000Z</published>
    <updated>2016-01-20T06:06:54.000Z</updated>
    <content type="html"><![CDATA[<p>这周回来帮崇雨审一个文章，燕大的关于动态物体避障的方案，总体来说写得比较自我，而且没有系统和实用的示例，只是从理论上写的一个方案，自己提出了很多新的东西，然而没办法去很好验证。整个流程是提取曲面特征，通过下一帧的Depth变化为依据，最大化The best Next View来求出摄像机的移动策略，这里将运动评估作为一个term加入到the best next view model里面，通过迭代优化的过程来求出摄像机的位置和方向(x,v).用观测到障碍物体的最大面积来作为调整相机的依据，从一定假设上可以实现避障。</p>
<h3 id="Dynamic_occlusion_avoidance_approach_based_on_the_depth_image_of_moving_visual_object"><a href="#Dynamic_occlusion_avoidance_approach_based_on_the_depth_image_of_moving_visual_object" class="headerlink" title="Dynamic occlusion avoidance approach based on the depth image of moving visual object"></a>Dynamic occlusion avoidance approach based on the depth image of moving visual object</h3><p>KeyWords:Moving Object,Depth Image,Dynamic Occlusion Avoidance,Best View Model</p>
<h4 id="Abstract_3ADynamic_Occlusion_avoidance_based_on_the_depth_image"><a href="#Abstract_3ADynamic_Occlusion_avoidance_based_on_the_depth_image" class="headerlink" title="Abstract:Dynamic Occlusion avoidance based on the depth image."></a>Abstract:Dynamic Occlusion avoidance based on the depth image.</h4><p>首先，用Anti-projection Transformation来获得移动物体的每个像素的三维坐标<br>然后，用第二张深度图来构建The Best View Model<br>第三，高斯曲率的特征是平面变换的本质计量，所以运动评估比较两个高斯曲率特征矩阵来实现<br>(Gaussian Curvature:The product of the maximum and minimum curvatures of the sectionsIt is the intrinsic measure of curvature.)<br>最后，结合Best View Model和运动检测结果，最优化目的是使摄像机行为满足移动物体的避障过程。</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>主要问题：通过物体的障碍提示来寻找最优观测方向和位置<br>Next Best View:</p>
<ul>
<li>用Octree模型来描述可视物体，然后对于不同观察情况的节点，给定不同的分数。</li>
<li>用大量周全的Candidate View</li>
<li>建立一个相似的参数模型，通过depth data和current fitted model来拟合</li>
<li>用B-spline计算信息增益来构建最佳NEXT VIEW</li>
<li>结合on-line theory来优化物体的3D重构<br>缺点：没有考虑遮蔽</li>
</ul>
<p>引入occlusion方法<br>缺点：受限于摄像机位置、特定的设备和先验信息，以上均不适用于移动的物体</p>
<h3 id="u4E09_u4E2A_u95EE_u9898_uFF1A"><a href="#u4E09_u4E2A_u95EE_u9898_uFF1A" class="headerlink" title="三个问题："></a>三个问题：</h3><p>1.如何解决动态遮蔽的避障问题? -&gt; 用一个优化模型，结合运动预测在Best view Model里<br>2.如何通过深度图来预测可视物体的运动? -&gt; 用两个高斯曲率特征矩阵的匹配来求R T,用到SIFT和SVD<br>3.如何有效评估动态遮蔽的影响? -&gt; 用“effective avoidance rate”来评估算法的性能</p>
<h3 id="Method_Overview"><a href="#Method_Overview" class="headerlink" title="Method Overview"></a>Method Overview</h3><p>1.The analysis of dynamic occlusion avoidance<br>获得遮蔽的最大区域</p>
<p>2.The overall idea of dynamic occlusion avoidance<br>首先，移动物体相近深度图的像素进行三维坐标恢复（用Anti-projection），则遮蔽暗示可以从第二幅图得到<br>基于上述，用第二张深度图的遮蔽信息来构建Best View Model<br>接着，用三维坐标点的形式，做高斯曲率特征矩阵来匹配两个相近的深度图<br>最后，将运动预测结合到最佳视角模型里。</p>
<h3 id="The_Approach_to_dynamic_occlusion_avoidance_based_on_depth_image"><a href="#The_Approach_to_dynamic_occlusion_avoidance_based_on_depth_image" class="headerlink" title="The Approach to dynamic occlusion avoidance based on depth image"></a>The Approach to dynamic occlusion avoidance based on depth image</h3><p>1.Constructing the occlusion region to be avoided<br><img src="/" alt=""><br>构建出patch</p>
<p>2.Constructing the best view model<br>通过构建最大化Occlusion面积</p>
<h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h3><p>step1:Calc the 3D pixels and the Gaussian Curvature Feature matrices<br>step2:Detect the Occlusion Boundary and establish the occlusion region in second depth<br>step3:Contruct the best view model<br>step4:Match the key points<br>step5:Solve the Objective formula<br>step6:Plan the next view of camera<br>step7:Acquire a depth image and calc the f(x)<br>step8:If the difference between two adjacent f(x) is less than a given threshold,then terminated,or jump to Step 4.</p>
<h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><p>1.Add emotion estimate in Best Next View to dynamic occlusion avoidance<br>2.Based the depth image to solve the R and T transformation<br>3.Propose the “Effective avoidance rate” to measure the performance of the algorithm.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这周回来帮崇雨审一个文章，燕大的关于动态物体避障的方案，总体来说写得比较自我，而且没有系统和实用的示例，只是从理论上写的一个方案，自己提出了很多新的东西，然而没办法去很好验证。整个流程是提取曲面特征，通过下一帧的Depth变化为依据，最大化The best Next Vie]]>
    </summary>
    
      <category term="paper" scheme="https://csrjtan.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[第一学期总结]]></title>
    <link href="https://csrjtan.github.io/2015/11/25/%E7%AC%AC%E4%B8%80%E5%AD%A6%E6%9C%9F%E6%80%BB%E7%BB%93/"/>
    <id>https://csrjtan.github.io/2015/11/25/第一学期总结/</id>
    <published>2015-11-25T00:52:01.000Z</published>
    <updated>2015-11-25T01:17:26.000Z</updated>
    <content type="html"><![CDATA[<h3 id="DJI_u65E0_u4EBA_u673A_u907F_u969C_u98DE_u884C_u9879_u76EE"><a href="#DJI_u65E0_u4EBA_u673A_u907F_u969C_u98DE_u884C_u9879_u76EE" class="headerlink" title="DJI无人机避障飞行项目"></a>DJI无人机避障飞行项目</h3><h4 id="u7814_u8BFB_u4E86CAMERA_CALIBRATION_2CSTEREO_VISION_2CSLAM"><a href="#u7814_u8BFB_u4E86CAMERA_CALIBRATION_2CSTEREO_VISION_2CSLAM" class="headerlink" title="研读了CAMERA CALIBRATION,STEREO VISION,SLAM"></a>研读了CAMERA CALIBRATION,STEREO VISION,SLAM</h4><p>其中做SLAM的时候基于opencv、pcl、g2o为依赖库的开发</p>
<h3 id="DJI_u90E8_u4EF6_u7684_u5B66_u4E60_u4F7F_u7528"><a href="#DJI_u90E8_u4EF6_u7684_u5B66_u4E60_u4F7F_u7528" class="headerlink" title="DJI部件的学习使用"></a>DJI部件的学习使用</h3><p>Guidance的数据订阅和传输接口，M100的GPS线损坏，而且室内校准较难，使用例程进行姿态控制</p>
<h3 id="u8BFE_u7A0BAdvanced_Algorithm"><a href="#u8BFE_u7A0BAdvanced_Algorithm" class="headerlink" title="课程Advanced Algorithm"></a>课程Advanced Algorithm</h3><p>学习了多种优化算法以及分析复杂度，课程report使用random algorithm来优化stereo matching的过程</p>
<h3 id="u4E00_u4E2A_u5B66_u671F_u8FC7_u53BB_u4E86_uFF0C_u5B66_u5230_u548C_u505A_u51FA_u6765_u7684_u6210_u679C_u4E0D_u591A_uFF0C_u53CD_u601D_u81EA_u5DF1_u7684_u6548_u7387_u4EE5_u53CA_u8FF7_u832B_uFF0C_u5BFC_u81F4_u9006_u98CE_u800C_u884C_uFF0C_u52A0_u6CB9_u5427_uFF0C_u4E0D_u7136_u8FDE_u6BD5_u4E1A_u90FD_u4E0D_u597D_u8BF4_u3002_u4E0D_u80FD_u518D_u4F9D_u8D56_u522B_u4EBA_u4E00_u8D77_u505A_u4E4B_u7C7B_u7684_uFF0C_u81EA_u5DF1_u72EC_u884C_u524D_u5F80_u5427_uFF0C_u95EE_u9898_u603B_u662F_u5B58_u5728_uFF0C_u95EE_u9898_u603B_u662F_u53EF_u4EE5_u89E3_u51B3_u7684_uFF0C_u4E0D_u8981_u9003_u907F_u62D6_u5EF6_u3002"><a href="#u4E00_u4E2A_u5B66_u671F_u8FC7_u53BB_u4E86_uFF0C_u5B66_u5230_u548C_u505A_u51FA_u6765_u7684_u6210_u679C_u4E0D_u591A_uFF0C_u53CD_u601D_u81EA_u5DF1_u7684_u6548_u7387_u4EE5_u53CA_u8FF7_u832B_uFF0C_u5BFC_u81F4_u9006_u98CE_u800C_u884C_uFF0C_u52A0_u6CB9_u5427_uFF0C_u4E0D_u7136_u8FDE_u6BD5_u4E1A_u90FD_u4E0D_u597D_u8BF4_u3002_u4E0D_u80FD_u518D_u4F9D_u8D56_u522B_u4EBA_u4E00_u8D77_u505A_u4E4B_u7C7B_u7684_uFF0C_u81EA_u5DF1_u72EC_u884C_u524D_u5F80_u5427_uFF0C_u95EE_u9898_u603B_u662F_u5B58_u5728_uFF0C_u95EE_u9898_u603B_u662F_u53EF_u4EE5_u89E3_u51B3_u7684_uFF0C_u4E0D_u8981_u9003_u907F_u62D6_u5EF6_u3002" class="headerlink" title="一个学期过去了，学到和做出来的成果不多，反思自己的效率以及迷茫，导致逆风而行，加油吧，不然连毕业都不好说。不能再依赖别人一起做之类的，自己独行前往吧，问题总是存在，问题总是可以解决的，不要逃避拖延。"></a>一个学期过去了，学到和做出来的成果不多，反思自己的效率以及迷茫，导致逆风而行，加油吧，不然连毕业都不好说。不能再依赖别人一起做之类的，自己独行前往吧，问题总是存在，问题总是可以解决的，不要逃避拖延。</h3>]]></content>
    <summary type="html">
    <![CDATA[<h3 id="DJI_u65E0_u4EBA_u673A_u907F_u969C_u98DE_u884C_u9879_u76EE"><a href="#DJI_u65E0_u4EBA_u673A_u907F_u969C_u98DE_u884C_u9879_u76EE" clas]]>
    </summary>
    
      <category term="总结" scheme="https://csrjtan.github.io/tags/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[斯坦福公开课机器人学]]></title>
    <link href="https://csrjtan.github.io/2015/11/14/OpenClassRobotic1/"/>
    <id>https://csrjtan.github.io/2015/11/14/OpenClassRobotic1/</id>
    <published>2015-11-13T16:04:37.000Z</published>
    <updated>2015-11-14T02:56:19.000Z</updated>
    <content type="html"><![CDATA[<h3 id="1-_u6982_u8FF0"><a href="#1-_u6982_u8FF0" class="headerlink" title="1.概述"></a>1.概述</h3><p>这门课程主要讲述机器人的一些基本知识，包括运动学，动力学，机臂控制，一些前沿研究。<br>由于最近做定位地图，涉及较多robotic的知识，在这里直接看一下入门，之后希望可以对ROS、PCL、G2O等重要工具展开学习。</p>
<h4 id="2-_u7A7A_u95F4_u63CF_u8FF0_uFF0C_u5E7F_u4E49_u7684_u5750_u6807"><a href="#2-_u7A7A_u95F4_u63CF_u8FF0_uFF0C_u5E7F_u4E49_u7684_u5750_u6807" class="headerlink" title="2.空间描述，广义的坐标"></a>2.空间描述，广义的坐标</h4><p>在二维空间，一个刚性物体的姿态只有3个自由度，$x,y,\theta$<br>在三维空间，一个刚性物体的姿态会有6个自由度，$x,y,z,\theta,\gamma,\phi$</p>
<p>这里描述坐标系的转换可以用一个矩阵，其中旋转矩阵$ R_{AB}=R_{BA}^T $,具有<strong>正交性</strong>，<br>$ R_{AB}^{-1} = R_{BA} = R_{BA}^T $，并且满足:<br>$$ r_X^2 = r_Y^2 = r_Z^2 = 1 $$<br>$$ r_X * r_Y = r_X * r_Z = r_Y * r_Z = 0 $$<br>其中正交矩阵相关的一些矩阵分解有：QR、SVD、谱分解和极分解。</p>
<p>相对而言，T不会满足旋转矩阵R的这些特性，这里也是基于右手系的约束。<br>为了方便矩阵的运算，我们将R和T矩阵组合并添加一维[0 0 0 1]变成4 * 4的矩阵，形成<strong>齐次</strong>.</p>
<h4 id="3-_u8BA8_u8BBA_u6B27_u62C9_u89D2_u548C_u65CB_u8F6C"><a href="#3-_u8BA8_u8BBA_u6B27_u62C9_u89D2_u548C_u65CB_u8F6C" class="headerlink" title="3.讨论欧拉角和旋转"></a>3.讨论欧拉角和旋转</h4><p>介绍了柔性制动器，气压驱动，上节课讲的内容的转换，可以描述坐标系转换，或者点的映射。由于扩展了齐次坐标系，所以末端的坐标系可以直接从基座变换矩阵T相乘解得，但在描述旋转的时候，用3*3的矩阵有9个参数，过于<strong>冗余</strong>，我们考虑更为精炼的表达方式，更高效地表示旋转。</p>
<p>引入一个，对于刚性物体的坐标表示方式根据坐标系的表达而变化，分为</p>
<ul>
<li>Cartesion : $(x,y,z)$</li>
<li>Cylindrical: $(\rho,\theta,\tau)$</li>
<li>Spherical: $(r,\theta,\phi)$<br>这是针对应用的场景而变化的，对于某些运动变化，采用适用的坐标系能大大简化问题，适应与问题。</li>
</ul>
<p>首先来讨论用<strong>三个角</strong>来表示旋转，分别是yaw,roll,hawl.$$R_{AB} = R_Z(\alpha).R_Y(\beta).r_X(\gamma)$$<br>求解一系列矩阵运算之后，出现cp = 0 的时候，会出现<strong>奇异性</strong>(回头查google)的点，此时无法进一步计算（因为除0出错）,无法进行继续跟踪，而其他以三个参数为旋转的也无法解决这个奇异点的问题。于是尝试进行四个参数的旋转表达，便有了经典的<strong>欧拉角</strong>，Euler Prameters。</p>
<p>欧拉角指根据x,y,z轴旋转，一共有24种顺规，使用前先确定规则，</p>
<h4 id="4-DH_u53C2_u6570"><a href="#4-DH_u53C2_u6570" class="headerlink" title="4.DH参数"></a>4.DH参数</h4><p>引入了“蜂鸟”机器人，如何通过连杆和终端执行器控制机械手，建立正运动学。用DH参数描述来表达对连杆的控制，，从而精确定义坐标系，设置不同参数进行变换得到总的变换，从而建立正运动学</p>
<h4 id="5-_u4EFF_u7075_u957F_u7C7B_u673A_u68B0_u81C2"><a href="#5-_u4EFF_u7075_u957F_u7C7B_u673A_u68B0_u81C2" class="headerlink" title="5.仿灵长类机械臂"></a>5.仿灵长类机械臂</h4><p>介绍了如何用欧拉角进行坐标转换，对ROBOT建立数学模型。<br>首先选定较优的坐标原点，尽量方便表达和减少参数，进而根据连杆的变换推导出新的节点的坐标表达，建立坐标表达矩阵，这样的坐标表达就包括（yaw,hawl,roll,Distance)</p>
]]></content>
    <summary type="html">
    <![CDATA[<h3 id="1-_u6982_u8FF0"><a href="#1-_u6982_u8FF0" class="headerlink" title="1.概述"></a>1.概述</h3><p>这门课程主要讲述机器人的一些基本知识，包括运动学，动力学，机臂控制，一些前沿研究。<]]>
    </summary>
    
      <category term="读书笔记" scheme="https://csrjtan.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[SLAM的学习总结]]></title>
    <link href="https://csrjtan.github.io/2015/11/08/SLAM%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    <id>https://csrjtan.github.io/2015/11/08/SLAM的学习总结/</id>
    <published>2015-11-08T09:31:46.000Z</published>
    <updated>2015-11-09T13:54:49.000Z</updated>
    <content type="html"><![CDATA[<h4 id="u597D_u4E45_u6CA1_u66F4_u65B0_u4E86_uFF0C_u6700_u8FD1_u786E_u5B9E_u4E1C_u897F_u6709_u70B9_u591A_u6709_u70B9_u4E71_uFF0C_u5FC5_u8981_u7684_u8D76_u7D27_u6765_u603B_u7ED3_u4E00_u4E0B_u4E86"><a href="#u597D_u4E45_u6CA1_u66F4_u65B0_u4E86_uFF0C_u6700_u8FD1_u786E_u5B9E_u4E1C_u897F_u6709_u70B9_u591A_u6709_u70B9_u4E71_uFF0C_u5FC5_u8981_u7684_u8D76_u7D27_u6765_u603B_u7ED3_u4E00_u4E0B_u4E86" class="headerlink" title="好久没更新了，最近确实东西有点多有点乱，必要的赶紧来总结一下了"></a>好久没更新了，最近确实东西有点多有点乱，必要的赶紧来总结一下了</h4><h3 id="u5173_u4E8ESLAM_u7684_u5B66_u4E60"><a href="#u5173_u4E8ESLAM_u7684_u5B66_u4E60" class="headerlink" title="关于SLAM的学习"></a>关于SLAM的学习</h3><p>首先，参考学习了很多关于SLAM的内容，从理论、研究到开源软件甚至于开源的调用库都知道不少，奈何自己在这方面确实尚浅，走了不少弯路。</p>
<h4 id="SLAM_u662F_u5373_u65F6_u5B9A_u4F4D_u548C_u5730_u56FE_u6784_u5EFA"><a href="#SLAM_u662F_u5373_u65F6_u5B9A_u4F4D_u548C_u5730_u56FE_u6784_u5EFA" class="headerlink" title="SLAM是即时定位和地图构建"></a>SLAM是即时定位和地图构建</h4><p>分为前后端，前端为点云的叠加构建地图，后端为根据位置和移动建图，并用非线性全局优化来修正叠加误差以及进行回环检测。（旧的方法还有涉及EKF的KALMAN滤波，但随着图增大算法会吃不消）</p>
<h4 id="u4E3B_u8981_u7684FrameWork_3A"><a href="#u4E3B_u8981_u7684FrameWork_3A" class="headerlink" title="主要的FrameWork:"></a>主要的FrameWork:</h4><p>1.通过RGB+D，反算三角原理计算得到空间坐标系上的点云<br>2.通过提取特征点和特征点匹配求出转换矩阵，从而将点云合并<br>3.将摄像机位置作为位姿顶点，转换矩阵作为转换边，得到图G(V,E)<br>4.利用位置叠加全局优化图G，这是非线性优化的过程</p>
<h4 id="u4F18_u5316_u6570_u5B66_u6A21_u578B_uFF1A"><a href="#u4F18_u5316_u6570_u5B66_u6A21_u578B_uFF1A" class="headerlink" title="优化数学模型："></a>优化数学模型：</h4><p>模型定义：<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/slam_1.png" alt=""><br><img src="http://7xl4js.com1.z0.glb.clouddn.com/slam_2.png" alt=""><br>其中 $$Mothion: x_p^{x+1} = f(x_p^i,u_i)+w_i$$<br>$$Observations: z_{i,j} = h(x_p^i,x_L^j)+v_{i,j}$$<br>求解SLAM前，我们有$u_i,z_{i,j}$分别代表传感器量度以及传感器误差，我们要求$x_p，x_L$<br>继而误差就是等式左右相减，我们目的用最小二乘全局最优化以下式子：<br>$$min \phi=\sum_i(e_p^i)^2+\sum_{i,j}(e_L^{i,j})^2$$<br>所以这是一个<strong>非线性和非凸</strong>问题,可以通过估计一个起始猜测，然后寻找各个方向上的雅克比和Hessian矩阵进行牛顿迭代等非线性优化方法。由于G的结构是稀疏的，所以可以对SLAM的全局非线性优化进行稀疏求解。（如Sparse Cholesky Decomposition）</p>
<p>这里可以利用到的一些工具包括：ROS、OctoMap、SiftGPU、OpenGL,opencv、eigen、<strong>pcl、g2o</strong>等等</p>
<p>可以看的书《Multiple View Geometry in CV》和《线性代数该这样读》，还有很多资源，参考下面的LINK，三个阶段：1.基础理论工具学习 2.执行别人的代码和阅读论文 3.阅读并改写改进，突破发论文</p>
<p>这里放一些有用的网站：<br><a href="http://blog.csdn.net/heyijia0327/article/details/47813405" target="_blank" rel="external">Graph Slam Tutorial:g2o</a></p>
<p><a href="http://vision.in.tum.de/data/datasets/rgbd-dataset" target="_blank" rel="external">Computer Vision Group RGB-D SLAM</a></p>
<p><a href="http://www.cnblogs.com/gaoxiang12/p/4633316.html" target="_blank" rel="external">一起做RGB-D SLAM系列</a></p>
<p><a href="http://www.zhihu.com/question/35186064" target="_blank" rel="external">知乎</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<h4 id="u597D_u4E45_u6CA1_u66F4_u65B0_u4E86_uFF0C_u6700_u8FD1_u786E_u5B9E_u4E1C_u897F_u6709_u70B9_u591A_u6709_u70B9_u4E71_uFF0C_u5FC5_u8981_]]>
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[数据挖掘导论（3）]]></title>
    <link href="https://csrjtan.github.io/2015/11/02/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AF%BC%E8%AE%BA%EF%BC%883%EF%BC%89/"/>
    <id>https://csrjtan.github.io/2015/11/02/数据挖掘导论（3）/</id>
    <published>2015-11-02T11:26:19.000Z</published>
    <updated>2015-11-03T06:46:26.000Z</updated>
    <content type="html"><![CDATA[<p>何女神，准备就期中考了，考到第7章，我有空继续看，顺便完善一下读书笔记，做人要有始有终。</p>
<h4 id="u56DE_u987E_u4E0A_u8282"><a href="#u56DE_u987E_u4E0A_u8282" class="headerlink" title="回顾上节"></a>回顾上节</h4><p>提到了决策树的生成，主要是选择合理的分裂属性，并且修正合理的生成树，以获得较少的训练误差和不会过于臃肿的树结构，基于Occam’s Razor Rules，这有利于更强的泛化和预测能力。</p>
<h3 id="u7B2C_u4E94_u7AE0"><a href="#u7B2C_u4E94_u7AE0" class="headerlink" title="第五章"></a>第五章</h3><p>这一章包含的内容多且重要，然而书本没有详细展开，这里我还是得仔细记录每一小节的内容。</p>
<h4 id="5-1_u57FA_u4E8E_u89C4_u5219_u7684_u5206_u7C7B_u5668"><a href="#5-1_u57FA_u4E8E_u89C4_u5219_u7684_u5206_u7C7B_u5668" class="headerlink" title="5.1基于规则的分类器"></a>5.1基于规则的分类器</h4><p>分类模型用析取范式 $R=(r_1 \bigvee r_2 \bigvee … \bigvee r_k)$<br>这里R成为<strong>规则集</strong><br>这里表示形式：$r_i:(条件_i)-&gt;y_i$<br>这里左边称为规则前提，右边为规则后件，包含预测结果。<br>分类规则的质量用覆盖率和准确率定义,D为数据集，A为规则前件，y为规则后件。<br>$$Converage（r) = \frac{|A|}{|D|}$$<br>$$Accuray(r)=\frac{|A\bigcap y|}{|A|}$$<br>基于定义，我们有<strong>互斥规则、穷举规则、有无序规则</strong></p>
<p>由于规则集中的规则不一定是互斥的，所有有可能分类的时候某条记录会属于多个类（也就是说某条记录会同时触发规则集中的超过1条的过则，而被触发的规则的类标号也不一样），这种情况有两种办法解决。<br>(1）有序规则。将规则集中的规则按照优先级降序排列，当一个测试记录出现时，由覆盖记录的最高秩的规则对其进行分类，这就避免由多条分类规则来预测而产生的类冲突问题<br>(2）无序规则。允许一条测试记录触发多条分类规则，把每条被触发规则的后件看作是对相应类的一次投票，然后计票确定测试记录的类标号。通常把记录指派到得票最多的类。</p>
<p>假设现在有一个记录它不能触发规则集合中的任何一个规则，那么它该如何就行分类呢？解决办法也有两个：<br>（1）穷举规则。如果对属性值的任一组合，R中都存在一条规则加以覆盖，则称规则集R具有穷举覆盖。这个性质确保每一条记录都至少被R中的一条规则覆盖。<br>(2）如果规则不是穷举的，那么必须添加一个默认规则rd：（）-&gt;yd来覆盖那些未被覆盖的记录。默认规则的前件为空，当所有其他规则失效时被触发。yd是默认类，通常被指定为没有被现存规则覆盖的训练记录的多数类.</p>
<p><strong>顺序覆盖算法</strong></p>
<pre><code>令E是训练记录，A是属性-值对的集合{（Aj,Vj）}
令Y0是类的有序集{y1,y2,...,yk}
令R={}是初始规则列表
for 每个类y in Y0-{yk} do
    while 终止条件不满足 do
        r = Learn-One-Rule(E,A,y)
        从E重删除被r覆盖的训练记录
        追加r到规则列表尾部:R=R V r
    end while
end for
把默认规则{}-&gt;yk插入到规则列表R尾部
</code></pre><p>这里是直接方法里的Learn-One-Rule函数，采用贪心原则。因为要找最佳规则是个NP-HARD问题，通过贪心方式近似，先产生一个初始规则r,不断求精，然后满足某种终止条件，修剪规则以改进泛化误差。书中介绍了规则增长的策略，这里不详述。</p>
<p>对于候选规则的选择，这里使用<strong>似然比</strong>作为统计量：<br>$$R=2\sum_{i=1}^kf_ilog(\frac{f_i}{e_i})$$<br>或者规则覆盖率的度量Laplace以及<strong>FOIL信息增益</strong><br>(在顺序覆盖算法中，需要删除覆盖到的训练记录，不然会重叠而误选。)可以使用<strong>RIPPER算法</strong>来建立规则集，而选取一个泛化能力较佳的模型。</p>
<h4 id="u8DDF_u51B3_u7B56_u6811_u6BD4_u8F83"><a href="#u8DDF_u51B3_u7B56_u6811_u6BD4_u8F83" class="headerlink" title="跟决策树比较"></a>跟决策树比较</h4><p>可以看到与决策树的相似性，这里我们可以用决策树作为参考，间接产生规则。<br>它们间的比较：</p>
<ul>
<li>规则集表达能力与之等价</li>
<li>基于规则分类器更易于解释，并性能相媲美</li>
<li>基于类的规则定序非常适应处理类分布不平衡的数据集</li>
</ul>
<h4 id="u603B_u7ED3"><a href="#u603B_u7ED3" class="headerlink" title="总结"></a>总结</h4><p>算法思想：先从训练集生成规则集合，用合取条件表示。对每个待分类的记录和规则集合中的规则进行比较，如果某条规则被触发，则记录被分类。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>何女神，准备就期中考了，考到第7章，我有空继续看，顺便完善一下读书笔记，做人要有始有终。</p>
<h4 id="u56DE_u987E_u4E0A_u8282"><a href="#u56DE_u987E_u4E0A_u8282" class="headerlink" t]]>
    </summary>
    
      <category term="读书笔记" scheme="https://csrjtan.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[数据挖掘导论（2）]]></title>
    <link href="https://csrjtan.github.io/2015/10/31/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AF%BC%E8%AE%BA%EF%BC%882%EF%BC%89/"/>
    <id>https://csrjtan.github.io/2015/10/31/数据挖掘导论（2）/</id>
    <published>2015-10-31T08:41:57.000Z</published>
    <updated>2015-11-02T12:15:29.000Z</updated>
    <content type="html"><![CDATA[<h3 id="u8FD9_u5199_u4E00_u4E0B_u7B2C_u4E09_u7B2C_u56DB_u7AE0_u7684_u4E3B_u8981_u5185_u5BB9"><a href="#u8FD9_u5199_u4E00_u4E0B_u7B2C_u4E09_u7B2C_u56DB_u7AE0_u7684_u4E3B_u8981_u5185_u5BB9" class="headerlink" title="这写一下第三第四章的主要内容"></a>这写一下第三第四章的主要内容</h3><h4 id="1-_u7B2C_u4E09_u7AE0_u5305_u62EC_u6C47_u603B_u7EDF_u8BA1_u3001_u53EF_u89C6_u5316_u548COLAP"><a href="#1-_u7B2C_u4E09_u7AE0_u5305_u62EC_u6C47_u603B_u7EDF_u8BA1_u3001_u53EF_u89C6_u5316_u548COLAP" class="headerlink" title="1.第三章包括汇总统计、可视化和OLAP"></a>1.第三章包括汇总统计、可视化和OLAP</h4><p>统计的<strong>特征数据点</strong>包括：频数,众数,百分位数,位置度量:均值和中位数,散布度量：极差和方差，绝对平均偏差、中卫偏差，多元统计包括协方差矩阵。</p>
<p>协方差矩阵：<br>$$ covariance(x_i,x_j)=\frac{1}{m-1}\sum_{k=1}^m(x_{ki} - \overline{x_i})(x_{kj}-\overline{x_j}) $$</p>
<p>相关矩阵元素：$ r_{ij} = \frac{covariance(x_i,x_j)}{s_is_j}$</p>
<h4 id="2-_u53EF_u89C6_u5316"><a href="#2-_u53EF_u89C6_u5316" class="headerlink" title="2.可视化"></a>2.可视化</h4><p>目的：1.快速吸收信息 2.充分调用人的领域知识<br>利用一些图示模型：叶箱图、直方图、相对频率直方图、二维直方图、盒状图、饼图、经验累计分布函数、散布图、可视高维</p>
<h4 id="3-OLAP"><a href="#3-OLAP" class="headerlink" title="3.OLAP"></a>3.OLAP</h4><p>计算聚集量、维归约和转轴、切片和切块、上卷下钻</p>
<h3 id="u7B2C_u56DB_u7AE0_uFF1A_u5206_u7C7B_u3001_u51B3_u7B56_u6811_u548C_u6A21_u578B_u8BC4_u4F30"><a href="#u7B2C_u56DB_u7AE0_uFF1A_u5206_u7C7B_u3001_u51B3_u7B56_u6811_u548C_u6A21_u578B_u8BC4_u4F30" class="headerlink" title="第四章：分类、决策树和模型评估"></a>第四章：分类、决策树和模型评估</h3><h4 id="u57FA_u672C_u6982_u5FF5"><a href="#u57FA_u672C_u6982_u5FF5" class="headerlink" title="基本概念"></a>基本概念</h4><p>1.建立决策树<br>Hunt算法，采用贪心选择划分数据的属性<br>关键问题：如何分裂训练数据，如何停止分裂<br>首先是选择最佳划分的度量,用<strong>纯性度量,信息增益</strong>:<br>$$ Entropy(t)= -\sum_{i=0}^{c=1}p(i|t)log_2p(i|t) $$<br>$$ Gini(t)= 1-\sum_{i=0}^{c=1}[p(i|t)]^2 $$<br>$$ Classification_error(t)=1-max_i[p(i|t)] $$<br>归纳算法:</p>
<pre><code>TreeGrowth(E,F)
if stopping_cond(E,F)=true then
    leaf = createNode()
    leaf.label = Classify(E)
    return leaf
else
    root = createNode()
    root.test_cond = find_best_split(E,F)
    make V = {v|v is one of the output in root.test_cond}
    for each v in V do
    E(v) = {e|root.test_cond(e)=v and e in E}
    child = TreeGrowth(E(v) ,F)
    将child作为root派生结点添加到树，将边(root-&gt;child)标记为v
    end for 
end if 
return root
</code></pre><h4 id="2-_u5965_u5361_u59C6_u5243_u5200_uFF1A_u7ED9_u5B9A_u4E24_u4E2A_u76F8_u540C_u6CDB_u5316_u8BEF_u5DEE_u7684_u6A21_u578B_uFF0C_u8F83_u7B80_u5355_u7684_u66F4_u53EF_u53D6_u3002"><a href="#2-_u5965_u5361_u59C6_u5243_u5200_uFF1A_u7ED9_u5B9A_u4E24_u4E2A_u76F8_u540C_u6CDB_u5316_u8BEF_u5DEE_u7684_u6A21_u578B_uFF0C_u8F83_u7B80_u5355_u7684_u66F4_u53EF_u53D6_u3002" class="headerlink" title="2.奥卡姆剃刀：给定两个相同泛化误差的模型，较简单的更可取。"></a>2.奥卡姆剃刀：给定两个相同泛化误差的模型，较简单的更可取。</h4><h4 id="u603B_u7ED3"><a href="#u603B_u7ED3" class="headerlink" title="总结"></a>总结</h4><p>思想：地柜选择一个属性对对象集合的类标进行分类，如果分类某一属性时发现剩下的对象属于同一类，此时不必再选择属性分类，用一个叶节点代表。否则，继续选择下一属性，知道某一分类结果全在一类或没属性为止。根据选择属性顺序可以分为ID3,C4.5.针对决策树归纳中的过分拟合，可以使用先剪枝和后剪枝的方法。</p>
<p>特点：* 找到最优决策树是NP-C</p>
<pre><code>* 采用避免过拟合的方法后，决策树对噪声干扰具有较好鲁棒性。
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[<h3 id="u8FD9_u5199_u4E00_u4E0B_u7B2C_u4E09_u7B2C_u56DB_u7AE0_u7684_u4E3B_u8981_u5185_u5BB9"><a href="#u8FD9_u5199_u4E00_u4E0B_u7B2C_u4E09_u]]>
    </summary>
    
      <category term="读书笔记" scheme="https://csrjtan.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[关于LATEX在MAC上的配置学习]]></title>
    <link href="https://csrjtan.github.io/2015/10/30/study-20151030/"/>
    <id>https://csrjtan.github.io/2015/10/30/study-20151030/</id>
    <published>2015-10-30T09:16:52.000Z</published>
    <updated>2015-10-31T01:40:42.000Z</updated>
    <content type="html"><![CDATA[<h3 id="LATEX_u7684_u5B66_u4E60"><a href="#LATEX_u7684_u5B66_u4E60" class="headerlink" title="LATEX的学习"></a>LATEX的学习</h3><p>今天来到了科大找高中同学科长学习，在万圣节的前夕本是玩心重重，但在香港的生活忙碌又充实，他也准备MID-TERM了，所以转了一圈之后我们来到颇负盛名的科大图书馆学习。这里先爆个照！<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/keda.jpg" alt=""></p>
<h4 id="u5356_u840C_u5B8C_u76F4_u63A5_u4E0A_u603B_u7ED3_uFF1A"><a href="#u5356_u840C_u5B8C_u76F4_u63A5_u4E0A_u603B_u7ED3_uFF1A" class="headerlink" title="卖萌完直接上总结："></a>卖萌完直接上总结：</h4><p>参考了网上挺多的资料，有些是过期失效或者怎么的。<br>步骤：<br>1.安装Sublime Text 2和MacTex</p>
<p>2.复制<a href="http://wbond.net/sublime_packages/package_control/installation" target="_blank" rel="external">http://wbond.net/sublime_packages/package_control/installation</a>里面的灰框代码到`Conttol+``里面，从而安装package_control</p>
<p>3.在ST里<code>Command+Shift+P</code>命令窗口输入<code>Install Package</code></p>
<p>4.装完继续安装<code>LaTeX Tools</code>，重启ST2</p>
<p>5.修改编译和中文环境（这一步我按照做没有成功，后来用另一个方法完成的，如果有相关的改进请提示我）终端运行</p>
<pre><code>sudo tlmgr update --self
sudo tlmgr install latexmk
</code></pre><p>然后在LaTeXTools.sublime-settings里面在builder-settings新增两个配置:</p>
<pre><code>&quot;program&quot; : &quot;xelatex&quot;,
&quot;command&quot; : [&quot;latexmk&quot;, &quot;-cd&quot;, &quot;-e&quot;, &quot;$pdflatex = &apos;xelatex -interaction=nonstopmode -synctex=1 %S %O&apos;&quot;, &quot;-f&quot;, &quot;-pdf&quot;],
</code></pre><p>最后LaTeXTools–Reconfigure LaTeXTools and migragte setttings来重建配置文件。</p>
<p>6.调用系统默认的Review来打开PDF，修改<br>    ~/Library/Application\ Support/Sublime\ Text/Packages/LaTeXTools/jumpToPDF.py,<br>将<code>if plat==&#39;darwub&#39;:</code>后的if-else语句注释，换成<code>subprocess.Popen([&#39;open&#39;]+[pdffile])</code></p>
<p>7.使用以下测试代码，在ST2上新建.tex文件，然后command+B</p>
<pre><code>\documentclass{article}
\usepackage{fontspec, xunicode, xltxtra}  
\setmainfont{Hiragino Sans GB}  
\title{Title}
\author{}

\begin{document}

\maketitle{}

\section{Introduction}

This is where you will write your content. 在这里写上内容。

\end{document}
</code></pre><p>8.最终大功告成！<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/latex1.png" alt=""></p>
<h4 id="u53C2_u8003_u94FE_u63A5_uFF1A"><a href="#u53C2_u8003_u94FE_u63A5_uFF1A" class="headerlink" title="参考链接："></a>参考链接：</h4><p>部署MAC上的Sublime:<a href="http://www.readern.com/sublime-text-latex-chinese-under-mac.html" target="_blank" rel="external">http://www.readern.com/sublime-text-latex-chinese-under-mac.html</a><br>Economistry（英文）：<a href="http://economistry.com/2013/01/installing-and-using-latex-for-mac/" target="_blank" rel="external">http://economistry.com/2013/01/installing-and-using-latex-for-mac/</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<h3 id="LATEX_u7684_u5B66_u4E60"><a href="#LATEX_u7684_u5B66_u4E60" class="headerlink" title="LATEX的学习"></a>LATEX的学习</h3><p>今天来到了科大找高中同学科长学习]]>
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[小组Talk1]]></title>
    <link href="https://csrjtan.github.io/2015/10/28/group-talk1/"/>
    <id>https://csrjtan.github.io/2015/10/28/group-talk1/</id>
    <published>2015-10-28T15:48:09.000Z</published>
    <updated>2015-10-30T02:20:38.000Z</updated>
    <content type="html"><![CDATA[<h2 id="u7406_u5927_u7B2C_u4E00_u6B21_u5C0F_u7EC4REPORT"><a href="#u7406_u5927_u7B2C_u4E00_u6B21_u5C0F_u7EC4REPORT" class="headerlink" title="理大第一次小组REPORT"></a>理大第一次小组REPORT</h2><p>讲讲进来三个月的工作成果和研究进展，分享一下所学内容<br>这里写下日记，总结之余，也将演讲内容在这里过一遍</p>
<h3 id="1-Big_Picture"><a href="#1-Big_Picture" class="headerlink" title="1.Big Picture"></a>1.Big Picture</h3><p>Target:实现无人机的自动导航以及躲避障碍物<br>使用SLAM：机器人从未知地点出发，运动过程中重复观测得到地图特征来定位自身未知和姿态，从而构建增量式地图，达到同时定位和地图构建的目的。<br>避障算法，Bug Algorithm:检测到障碍物，进行闪躲，但依然朝终点靠近<br>这里的起始点和终点乃至地图都可以pre-defined。</p>
<p>由于这里面核心用到的是环境的三维信息，所以Stereo Matching的精度技术显得尤为重要</p>
<h3 id="2-Stereo_Vision"><a href="#2-Stereo_Vision" class="headerlink" title="2.Stereo Vision"></a>2.Stereo Vision</h3><h4 id="2-1_Camera_Calibration"><a href="#2-1_Camera_Calibration" class="headerlink" title="2.1 Camera Calibration"></a>2.1 Camera Calibration</h4><p>这一节可以看之前camera calibration的日志，针孔平面可由相似三角求出，但由于针孔曝光不足，所以一般用透镜，所以引入了透镜畸变误差，建立数学模型解决。然后我们标定的目的主要是为了能求出摄像机的内参数，包括焦距和成像中心点偏移。以及畸变参数，从而对图像进行修正。</p>
<h3 id="2-2_Stereo_Matching"><a href="#2-2_Stereo_Matching" class="headerlink" title="2.2 Stereo Matching"></a>2.2 Stereo Matching</h3><p>这一节可以参考之前stereo matching的日志，图像的深度Z可以通过简单的三角测量求解所得。所以StereoMatching的主要流程就是图像校正、立体匹配、计算深度。其中主要的步骤在于立体匹配：包括了预处理滤波、代价聚合计算、视差计算以及视差精细等步骤。</p>
<h3 id="3-DJI_Developer"><a href="#3-DJI_Developer" class="headerlink" title="3.DJI Developer"></a>3.DJI Developer</h3><h4 id="3-1_Matrix_100"><a href="#3-1_Matrix_100" class="headerlink" title="3.1 Matrix 100"></a>3.1 Matrix 100</h4><p>首先介绍一下M100这个可扩展飞行平台，它主要特点的续航时间长，负重好，飞行稳定性强，搭载的N1飞控可扩展接口和开发设备接口十分多，强大的可扩展性。<br>平台主要提供了Android和Onboard SDK，其中Android SDK主要控制摄像机、电池和云台以及飞行参数，但由于N1连接遥控器再用USB与手机连接的，所以数据传输是不稳定且难以做到实时的，所以这一块一般用于做一些监控拍摄，指派任务的非实时任务为佳。</p>
<p>相对来说OnBoard SDK提供了开放协议从而对飞控进行命令委派，进而对飞行器进行姿态调整以及飞行模式切换等。这需要使用机载设备对采集的环境数据进行实时采集反馈。</p>
<h4 id="3-2_Guidance"><a href="#3-2_Guidance" class="headerlink" title="3.2 Guidance"></a>3.2 Guidance</h4><p>Guidance组件是一个强大的信息采集组件，具有5个超声结合双目的数据采集器，然后官方提供了GUIDANCE的数据采集例程，而且它们发了一个paper在CVPR15的WORKSHOP。</p>
<p>最后，这里提供一下这次GROUP_TALK的PPT。<br><a href="http://pan.baidu.com/s/1i3nRCtJ" target="_blank" rel="external">PPT&amp;数据采集</a><br>密码: d1s4</p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="u7406_u5927_u7B2C_u4E00_u6B21_u5C0F_u7EC4REPORT"><a href="#u7406_u5927_u7B2C_u4E00_u6B21_u5C0F_u7EC4REPORT" class="headerlink" title]]>
    </summary>
    
      <category term="group" scheme="https://csrjtan.github.io/tags/group/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Guidance:A Visual Sensing Platform For Robotic Applications]]></title>
    <link href="https://csrjtan.github.io/2015/10/28/paper-reading-20151028/"/>
    <id>https://csrjtan.github.io/2015/10/28/paper-reading-20151028/</id>
    <published>2015-10-28T12:38:38.000Z</published>
    <updated>2015-10-28T14:13:30.000Z</updated>
    <content type="html"><![CDATA[<h3 id="DJI_u5728CVPR15_u4E0A_u53D1_u7684_u5173_u4E8EGUIDANCE_u7EC4_u4EF6_u7684_u6587_u7AE0"><a href="#DJI_u5728CVPR15_u4E0A_u53D1_u7684_u5173_u4E8EGUIDANCE_u7EC4_u4EF6_u7684_u6587_u7AE0" class="headerlink" title="DJI在CVPR15上发的关于GUIDANCE组件的文章"></a>DJI在CVPR15上发的关于GUIDANCE组件的文章</h3><p>入手了GUIDANCE一周，这里对这篇文章细细品味一下。</p>
<p>由于缺乏一个强大的视觉感知平台，开发了一个易于扩展的数据采集平台。<br>这个论文可以主要参考一下DJI人员开发GUIDANCE时候，内置了什么样的算法。</p>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h3><p>提到了VO（Visual Odometry）的概念，SLAM是VO[1]的扩展。关于SLAM[2][3]，是用于机器人定位位置识别。大部分是单目的，这里也有双目的方法[4]。这里GUIDANCE包括了5个方向的双目，可以提供深度图，超声数据，障碍物距离等内容。接下来，故事的展开分为内置函数，SDK接口以及相关应用</p>
<h3 id="2-_u5185_u7F6E_uFF1AGuidance_u662F_u5347_u7EA7_u7248_u7684_5B6_5D"><a href="#2-_u5185_u7F6E_uFF1AGuidance_u662F_u5347_u7EA7_u7248_u7684_5B6_5D" class="headerlink" title="2.内置：Guidance是升级版的[6]"></a>2.内置：Guidance是升级版的[6]</h3><p><img src="http://7xl4js.com1.z0.glb.clouddn.com/guidance_1.png" alt="SDK_Build_In"><br><a id="more"></a><br><img src="http://7xl4js.com1.z0.glb.clouddn.com/guidance_2.png" alt=""><br>对于Matching Refinement用了[7]</p>
<h3 id="4-_u53EF_u6269_u5C55_u7684_u667A_u80FD_u5E94_u7528_uFF1A"><a href="#4-_u53EF_u6269_u5C55_u7684_u667A_u80FD_u5E94_u7528_uFF1A" class="headerlink" title="4.可扩展的智能应用："></a>4.可扩展的智能应用：</h3><p>1.Autonomous Navigation<br>  拥有高精度的GPS定位<br>2.Visual SLAM[8]<br>  依赖高精度的VO结果<br>3.Depth Based Tracking</p>
<p>为了开发SLAM避障，请阅读下面黑体</p>
<p>[1]C. Forster, M. Pizzoli, and D. Scaramuzza. SVO: Fast semi- direct monocular visual odometry. In ICRA, 2014.<br><strong>[2]J. Engel, T. Schops, and D. Cremers. LSD-SLAM: Large- scale direct monocular SLAM. In ECCV, 2014.<br>[3]A. Davison. Real-time simultaneous localisation and map- ping with a single camera. In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, pages 1403–1410 vol.2, Oct 2003 </strong><br>[4] F. Endres, J. Hess, N. Engelhard, J. Sturm, D. Cremers, and W. Burgard. An evaluation of the RGB-D SLAM system. In Robotics and Automation (ICRA), 2012 IEEE International Conference on, pages 1691–1696, May 2012.<br>[5]D. Honegger, L. Meier, P. Tanskanen, and M. Pollefeys. An open source and open hardware embedded metric optical flow cmos camera for indoor and outdoor applications. In Robotics and Automation (ICRA), IEEE International Con- ference on, pages 1736–1741, May 2013.<br>[6]G. Zhou, A. Liu, K. Yang, T. Wang, and Z. Li. An embedded solution to visual mapping for consumer drones. In Computer Vision and Pattern Recognition Workshops (CVPRW), IEEE Conference on, pages 670–675, 2014.</p>
<p>[7]B. D. Lucas and T. Kanade. An iterative image registration technique with an application to stereo vision. In Proceed- ings of the 7th International Joint Conference on Artificial<br>Intelligence - Volume 2, pages 674–679, 1981<br><strong>[8]G. Klein and D. Murray. Parallel tracking and mapping for small AR workspaces. In Sixth IEEE and ACM Internation- al Symposium on Mixed and Augmented Reality (ISMAR), 2007.</strong></p>
]]></content>
    <summary type="html">
    <![CDATA[<h3 id="DJI_u5728CVPR15_u4E0A_u53D1_u7684_u5173_u4E8EGUIDANCE_u7EC4_u4EF6_u7684_u6587_u7AE0"><a href="#DJI_u5728CVPR15_u4E0A_u53D1_u7684_u5173_u4E8EGUIDANCE_u7EC4_u4EF6_u7684_u6587_u7AE0" class="headerlink" title="DJI在CVPR15上发的关于GUIDANCE组件的文章"></a>DJI在CVPR15上发的关于GUIDANCE组件的文章</h3><p>入手了GUIDANCE一周，这里对这篇文章细细品味一下。</p>
<p>由于缺乏一个强大的视觉感知平台，开发了一个易于扩展的数据采集平台。<br>这个论文可以主要参考一下DJI人员开发GUIDANCE时候，内置了什么样的算法。</p>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h3><p>提到了VO（Visual Odometry）的概念，SLAM是VO[1]的扩展。关于SLAM[2][3]，是用于机器人定位位置识别。大部分是单目的，这里也有双目的方法[4]。这里GUIDANCE包括了5个方向的双目，可以提供深度图，超声数据，障碍物距离等内容。接下来，故事的展开分为内置函数，SDK接口以及相关应用</p>
<h3 id="2-_u5185_u7F6E_uFF1AGuidance_u662F_u5347_u7EA7_u7248_u7684_5B6_5D"><a href="#2-_u5185_u7F6E_uFF1AGuidance_u662F_u5347_u7EA7_u7248_u7684_5B6_5D" class="headerlink" title="2.内置：Guidance是升级版的[6]"></a>2.内置：Guidance是升级版的[6]</h3><p><img src="http://7xl4js.com1.z0.glb.clouddn.com/guidance_1.png" alt="SDK_Build_In"><br>]]>
    
    </summary>
    
      <category term="paper" scheme="https://csrjtan.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[摄像头校正（Camera Calibration）]]></title>
    <link href="https://csrjtan.github.io/2015/10/27/camera-calibration/"/>
    <id>https://csrjtan.github.io/2015/10/27/camera-calibration/</id>
    <published>2015-10-27T02:29:57.000Z</published>
    <updated>2015-10-27T15:10:26.000Z</updated>
    <content type="html"><![CDATA[<h3 id="u4E3A_u4E86_u603B_u7ED3_u4E00_u4E0B_u8FDB_u5165_u7406_u5927_u4EE5_u6765_u5BF9_u4E8EStereo_Vision_u7684_u5B66_u4E60_uFF0C_u8FD9_u91CC_u5206_u4EAB_u4E00_u4E0B_u5BF9_u4E8ECamera_Calibration_u7684_u5B66_u4E60"><a href="#u4E3A_u4E86_u603B_u7ED3_u4E00_u4E0B_u8FDB_u5165_u7406_u5927_u4EE5_u6765_u5BF9_u4E8EStereo_Vision_u7684_u5B66_u4E60_uFF0C_u8FD9_u91CC_u5206_u4EAB_u4E00_u4E0B_u5BF9_u4E8ECamera_Calibration_u7684_u5B66_u4E60" class="headerlink" title="为了总结一下进入理大以来对于Stereo Vision的学习，这里分享一下对于Camera Calibration的学习"></a>为了总结一下进入理大以来对于Stereo Vision的学习，这里分享一下对于Camera Calibration的学习</h3><p>首先分享一下好的资源<a href="http://wycwang.blogspot.hk/2012/09/camera-calibration-part-1-camera-model.html" target="_blank" rel="external">Richard’s blog</a> 《学习opencv》的第十一章，照相机匹配，还有张正友的经典论文。</p>
<h3 id="1-_u9488_u5B54_u6A21_u578B_28Pinhole_model_29"><a href="#1-_u9488_u5B54_u6A21_u578B_28Pinhole_model_29" class="headerlink" title="1.针孔模型(Pinhole model)"></a>1.针孔模型(Pinhole model)</h3><p><img src="http://7xl4js.com1.z0.glb.clouddn.com/calibration1.png" alt=""></p>
<p>从高中物理的针孔成像定理以及相似三角形定义可得,Z为到针孔距离，f为焦距:<br>$$-x=f\frac{X}{Z}$$</p>
<p>换一个方式，由于人眼看到的并非是倒像，其实可以如下图一样，将成像平面前移到眼睛前方，或者进行一个重新对应成像的过程，从而把倒像恢复，但大小不变的。<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/calibration2.png" alt=""><br>$$x=f\frac{X}{Z}$$</p>
<p>为了使得影像中心没有偏差，加入位移参数$C_x和C_y$，另外成像平面的像素长宽的比例不一样，所以可以加入$s_x和s_y$调整。但这是可以和焦距融合的，所以最终得到公式：<br>$$q=M* Q$$<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/calibration3.png" alt=""><br>$f_x ，f_y ，c_x ，c_y$为摄像机的内参数，一旦标定是固定不变的。</p>
<h3 id="2-_u900F_u955C_u6A21_u578B"><a href="#2-_u900F_u955C_u6A21_u578B" class="headerlink" title="2.透镜模型"></a>2.透镜模型</h3><p>由于针孔对光线的采集不足，需要长曝光，这是不可取的。所以选用透镜，但这回带来透镜畸变，主要是径向（Radial）和切向(Tangent)畸变<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/calibration4.png" alt=""><br>校正选用的Model如下，其中径向由泰勒展开的前三个偶数项（由于中心对称的偶函数），而切向的恢复拟合模型可以参考详细的数学方法，这里不关注。</p>
<p><img src="http://7xl4js.com1.z0.glb.clouddn.com/calibration5.png" alt="校正radial"></p>
<p><img src="http://7xl4js.com1.z0.glb.clouddn.com/calibration6.png" alt="校正tangent"><br>这里k1,k2,p1,p2,k3成为摄像机的形变参数(Distortion parameteres)</p>
<h3 id="3-Calibration"><a href="#3-Calibration" class="headerlink" title="3.Calibration"></a>3.Calibration</h3><p>外部参数是用来将世界坐标系和Camera坐标系之间的坐标系统进行转换。通过3个维度的旋转和平移可得：<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/calibration7.png" alt=""></p>
<h3 id="4-_u5F20_u6B63_u53CB_u65B9_u6CD5"><a href="#4-_u5F20_u6B63_u53CB_u65B9_u6CD5" class="headerlink" title="4.张正友方法"></a>4.张正友方法</h3><p>由于OpenCV以及matlab所使用的方法都是基于张正友的方法，这里介绍一下，其特点是校正方法的弹性以及校正物件比较容易，为一个平面物件。<br>由于校正物体为平面物体，成像也是一个平面物体，而平面物体上的点Transform到另一个平面上，这在CV领域理称为Planar Homography<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/calibration8.png" alt="Planar Homography"><br>基于如此假设，减少一个Z轴上的旋转向量，得到的映射方程可以简化为如下：<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/calibration9.png" alt=""><br>通过$H=A*[r_1 r_2 t]$代入可得以下解：<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/calibration10.png" alt=""></p>
<h4 id="u5982_u6B64_uFF0C_u6211_u4EEC_u6709Homography_Constrain_u7684_u7EA6_u675F"><a href="#u5982_u6B64_uFF0C_u6211_u4EEC_u6709Homography_Constrain_u7684_u7EA6_u675F" class="headerlink" title="如此，我们有Homography Constrain的约束"></a>如此，我们有Homography Constrain的约束</h4><p>$r_1 r_2$为相互垂直的旋转向量，且大小相同,即$r_1^T*r_2=0$以及$||r_1||=||r_2||和r_1^T*r_1=r_2^T*r_2$从而代入上面的式子进行矩阵求解，最终得到如下结果：<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/calibration11.png" alt=""><br>其中$B=A^{-T}A^{-1}$，B为对称矩阵，所以只需要知道6个矩阵元素就好。由此推出要求得一个解，至少需要3幅图像的6对已知对应点。（多多益善）<br>此外，由于$r_1 和 r_2$不一定相互垂直，所以有时需要对R进行奇异值分解。</p>
<p>有解的前提是2NK&gt;=6K+4,即是(N-3)K&gt;=2</p>
<h3 id="4-OpenCV_u7684_u76F8_u5173_u51FD_u6570"><a href="#4-OpenCV_u7684_u76F8_u5173_u51FD_u6570" class="headerlink" title="4.OpenCV的相关函数"></a>4.OpenCV的相关函数</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//寻找棋盘角点</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cvfindChessboardCorners</span><span class="params">(</span><br><span class="line">	<span class="keyword">const</span> <span class="keyword">void</span>* image,</span><br><span class="line">	CvSize 		pattern_size,</span><br><span class="line">	CvPoint2D32f*	corners,</span><br><span class="line">	<span class="keyword">int</span>*		corner_count = <span class="literal">NULL</span></span><br><span class="line">	<span class="keyword">int</span>		flags = CV_CALIB_V_ADAPTIVE_THRESH</span><br><span class="line">	)</span></span>;</span><br><span class="line"><span class="comment">//画出找到的角点</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvDrawChessboardCorner</span><span class="params">(</span><br><span class="line">	CvArr*	image,</span><br><span class="line">	CvSize	pattern_size,</span><br><span class="line">	CvPoint2D32f* corners,</span><br><span class="line">	<span class="keyword">int</span> count,</span><br><span class="line">	<span class="keyword">int</span> pattern_was_found</span><br><span class="line">	)</span></span>;</span><br><span class="line"><span class="comment">//摄像机标定</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cvCalibrateCamera2</span><span class="params">(</span><br><span class="line">	CvMat* object_points,</span><br><span class="line">	CvMat* image_points,</span><br><span class="line">	<span class="keyword">int</span>* point_counts,</span><br><span class="line">	CvSize image_size,</span><br><span class="line">	CvMat* intrinsic_matrix,</span><br><span class="line">	CvMat* distortion_coeffs,</span><br><span class="line">	CvMat* rotation_vectors = <span class="literal">NULL</span>,</span><br><span class="line">	CvMat* translation_vectors = <span class="literal">NULL</span>,</span><br><span class="line">	<span class="keyword">int</span> flags = <span class="number">0</span></span><br><span class="line">	)</span></span>;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>计算完内参数后，也由单应性条件计算得到外参数</p>
]]></content>
    <summary type="html">
    <![CDATA[<h3 id="u4E3A_u4E86_u603B_u7ED3_u4E00_u4E0B_u8FDB_u5165_u7406_u5927_u4EE5_u6765_u5BF9_u4E8EStereo_Vision_u7684_u5B66_u4E60_uFF0C_u8FD9_u91CC]]>
    </summary>
    
      <category term="Stereo Vision" scheme="https://csrjtan.github.io/tags/Stereo-Vision/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Small Vision Systems :Hardware and Implementation]]></title>
    <link href="https://csrjtan.github.io/2015/10/25/paper-reading-20151025/"/>
    <id>https://csrjtan.github.io/2015/10/25/paper-reading-20151025/</id>
    <published>2015-10-25T12:50:44.000Z</published>
    <updated>2015-10-27T04:32:37.000Z</updated>
    <content type="html"><![CDATA[<p><strong>Author</strong>:Kurt Konolige<br>这篇文章是opencv里面的StereoMatchingBM算法的参考算法，所以特意找出来研读一番，虽然是1998的产物，但还是值得一看的，它实际有效地用于移动机器人。它的特点是快，实时。但当参数不Tuned的时候，效果不佳。所以需要进行良好的标定和参数的调整。对于一张480*320的32位深度图计算只需要30~40毫秒的时间。这与opencv里面的StereoMatchingGC算法形成对比，GC算法耗时长，以上图则需要几秒但恢复精度高。</p>
<h4 id="2-1_Area_Correlation__u8FD9_u7BC7_u6587_u7AE0_u7684StereoMatching_u4E3B_u8981_u5E94_u7528_u5728_u79FB_u52A8_u673A_u5668_u4EBA_u4E0A_uFF0C_u5B83_u7528_u5230_u7684_u533A_u57DF_u5173_u8054_u6B65_u9AA4_u6709_u4E94_uFF1A"><a href="#2-1_Area_Correlation__u8FD9_u7BC7_u6587_u7AE0_u7684StereoMatching_u4E3B_u8981_u5E94_u7528_u5728_u79FB_u52A8_u673A_u5668_u4EBA_u4E0A_uFF0C_u5B83_u7528_u5230_u7684_u533A_u57DF_u5173_u8054_u6B65_u9AA4_u6709_u4E94_uFF1A" class="headerlink" title="2.1 Area Correlation 这篇文章的StereoMatching主要应用在移动机器人上，它用到的区域关联步骤有五："></a>2.1 Area Correlation 这篇文章的StereoMatching主要应用在移动机器人上，它用到的区域关联步骤有五：</h4><p>1.Geometry correction:图像的几何和扭曲的恢复调整<br>2.Image Transform:灰度图像素值的去误差调整<br>3.Area Correlation:通过Searching Window区域关联匹配<br>4.Exrema Extraction:最佳点提取，用于生成深度图<br>5.Post-filtering:深度图后处理，去噪滤波等,去除误匹配点，如重复和平滑区域的无匹配等</p>
<h4 id="2-2_Epipolar_Geometry"><a href="#2-2_Epipolar_Geometry" class="headerlink" title="2.2 Epipolar Geometry"></a>2.2 Epipolar Geometry</h4><p>极平面约束导致，p从参照图映射到匹配图，肯定在极曲线(epipolar curves)上。假若图片是理想映射，则该epipolar curves是一条直线。为了达到这样的理想条件，为此，确保图像在同一平面，扫描线和图像中心水平对齐，共同拥有相同的焦距，最好还需要知道内外参数。所以照相机的标定起到重要作用。</p>
<p>实际应用中，不断计算标定不实际，只能确定内参数后，再根据偏差进行自动标定。</p>
<h3 id="3-_u8BE5_u8BBA_u6587_u7CFB_u7EDF_u7684_u7B97_u6CD5"><a href="#3-_u8BE5_u8BBA_u6587_u7CFB_u7EDF_u7684_u7B97_u6CD5" class="headerlink" title="3.该论文系统的算法"></a>3.该论文系统的算法</h3><p>1.LOG滤波去噪，用L1绝对值比较<br>2.进行深度搜索<br>3.后处理，去除奇异点，使用左右和右左校验<br>4.使用X4的插值</p>
<p>作者还对比了使用标定照相机内参后，得到的深度图更为清晰稳定。而针对left/right匹配时，寻找对应最优点采用了hill-climbing和遍历周围最小空间，进行贪心选择。最后对比了一系列StereoMatching组合方法的精度、时间、耗能的区别。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><strong>Author</strong>:Kurt Konolige<br>这篇文章是opencv里面的StereoMatchingBM算法的参考算法，所以特意找出来研读一番，虽然是1998的产物，但还是值得一看的，它实际有效地用于移动机器人。它的特点是快，实时。但当]]>
    </summary>
    
      <category term="paper" scheme="https://csrjtan.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[数据挖掘导论（1）]]></title>
    <link href="https://csrjtan.github.io/2015/10/25/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AF%BC%E8%AE%BA%EF%BC%881%EF%BC%89/"/>
    <id>https://csrjtan.github.io/2015/10/25/数据挖掘导论（1）/</id>
    <published>2015-10-25T03:06:29.000Z</published>
    <updated>2015-10-25T09:17:55.000Z</updated>
    <content type="html"><![CDATA[<h3 id="u5468_u672B_u548C_u4F55_u5973_u795E_u4E00_u8D77_u8BFB_u4E66_uFF0C_u5979_u6700_u8FD1_u5B66_u6570_u636E_u6316_u6398_uFF0C_u6240_u4EE5_u6211_u5C31_u966A_u8BFB_uFF0C_u8FD9_u91CC_u8BB0_u5F55_u4E00_u4E0B_u8BFB_u4E66_u5FC3_u5F97_u3002"><a href="#u5468_u672B_u548C_u4F55_u5973_u795E_u4E00_u8D77_u8BFB_u4E66_uFF0C_u5979_u6700_u8FD1_u5B66_u6570_u636E_u6316_u6398_uFF0C_u6240_u4EE5_u6211_u5C31_u966A_u8BFB_uFF0C_u8FD9_u91CC_u8BB0_u5F55_u4E00_u4E0B_u8BFB_u4E66_u5FC3_u5F97_u3002" class="headerlink" title="周末和何女神一起读书，她最近学数据挖掘，所以我就陪读，这里记录一下读书心得。"></a>周末和何女神一起读书，她最近学数据挖掘，所以我就陪读，这里记录一下读书心得。</h3><p>首先，这本书应该是数据挖掘领域的经典，可以找到网上的数据挖掘导论PDF<br>作者是Michingan State U的Associate Professor <a href="http://www.cse.msu.edu/~ptan/" target="_blank" rel="external">Pang-Ning Tan</a>、University of Minnesota的<a href="http://www-users.cs.umn.edu/~kumar/" target="_blank" rel="external">Vipin Kumar</a></p>
<p>好，马上开始正题：</p>
<h4 id="u4E66_u672C_u5185_u5BB9_u4E0E_u7EC4_u7EC7"><a href="#u4E66_u672C_u5185_u5BB9_u4E0E_u7EC4_u7EC7" class="headerlink" title="书本内容与组织"></a>书本内容与组织</h4><p>第一章，绪论<br>第二章，讨论数据基本类型、质量、预处理及相似性和相异性度量<br>第三章，关于数据数据探查<br>第四、五章，关于分类<br>第六、七章，讨论关联分析<br>第八、九章，讨论聚类分析<br>第十章，讨论数据挖掘的异常检测<br><a id="more"></a></p>
<h3 id="u7B2C_u4E8C_u7AE0__u6570_u636E"><a href="#u7B2C_u4E8C_u7AE0__u6570_u636E" class="headerlink" title="第二章 数据"></a>第二章 数据</h3><p>1.1 属性Attri,对象的性质和特征。测量标度是属性的数值、符号甚至关联规则（函数）。主要是将物理世界的连续量，量化离散成适当的、计算机可以处理的数据。</p>
<p>1.2 数据集的特性：维度高、稀疏性、适当分辨率</p>
<p>2.1 数据记录的方式：矩阵、稀疏矩阵、基于图形的数据、有序数据、时序数据、空间数据</p>
<p>2.2 数据质量，受到测量误差和收集错误影响，包括噪声、精度、离群点、遗漏值、重复值以及不一致值。数据集之于应用也会受到时效性、相关性的。</p>
<p>2.3 数据预处理：<strong>聚集、抽样、维归约、特征子集选择、特征创建、离散化和二元化、变量变换</strong></p>
<p>聚集动机：减少内存和处理时间、使用高层的数据视图起到了范围和<strong>标度转换</strong>的作用</p>
<p>抽样：抽取有代表性有效的数据样本，方法有单随机（无放回和有放回）抽样和分层抽样，为了确定适合大小的样本容量，还可以进行自适应或者渐进的抽样。</p>
<p>维归约：就是降维，将旧属性合并成新属性，避免维度灾难。可以用线性代数方法，将<strong>高维空间投影到低维空间</strong>，比如PCA找出主成分并利用正交特性捕捉数据最大变差，SVD也相似。</p>
<p><strong>特征子集选择</strong>：嵌入方法，嵌入到算法模型，算法来决定特征属性。与嵌入相反的过滤方法，使算法与特征选择相关度小。包装方法来找到最佳属性子集。或者直接使用特征加权。</p>
<p>特征创建：1.特征提取 2.映射到新空间（傅里叶变换） 3.构造特征</p>
<p>二元化：将属性转换成二元属性<br>离散化：非监督离散化和监督离散化，<strong>基于信息熵</strong>的方法较佳<br>$$ e_i =- \sum_{j=1}^k p_{ij}log_2p_{ij} $$<br>定义$m_{ij}$是区间i中类j的值的个数，则$p_{ij}=m_{ij}/m_i$是第i个区间种类j的概率<br>区间的熵是区间纯度的度量，如果区间只有1个值，则熵为0，不影响总熵。如果一个区间中值类出现频率相等（区间不纯，含信息多），则熵最大。决策树的属性选取与此有相似的思想。</p>
<p>变量变换：1.简单函数：将不具有高斯分布的数据变换成高斯分布或者压缩大的数值（注意是否变换导致数据集特性的丢失，思考以下问题：如需要保序么？作用到全部值？变换对0~1之间的值有如何影响？）2.规范化：例如以0为均值，1为标准差。通常用于方便计算，并使数据集具有特定的特质</p>
<p>相似性和相异性：用邻近度表示相似或相异，包括相关和欧几距离，Jaccard和余弦相似度量，前两者使用时间序列稠密数据或二维点，后两者适用文档、图像等稀疏数据。一般得到邻近度后，原始数据便不再需要。<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/datamine_1.png" alt=""><br>通常我们会定义一些距离，如L0、L1、$L_\infty$,它们满足<strong>非负性、对称性和三角不等式</strong>,满足此条件称为度量（metric）,由此可延伸出测度学习（Metric Learning)。其中还可以用集合差，时间之间的距离<br>余弦 $cos(x,y)=\frac{x}{||x||}.\frac{y}{||y||}$</p>
<p>相关性 $corr(x,y)=\frac{covariance(x,y)}{standard\_deviation(x)*standard\_deviation(y)} =\frac{s_xy}{s_xs_y}$</p>
<p>Bregman散度：给定严格凸函数$\phi$,则该函数生成的Bregman散度D(x,y)通过下面公式给出：<br>$$D(x,y)=\phi(x)-\phi(y)-&lt;\Delta\phi(y),(x-y)&gt;$$<br>其中$\Delta\phi(y)$计算$\phi$的梯度，$&lt;\Delta\phi(y),(x-y)&gt;$是$\Delta\phi(y)$和(x-y)的内积。</p>
<p>2.4 邻近度计算问题<br>1.距离度量的标准化和相关性（假定属性相同类型）<br>$$mahalanobis(x,y)=(x-y)\sum^{-1}(x-y)^T$$<br>其中$\sum^{-1}$是数据协方差矩阵的逆，第ij个元素是第i个和第j个属性的协方差。这个是为了减少不同尺度变量的影响，虽然计算量大，但对于属性相关的对象是值得的。<br>2.组合异种属性的相似度<br>3.使用权值<br>$$similarity(x,y)=\frac{\sum_{k=1}^nw_k\delta_ks_k(x,y)}{\sum_{k=1}^n}\delta_k$$<br>其中$\delta=0$，如果第k个属性是非对成熟性，并且两个对象在该属性上的值都是0，或者其中一个对象的第k个属性具有遗漏值。$\delta=1$，则两个对象之间的相似度有以上定义，以上的公式包含了组合异种属性和使用权值的效果。</p>
<p>2.5 关于如何选取正确的邻近性度量<br>选取与数据类型相适应的：稠密和连续的数据，通常使用距离度量。<strong>稀疏数据</strong>常常包含非对称数据，通常使用<strong>忽略0-0匹配</strong>的相似性度量。因为对象相似依赖于共同具有的性质，而不是缺失的性质，若考虑稀疏数据缺失的性质，则它们都高度相似。所以<strong>余弦、Jaccard和广义Jaccard度量</strong>更适合。<br>对于比较时间序列，如果量值重要，则用欧几距离。假若<strong>序列形状</strong>重要，则<strong>考虑相关度</strong>（使用考虑量和级的差异的内置规范化）。</p>
]]></content>
    <summary type="html">
    <![CDATA[<h3 id="u5468_u672B_u548C_u4F55_u5973_u795E_u4E00_u8D77_u8BFB_u4E66_uFF0C_u5979_u6700_u8FD1_u5B66_u6570_u636E_u6316_u6398_uFF0C_u6240_u4EE5_u6211_u5C31_u966A_u8BFB_uFF0C_u8FD9_u91CC_u8BB0_u5F55_u4E00_u4E0B_u8BFB_u4E66_u5FC3_u5F97_u3002"><a href="#u5468_u672B_u548C_u4F55_u5973_u795E_u4E00_u8D77_u8BFB_u4E66_uFF0C_u5979_u6700_u8FD1_u5B66_u6570_u636E_u6316_u6398_uFF0C_u6240_u4EE5_u6211_u5C31_u966A_u8BFB_uFF0C_u8FD9_u91CC_u8BB0_u5F55_u4E00_u4E0B_u8BFB_u4E66_u5FC3_u5F97_u3002" class="headerlink" title="周末和何女神一起读书，她最近学数据挖掘，所以我就陪读，这里记录一下读书心得。"></a>周末和何女神一起读书，她最近学数据挖掘，所以我就陪读，这里记录一下读书心得。</h3><p>首先，这本书应该是数据挖掘领域的经典，可以找到网上的数据挖掘导论PDF<br>作者是Michingan State U的Associate Professor <a href="http://www.cse.msu.edu/~ptan/">Pang-Ning Tan</a>、University of Minnesota的<a href="http://www-users.cs.umn.edu/~kumar/">Vipin Kumar</a></p>
<p>好，马上开始正题：</p>
<h4 id="u4E66_u672C_u5185_u5BB9_u4E0E_u7EC4_u7EC7"><a href="#u4E66_u672C_u5185_u5BB9_u4E0E_u7EC4_u7EC7" class="headerlink" title="书本内容与组织"></a>书本内容与组织</h4><p>第一章，绪论<br>第二章，讨论数据基本类型、质量、预处理及相似性和相异性度量<br>第三章，关于数据数据探查<br>第四、五章，关于分类<br>第六、七章，讨论关联分析<br>第八、九章，讨论聚类分析<br>第十章，讨论数据挖掘的异常检测<br>]]>
    
    </summary>
    
      <category term="读书笔记" scheme="https://csrjtan.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[论文阅读方法和技巧]]></title>
    <link href="https://csrjtan.github.io/2015/10/22/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%96%B9%E6%B3%95/"/>
    <id>https://csrjtan.github.io/2015/10/22/论文阅读方法/</id>
    <published>2015-10-22T08:58:39.000Z</published>
    <updated>2015-10-22T09:01:29.000Z</updated>
    <content type="html"><![CDATA[<h4 id="u5173_u4E8E_u8BBA_u6587_u9605_u8BFB_u7684_u65B9_u6CD5_u548C_u5FC3_u5F97"><a href="#u5173_u4E8E_u8BBA_u6587_u9605_u8BFB_u7684_u65B9_u6CD5_u548C_u5FC3_u5F97" class="headerlink" title="关于论文阅读的方法和心得"></a>关于论文阅读的方法和心得</h4><p><a href="https://www.evernote.com/shard/s608/sh/b2f2b9b0-91a5-4abf-81f4-599d3fa52bb9/01af8a34804f5cd1/res/bda4e9d8-e543-4cfd-9bb1-f42e7a626894/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87.pdf" target="_blank" rel="external">PDF</a></p>
<p><img src="http://7xl4js.com1.z0.glb.clouddn.com/论文阅读.png" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[<h4 id="u5173_u4E8E_u8BBA_u6587_u9605_u8BFB_u7684_u65B9_u6CD5_u548C_u5FC3_u5F97"><a href="#u5173_u4E8E_u8BBA_u6587_u9605_u8BFB_u7684_u65B9_u]]>
    </summary>
    
      <category term="读书笔记" scheme="https://csrjtan.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[代码大全（1）]]></title>
    <link href="https://csrjtan.github.io/2015/10/20/%E4%BB%A3%E7%A0%81%E5%A4%A7%E5%85%A8%EF%BC%88%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B01%EF%BC%89/"/>
    <id>https://csrjtan.github.io/2015/10/20/代码大全（读书笔记1）/</id>
    <published>2015-10-20T05:25:34.000Z</published>
    <updated>2015-10-22T05:09:39.000Z</updated>
    <content type="html"><![CDATA[<h3 id="u6574_u7406_u4E00_u4E0B_u6700_u8FD1_u7684_u8BFB_u4E66_u7B14_u8BB0_uFF0C_u9996_u5148_u5E94_u8BE5_u662F_u300A_u4EE3_u7801_u5927_u5168_u300B_u8FD9_u672C_u4E66_uFF0C_u8FD9_u91CC_u5199_u4E00_u4E0B_u7B2C_u4E00_u90E8_u5206_u201C_u6253_u597D_u57FA_u7840_u201D"><a href="#u6574_u7406_u4E00_u4E0B_u6700_u8FD1_u7684_u8BFB_u4E66_u7B14_u8BB0_uFF0C_u9996_u5148_u5E94_u8BE5_u662F_u300A_u4EE3_u7801_u5927_u5168_u300B_u8FD9_u672C_u4E66_uFF0C_u8FD9_u91CC_u5199_u4E00_u4E0B_u7B2C_u4E00_u90E8_u5206_u201C_u6253_u597D_u57FA_u7840_u201D" class="headerlink" title="整理一下最近的读书笔记，首先应该是《代码大全》这本书，这里写一下第一部分“打好基础”"></a>整理一下最近的读书笔记，首先应该是《代码大全》这本书，这里写一下第一部分“打好基础”</h3><p>最近的工作重心，由CV的识别和3D重建转移了，所以paper读得少了，但不能停。在理大的学习需要自觉，意识到自己在专注上的缺乏，所以需要继续更新博客，完善这个博客来希望自己有所提高。废话少说，直接来读书心得了。<a id="more"></a><br><img src="http://7xl4js.com1.z0.glb.clouddn.com/构建项目.png" alt=""></p>
<h3 id="chap_1-_u8FDB_u5165_u67B6_u6784_u7684_u4E16_u754C"><a href="#chap_1-_u8FDB_u5165_u67B6_u6784_u7684_u4E16_u754C" class="headerlink" title="chap 1.进入架构的世界"></a>chap 1.进入架构的世界</h3><p>软件架构包括：定义问题、需求分析、规划构建、软件架构、<strong>详细设计</strong>、<strong>编码和单元测试</strong>、集成测试、集成、系统测试以及保障维护</p>
<p>作者强调设计和架构是很重要的，不要一上来就抠码，接下来分析如何设计，这对软件质量有实质性的帮助。</p>
<h3 id="chap_2-_u7528_u9690_u55BB_u6765_u5145_u5206_u7406_u89E3_u8F6F_u4EF6_u5F00_u53D1"><a href="#chap_2-_u7528_u9690_u55BB_u6765_u5145_u5206_u7406_u89E3_u8F6F_u4EF6_u5F00_u53D1" class="headerlink" title="chap 2.用隐喻来充分理解软件开发"></a>chap 2.用隐喻来充分理解软件开发</h3><p>软件领域有一些共同的隐喻，用于方便交流，更好地理解抽象概念。</p>
<h3 id="chap_3-_u524D_u671F_u51C6_u5907"><a href="#chap_3-_u524D_u671F_u51C6_u5907" class="headerlink" title="chap 3.前期准备"></a>chap 3.前期准备</h3><p><strong>涉及</strong>：程序组织、主要的类（80/20法则）、数据设计、业务规则、用户界面设计、资源管理、安全性、性能、可伸缩小、互用性、国际化/本地化、输入输出、错误处理、容错性、<br>架构可能性、过度工程、关于“买”还是“造”的决策、关于复用、变更策略、架构总体质量<br>目标：在于降低风险，同时增强系统清晰度可读性</p>
<h3 id="chap_4-_u5173_u952E_u7684_u201C_u6784_u5EFA_u201D_u51B3_u7B56"><a href="#chap_4-_u5173_u952E_u7684_u201C_u6784_u5EFA_u201D_u51B3_u7B56" class="headerlink" title="chap 4.关键的“构建”决策"></a>chap 4.关键的“构建”决策</h3><p>选择编程语言、编程约定（人为规则）、深入一种语言、<strong>发明自己编码约定标准类库以及其他改进措施</strong></p>
<h3 id="chap_5-_u8F6F_u4EF6_u6784_u5EFA_u4E2D_u7684_u8BBE_u8BA1"><a href="#chap_5-_u8F6F_u4EF6_u6784_u5EFA_u4E2D_u7684_u8BBE_u8BA1" class="headerlink" title="chap 5.软件构建中的设计"></a>chap 5.软件构建中的设计</h3><p>正确认识设计：险恶、无章法、确定取舍、调整顺序、不确定、启发式过程</p>
<p>要点：管理复杂度</p>
<p><strong>目标</strong>：最小复杂度、易于维护、松散耦合、可扩展、可充用、高扇入（大量高层类用同一基础类）、低扇出（减少使用不同底层类的个数）、可移植性、精简性、层次性、用标准技术</p>
<p>层次：软件系统-&gt;子系统或包（定义通讯规则来分隔）-&gt; 类 -&gt; 子程序 -&gt;内部编程</p>
<p><em>找出现实世界对象：辩识对象及其方法属性，确定对对象进行的操作，确定各对象对其他对对象的操作，确定对象可见部分，定义接口</em></p>
<p>最后就是将类形成一致的抽象，继而层次化各个类，尽可能达到信息隐藏：1.隐藏复杂度 2.隐藏变化源</p>
<p>为此，我们要明确信息隐藏的障碍和难点：信息过渡分散、有循环的依赖、误把类内数据当全局数据、找出容易改变的区域隔离开（包括业务规则、硬件依赖、IO、困难的设计区域、枚举出状态变量）</p>
<p><strong>保持松散耦合</strong>：常见简单数据参数耦合、简单对象耦合、对象参数耦合、语义耦合（复杂！）</p>
<h4 id="u5B66_u4E60_u4E00_u4E9B_u5E38_u7528_u7684_u8BBE_u8BA1_u6A21_u5F0F_uFF1A_u9002_u914D_u5668_u3001_u6865_u63A5_u3001_u88C5_u9970_u5668_u3001_u5916_u89C2_u3001_u5DE5_u5382_u65B9_u6CD5_u3001_u89C2_u5BDF_u8005_u3001_u5355_u4EF6_u3001_u7B56_u7565_u548C_u6A21_u677F_u65B9_u6CD5"><a href="#u5B66_u4E60_u4E00_u4E9B_u5E38_u7528_u7684_u8BBE_u8BA1_u6A21_u5F0F_uFF1A_u9002_u914D_u5668_u3001_u6865_u63A5_u3001_u88C5_u9970_u5668_u3001_u5916_u89C2_u3001_u5DE5_u5382_u65B9_u6CD5_u3001_u89C2_u5BDF_u8005_u3001_u5355_u4EF6_u3001_u7B56_u7565_u548C_u6A21_u677F_u65B9_u6CD5" class="headerlink" title="学习一些常用的设计模式：适配器、桥接、装饰器、外观、工厂方法、观察者、单件、策略和模板方法"></a>学习一些常用的设计模式：适配器、桥接、装饰器、外观、工厂方法、观察者、单件、策略和模板方法</h4><p>最后需要明确项目的类型来规划设计到哪一个细节阶段，并记录<strong>设计成果</strong>，保留设计挂图</p>
]]></content>
    <summary type="html">
    <![CDATA[<h3 id="u6574_u7406_u4E00_u4E0B_u6700_u8FD1_u7684_u8BFB_u4E66_u7B14_u8BB0_uFF0C_u9996_u5148_u5E94_u8BE5_u662F_u300A_u4EE3_u7801_u5927_u5168_u300B_u8FD9_u672C_u4E66_uFF0C_u8FD9_u91CC_u5199_u4E00_u4E0B_u7B2C_u4E00_u90E8_u5206_u201C_u6253_u597D_u57FA_u7840_u201D"><a href="#u6574_u7406_u4E00_u4E0B_u6700_u8FD1_u7684_u8BFB_u4E66_u7B14_u8BB0_uFF0C_u9996_u5148_u5E94_u8BE5_u662F_u300A_u4EE3_u7801_u5927_u5168_u300B_u8FD9_u672C_u4E66_uFF0C_u8FD9_u91CC_u5199_u4E00_u4E0B_u7B2C_u4E00_u90E8_u5206_u201C_u6253_u597D_u57FA_u7840_u201D" class="headerlink" title="整理一下最近的读书笔记，首先应该是《代码大全》这本书，这里写一下第一部分“打好基础”"></a>整理一下最近的读书笔记，首先应该是《代码大全》这本书，这里写一下第一部分“打好基础”</h3><p>最近的工作重心，由CV的识别和3D重建转移了，所以paper读得少了，但不能停。在理大的学习需要自觉，意识到自己在专注上的缺乏，所以需要继续更新博客，完善这个博客来希望自己有所提高。废话少说，直接来读书心得了。]]>
    
    </summary>
    
      <category term="读书笔记" scheme="https://csrjtan.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ppt_learning_Stereo_Vision]]></title>
    <link href="https://csrjtan.github.io/2015/09/03/ppt-learning-Stereo-Vision/"/>
    <id>https://csrjtan.github.io/2015/09/03/ppt-learning-Stereo-Vision/</id>
    <published>2015-09-03T08:11:25.000Z</published>
    <updated>2015-10-28T14:39:55.000Z</updated>
    <content type="html"><![CDATA[<p><a href="http://vision.deis.unibo.it/~smatt/Seminars/StereoVision.pdf" target="_blank" rel="external">原文链接</a></p>
<h3 id="Stereo_Vision_3AAlgorithms_and_Applications"><a href="#Stereo_Vision_3AAlgorithms_and_Applications" class="headerlink" title="Stereo Vision:Algorithms and Applications"></a>Stereo Vision:Algorithms and Applications</h3><h4 id="Author_3AStefano_Mattoccia"><a href="#Author_3AStefano_Mattoccia" class="headerlink" title="Author:Stefano Mattoccia"></a>Author:<a href="http://vision.deis.unibo.it/~smatt/Site/Home.html" target="_blank" rel="external">Stefano Mattoccia</a></h4><h4 id="Lab_3AUniversity_of_Bologna"><a href="#Lab_3AUniversity_of_Bologna" class="headerlink" title="Lab:University of Bologna"></a>Lab:University of Bologna</h4><p>包括：1.介绍 2.综述 3.匹配算法 4.计算优化 5.硬件实现 6.应用<br><a id="more"></a></p>
<h3 id="1-Intro"><a href="#1-Intro" class="headerlink" title="1.Intro"></a>1.Intro</h3><p>Target:Stereo Vision的目的在于从两个或以上的摄像机获取深度信息</p>
<ul>
<li>双目视觉系统</li>
<li>密度立体算法</li>
<li>立体视觉应用</li>
</ul>
<h4 id="u5B9A_u4E49_3AEpipolar_constraint_28_u6781_u51E0_u4F55_u7EA6_u675F_29"><a href="#u5B9A_u4E49_3AEpipolar_constraint_28_u6781_u51E0_u4F55_u7EA6_u675F_29" class="headerlink" title="定义:Epipolar constraint(极几何约束)"></a>定义:Epipolar constraint(极几何约束)</h4><p><img src="http://7xl4js.com1.z0.glb.clouddn.com/epipolar_constraint.png" alt=""><br>对于图像R，P和Q在同一点上，近的掩盖远的<br>对于图像T，P和Q映在p和q上，红线 $PO_R$ 落在绿线pq的同一平面 $\Pi_T$上，这个称为极约束</p>
<h4 id="u5B9A_u4E49_uFF1A_u89C6_u5DEE_uFF08Disparity_uFF09"><a href="#u5B9A_u4E49_uFF1A_u89C6_u5DEE_uFF08Disparity_uFF09" class="headerlink" title="定义：视差（Disparity）"></a>定义：视差（Disparity）</h4><p><img src="http://7xl4js.com1.z0.glb.clouddn.com/disparity.png" alt=""><br>经过三角形相似性原理可以推导得到<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/dis_res.png" alt=""></p>
<h4 id="u5B9A_u4E49_uFF1A_u53CC_u76EE_u89C6_u754C_uFF08Horopter_uFF09"><a href="#u5B9A_u4E49_uFF1A_u53CC_u76EE_u89C6_u754C_uFF08Horopter_uFF09" class="headerlink" title="定义：双目视界（Horopter）"></a>定义：双目视界（Horopter）</h4><p>The range field of system is constrained by disparity range $[d_{min},d_{max}]$<br>一般可以离散化视差值，较好的离散化是通过subpixel方法可得<br>若用5个离散化的值，则可设置 $[d_{min},d_{min}+4]$</p>
<h3 id="2-Overview_of_stereo_vision_system"><a href="#2-Overview_of_stereo_vision_system" class="headerlink" title="2.Overview of stereo vision system"></a>2.Overview of stereo vision system</h3><p><img src="http://7xl4js.com1.z0.glb.clouddn.com/overview_stereo.png" alt=""></p>
<h4 id="2-1_Calibration_28offline_29"><a href="#2-1_Calibration_28offline_29" class="headerlink" title="2.1 Calibration(offline)"></a>2.1 Calibration(offline)</h4><p>  target:finding parameters of the camera system</p>
<ul>
<li>Intrinsic parameters of two cameras:Focal length,image center,lenses distortion</li>
<li>Extrinsic parameters R and T aligns of two cameras<br>Methods:用10+对已知的立体点匹配，最典型用checkerboard<br>可用Opencv,Matlab<a href="http://www.vision.caltech.edu/bouguetj/calib_doc/" target="_blank" rel="external">[1]</a>,详细的标定方法可参考资料[2,3,4]</li>
</ul>
<h4 id="2-2_Rectification"><a href="#2-2_Rectification" class="headerlink" title="2.2 Rectification"></a>2.2 Rectification</h4><p>  target: Adjust stereo camera in standard form<br>  steps: a) removes lens distortions<br>             b) turns the stereo pair in standard form</p>
<h4 id="2-3_Stereo_Correspondence"><a href="#2-3_Stereo_Correspondence" class="headerlink" title="2.3 Stereo Correspondence"></a>2.3 Stereo Correspondence</h4><p>  target: finding homologous points in stereo pair, generate disparity map</p>
<h4 id="2-4_Triangulation"><a href="#2-4_Triangulation" class="headerlink" title="2.4 Triangulation"></a>2.4 Triangulation</h4><p>  target: Calculate the position of the correspondence in the 3D</p>
<p>  $Z=\frac{b*f}{d}$        $X=Z\frac{x_R}{f}$      $Y=Z\frac{y_R}{f}$</p>
<p>Relevant:</p>
<ul>
<li>Datasets:stereo sequences<br>  <a href="http://vision.deis.unibo.it/~smatt/stereo.htm" target="_blank" rel="external">Including</a> </li>
</ul>
<ul>
<li>calibration parameters</li>
<li>original sequences</li>
<li>rectified sequences</li>
<li>disparity maps</li>
</ul>
<ul>
<li><a href="http://vision.middlebury.edu/stereo/eval/" target="_blank" rel="external">Middlebury stereo evaluation</a><br>提供了一个框架和一个数据集来测试新颖方法的性能和效果基线</li>
</ul>
<p>待克服的难点：The common pitfalls make the stereo correspondence so challenging: Photometric distortions and noise, Specular surfaces, Foreshortening, Perspective distortions, Uniform/Ambiguous regions, Repetitive patterns, Transparent objects, Occlusions and discontinuities</p>
<h3 id="3-The_correspondence_problem"><a href="#3-The_correspondence_problem" class="headerlink" title="3.The correspondence problem"></a>3.The correspondence problem</h3><p>  由于[5]，大部分立体算法基于一下步骤：</p>
<ul>
<li>1.匹配代价计算（Matching cost computation）</li>
<li>2.代价聚合(Cost Aggregation)</li>
<li>3.视差计算/优化(Disparity computation/Optimization)</li>
<li>4.视差精细(Disparity refinement)<br>Local:1-&gt;2-&gt;3    (用WTA策略)<br>Global:1(-&gt;2)-&gt;3   semi-global</li>
</ul>
<h4 id="3-1__u9884_u5904_u7406_uFF080_uFF09"><a href="#3-1__u9884_u5904_u7406_uFF080_uFF09" class="headerlink" title="3.1 预处理（0）"></a>3.1 预处理（0）</h4><p>  典型方法：Laplacian of Gaussian(LoG)滤波[6],邻域均值去除[7],Bilateral Filtering[8]</p>
<p>  <strong>优化</strong>简单的像素比对方法:</p>
<ul>
<li>Local 用窗口像素代价聚合减小 Signal to noise ratio(SNR)</li>
<li>Global 最小化代价函数，优化Pixel-based的代价匹配</li>
</ul>
<p>$$E(d)=E_{data}(d) + E_{smooth}(d)$$</p>
<h4 id="3-2__u5339_u914D_u4EE3_u4EF7_u8BA1_u7B97_281_29"><a href="#3-2__u5339_u914D_u4EE3_u4EF7_u8BA1_u7B97_281_29" class="headerlink" title="3.2 匹配代价计算(1)"></a>3.2 匹配代价计算(1)</h4><p>  3.2.1 单值匹配</p>
<ul>
<li>绝对值差<br>$e(x,y,d) = |I_R(x,y)-I_T(x+d,y)|$</li>
<li>平方差<br>$e(x,y,d) = (I_R(x,y)-I_T(x+d,y))^2$</li>
<li>鲁棒方法<br>限制outliers的影响，如Truncated Absolute Differences(TAD)<br>$e(x,y,d) = min \{ |I_R(x,y)-I_T(x+d,y)|,T \}$<br>3.2.2 区域匹配</li>
<li>绝对和（SAD）<br>$C(x,y,d) = \sum_{x\in S}|I_R(x,y)-I_T(x+d,y)|$</li>
<li>平方差和 （SSD）<br>$C(x,y,d) = \sum_{x\in S}(I_R(x,y)-I_T(x+d,y))^2$</li>
<li>截断绝对差和（STAD)<br>$C(x,y,d) = \sum_{x\in S}\{|I_R(x,y)-I_T(x+d,y)|,T\}$<br>其他一些方法：Normalized Cross Correlation[9], Zero mean Normalized Cross Correlation[10], Gradient based MF[11], Non parametric[12,13], Mutual Information[14]</li>
</ul>
<h4 id="3-3__u4EE3_u4EF7_u805A_u5408_282_29"><a href="#3-3__u4EE3_u4EF7_u805A_u5408_282_29" class="headerlink" title="3.3 代价聚合(2)"></a>3.3 代价聚合(2)</h4><p>  固定窗口(FW):<br>  弊端：1.假设图像正面平行 2.忽略深度不连续性 3.不能解决均匀区域 4.无法解决重复区域<br>  优势：简单，使用，实时，不占内存空间，硬件耗电小<br>  优化方法：1.积分图Integral Images（II 1984） 2.箱过滤Box-Filtering(BF 1981) 3.Single Instruction Multiple Data(SIMD)<br>  <img src="http://7xl4js.com1.z0.glb.clouddn.com/BF1.png" alt=""><br>  <img src="http://7xl4js.com1.z0.glb.clouddn.com/BF2.png" alt=""><br>  两者比较：每个点都需要四次运算，II可以处理不通形状的窗口，但需要更多内存，容易溢出，因为若果图片大小为S，则需要$S^2$。不同的BF可以参看[15]</p>
<p>  3.3.1 一个具体的示例（LIVE DEMO）[16][17]</p>
<pre><code>- 灰度图
  - 预处理：均值提取
  - 代价匹配： 绝对差
  - 聚合代价： 固定窗口（FW）
  - 视差选择： 胜者为王(WTA)
  - 局外点优化
  - 抛弃均匀区域
  - 优化：BF+SIMD
  - 像素插值1/16对于每个像素
  - 实时运行在普通PC上
</code></pre><p>  3.3.2 一些好的方法<br>   Shiftable Windows[18], Multiple Window[19], Variable Window [20], Segmentation based(Assume each segment smoothly)[21], Bilateral Filtering[22], Adaptive Weights[23], Segment Supoort[24], Fast Aggregation[25], Fast Bilateral Stereo(FBS)[26], Locally Consistent(LC) stereo[27]</p>
<h4 id="3-4__u89C6_u5DEE_u8BA1_u7B97/_u4F18_u5316_283_29"><a href="#3-4__u89C6_u5DEE_u8BA1_u7B97/_u4F18_u5316_283_29" class="headerlink" title="3.4 视差计算/优化(3)"></a>3.4 视差计算/优化(3)</h4><p>  目的：寻找最佳的视差点，最小化代价函数<br>  由于是NP-Hard问题，可以用近似的方法求解</p>
<ul>
<li>Graph Cuts[28]</li>
<li>Belief Propagation[29]</li>
<li>Cooperative optimization[30]</li>
<li>一些能量最小化的方法可以参看PAMI期刊文章[31]</li>
<li>Dynamic Programming(DP) [5]</li>
<li>Scanline Optimization(SO) [32]</li>
<li>SO + Support Aggregation [33]</li>
<li>Enforcing Local Consistency of disparity in SO/DP [34]</li>
</ul>
<h4 id="3-5__u89C6_u5DEE_u7CBE_u7EC6_284_29"><a href="#3-5__u89C6_u5DEE_u7CBE_u7EC6_284_29" class="headerlink" title="3.5 视差精细(4)"></a>3.5 视差精细(4)</h4><ul>
<li>匹配算法包括一些outliers，必须识别并纠正</li>
<li>由于细化了像素级别，需要优化视差的精确性<br>3.5.1 Sub-pixel插值<br>计算临近插值最小的值，[35]提出了floating-point free方法<br>3.5.2 图像滤波<br>中值滤波，形态操作，BF<br>3.5.3 双向匹配（Bidirection Matching,BM）<br>用于检测outliers[36],左右匹配差异小于T，一般T为1<br>$$|d_{LR}(x,y) - d_{RL}(x+d_{LR}(x,y),y)|&lt;T$$<br>3.5.4 单向匹配步骤(Single Matching Phase,SMP)[37]<br>3.5.5 分割方法<br>Segmentation based outliers identification and replacement<br>基于两个假设:1.每段分割片的视差变化平滑 2.每个分割面近似在同一平面<br>对于每个切割面都在3D平面,满足 $d(x,y)=ax+by+\gamma$<br>对于这样鲁棒的平面，可以使用方法：RANSAC[38]和Histogram Voting[30]<br>3.5.6 Accurate Localization of borders and occlusions[39]</li>
</ul>
<h3 id="4-_u5E94_u7528"><a href="#4-_u5E94_u7528" class="headerlink" title="4.应用"></a>4.应用</h3><p>  3D跟踪：物体计数、监控轨迹、安保<br>  扫描，2D和3D转化，增强现实 </p>
<p>  列出的下列Reference请耐心仔细阅读，可参考原PPT的概括进行参考阅读。由于PPT是2012更新的，一些2010后的前沿方法请另外查询，从CVPR、PAMI等会议或MiddleBury的测评网站上查询阅读。</p>
<hr>
<p>[1] Jean-Yves Bouguet , Camera Calibration Toolbox for Matlab<br>[2] E. Trucco, A. Verri, Introductory Techniques for 3-D Computer Vision, Prentice Hall, 1998<br>[3] R.I.Hartley, A. Zisserman, Multiple View Geometry in Computer Vision, Cambridge University Press, 2000<br>[4] G. Bradsky, A. Kaehler, Learning Opencv, O’Reilly, 2008<br>[5] D. Scharstein and R. Szeliski, A taxonomy and evaluation of dense two-frame stereo correspondence algorithms<br>Int. Jour. Computer Vision, 47(1/2/3):7–42, 2002<br>[6] T. Kanade, H. Kato, S. Kimura, A. Yoshida, and K. Oda, Development of a Video-Rate Stereo Machine<br>International Robotics and Systems Conference (IROS ‘95), Human Robot Interaction and Cooperative Robots, 1995<br>[7] O. Faugeras, B. Hotz, H. Mathieu, T. Viville, Z. Zhang, P. Fua, E. Thron, L. Moll, G. Berry,<br>Real-time correlation-based stereo: Algorithm. Implementation and Applications, INRIA TR n. 2013, 1993<br>[8] A. Ansar, A. Castano, L. Matthies, Enhanced real-time stereo using bilateral filtering<br>IEEE Conference on Computer Vision and Pattern Recognition 2004<br>[9] S. Mattoccia, F. Tombari, L. Di Stefano, Fast full-search equivalent template matching by Enhanced Bounded<br>Correlation, IEEE Transactions on Image Processing, 17(4), pp 528-538, April 2008<br>[10] L. Di Stefano, S. Mattoccia, F. Tombari, ZNCC-based template matching using Bounded Partial Correlation<br>Pattern Recognition Letters, 16(14), pp 2129-2134, October 2005<br>[11] F. Tombari, L. Di Stefano, S. Mattoccia, A. Galanti, Performance evaluation of robust matching measures<br>3rd International Conference on Computer Vision Theory and Applications (VISAPP 2008)<br>[12] R. Zabih, J John Woodll Non-parametric Local Transforms for Computing Visual Correspondence, ECCV 1994<br>[13] D. N. Bhat, S. K. Nayar, Ordinal measures for visual correspondence, CVPR 1996<br>[14] H. Hirschmüller. Stereo vision in structured environments by consistent semi-global matching.<br>CVPR 2006, PAMI 30(2):328-341, 2008<br>[15] Changming Sun, Recursive Algorithms for Diamond, Hexagon and General Polygonal Shaped Window Operations<br>Pattern Recognition Letters, 27(6):556-566, April 2006<br>[16] L. Di Stefano, M. Marchionni, S. Mattoccia, A fast area-based stereo matching algorithm, Image and Vision Computing,<br>22(12), pp 983-1005, October 2004<br>[17] L. Di Stefano, M. Marchionni, S. Mattoccia, A PC-based real-time stereo vision system, Machine Graphics &amp; Vision,<br>13(3), pp. 197-220, January 2004<br>[18] D. Scharstein and R. Szeliski, A taxonomy and evaluation of dense two-frame stereo correspondence algorithms<br>Int. Jour. Computer Vision, 47(1/2/3):7–42, 2002<br>[19] H. Hirschmuller, P. Innocent, and J. Garibaldi, Real-time correlation-based stereo vision with reduced border errors<br>Int. Journ. of Computer Vision, 47:1–3, 2002<br>[20]  O. Veksler. Fast variable window for stereo correspondence using integral images, In Proc. Conf. on Computer Vision<br>and Pattern Recognition (CVPR 2003), pages 556–561, 2003<br>[21]  M. Gerrits and P. Bekaert. Local Stereo Matching with Segmentation-based Outlier Rejection<br>In Proc. Canadian Conf. on Computer and Robot Vision (CRV 2006), pages 66-66, 2006<br>[22] C. Tomasi and R. Manduchi. Bilateral filtering for gray and color images. In ICCV98, pages 839–846, 1998<br>[23] K. Yoon and I. Kweon. Adaptive support-weight approach for correspondence search IEEE PAMI, 28(4):650–656, 2006<br>[24] F. Tombari, S. Mattoccia, L. Di Stefano, Segmentation-based adaptive support for accurate stereo correspondence IEEE Pacific-Rim Symposium on Image and Video Technology (PSIVT 2007)<br>[25] F. Tombari, S. Mattoccia, L. Di Stefano, E. Addimanda, Near real-time stereo based on effective cost aggregation International Conference on Pattern Recognition (ICPR 2008)<br>[26] S. Mattoccia, S. Giardino,A. Gambini, Accurate and efficient cost aggregation strategy for stereo correspondence<br>based on approximated joint bilateral filtering, Asian Conference on Computer Vision (ACCV2009)<br>[27] S. Mattoccia, A locally global approach to stereo correspondence, 3D Digital Imaging and Modeling (3DIM2009)<br>[28] V. Kolmogorov and R. Zabih, Computing visual correspondence with occlusions using graph cuts, ICCV 2001<br>[29] A. Klaus, M. Sormann and K. Karner, Segment-based stereo matching using belief propagation and a self-adapting<br>dissimilarity measure, ICPR 2006<br>[30] Z. Wang and Z. Zheng, A region based stereo matching algorithm using cooperative optimization, CVPR 2008<br>[31] R.Szeliski, R. Zabih, D. Scharstein, O. Veksler, V. Kolmogorov, A. Agarwala, M. Tappen, C. Rother, A Comparative<br>Study of Energy Minimization Methods for Markov Random Fields with Smoothness-Based Priors, IEEE Transactions on<br>Pattern Analysis and Machine Intelligence, 30, 6, June 2008, pp 1068-1080<br>[32] H. Hirschmüller. Stereo vision in structured environments by consistent semi-global matching.<br>CVPR 2006, PAMI 30(2):328-341, 2008<br>[33] S. Mattoccia, F. Tombari, and L. Di Stefano, Stereo vision enabling precise border localization within a scanline<br>optimization framework, ACCV 2007<br>[34] S. Mattoccia, Improving the accuracy of fast dense stereo correspondence algorithms by enforcing local consistency of disparity fields, 3DPVT2010<br>[35] L. Di Stefano, S. Mattoccia, Real-time stereo within the VIDET project Real-Time Imaging, 8(5), pp. 439-453, Oct. 2002<br>[36] P. Fua, Combining stereo and monocular information to compute dense depth maps that preserve depth discontinuities 12th. Int. Joint Conf. on Artificial Intelligence, pp 1292–1298, 1993<br>[37] L. Di Stefano, M. Marchionni, S. Mattoccia, A fast area-based stereo matching algorithm, Image and Vision Computing,<br>22(12), pp 983-1005, October 2004<br>[38] M. A. Fischler and R. C. Bolles, Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image<br>Analysis and Automated Cartography, Comm. of the ACM 24: 381–395, June 1981<br>[39] S. Mattoccia, F. Tombari, and L. Di Stefano, Stereo vision enabling precise border localization within a scanline optimization framework, ACCV 2007</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="http://vision.deis.unibo.it/~smatt/Seminars/StereoVision.pdf">原文链接</a></p>
<h3 id="Stereo_Vision_3AAlgorithms_and_Applications"><a href="#Stereo_Vision_3AAlgorithms_and_Applications" class="headerlink" title="Stereo Vision:Algorithms and Applications"></a>Stereo Vision:Algorithms and Applications</h3><h4 id="Author_3AStefano_Mattoccia"><a href="#Author_3AStefano_Mattoccia" class="headerlink" title="Author:Stefano Mattoccia"></a>Author:<a href="http://vision.deis.unibo.it/~smatt/Site/Home.html">Stefano Mattoccia</a></h4><h4 id="Lab_3AUniversity_of_Bologna"><a href="#Lab_3AUniversity_of_Bologna" class="headerlink" title="Lab:University of Bologna"></a>Lab:University of Bologna</h4><p>包括：1.介绍 2.综述 3.匹配算法 4.计算优化 5.硬件实现 6.应用<br>]]>
    
    </summary>
    
      <category term="paper" scheme="https://csrjtan.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Accuracy and Resolution of Kinect Depth Data for Indoor Mapping Applications]]></title>
    <link href="https://csrjtan.github.io/2015/08/27/paper-reading-20150826/"/>
    <id>https://csrjtan.github.io/2015/08/27/paper-reading-20150826/</id>
    <published>2015-08-27T01:57:19.000Z</published>
    <updated>2015-10-28T14:40:00.000Z</updated>
    <content type="html"><![CDATA[<p><strong>Author</strong>:Kourosh Khoshelham and Sander Oude Elberink</p>
<p><strong>Abstract</strong>: Discuss the calibration of Kinect sensor and analysis it based on the mathematical model of depth measurement from disparity.<br><a id="more"></a><br>1.<strong>Introduce</strong><br>   sec 1 介绍全文<br>   sec 2 介绍原理和模型<br>   sec 3 介绍误差影响要素<br>   sec 4 实验结果和分析<br>   sec 5 总结和remark</p>
<p>2.<strong>深度测量 with triangulation</strong></p>
<p>Triangulation Methods of Kinect [<a href="https://www.google.com/patents/US8150142" target="_blank" rel="external">1</a>] laser source emits single beam which split into multiple beams by diffraction.</p>
<p>The <strong>reference pattern</strong> is obtained by capturing a plane at a known distance.</p>
<p><strong>Disparity</strong>:The shifts are measured for all speckles by a simple image correlation procedure,which yield a disparity image.</p>
<p>以下图为例，分析<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/triangulation.png" alt=""><br>o为参考平面，k为目标物体位置。由laser projector（不动）发出多个光束，ir camera调整角度使接收到的目标反射光斑落在参考平面上，得到偏移位置d.</p>
<p>假设完成相机标定后，则 $ Z_0 $, f, b 已知，d也可以获得。<br>根据数学三角原理得到：</p>
<p>$$ \frac{D}{b}=\frac{Z_0-Z_k}{Z_0} $$<br>$$\frac{d}{f}=\frac{D}{Z_k}$$<br>推导化简得:<br>            $ Z_k = \frac{Z_0}{1+\frac{Z_0}{fb}d} $ （1）<br>假设坐标系正交，得到Z轴坐标距离后，由于f定义了关于点的图像规模（imaging scale）所以得到物体的光点坐标为<br>$$ X_k=-\frac{Z_k}{f}(x_k-x_0+\delta x)$$<br>$$ Y_k=-\frac{Z_k}{f}(y_k-y_0+\delta y)$$<br>其中$x_k$ 和 $y_k$为图像坐标点，$x_0$ 和 $y_0$为坐标原点，$\delta x$ 和$\delta y$ 是镜头扭曲误差校正系数</p>
<p>3.相机标定<br>我们需要求得标定的参数变量如下：<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/parameters.png" alt=""><br>前三个通过红外照相机标准标定可得<br>由于USB带宽所限，标定use reduced infrared images instead of actual sensor。<br>Disparity measurement 采样为0~2047 11bits，则 d = md’+n。 m, n为线性化参数。<br>代入 公式（1）可得<br>$$Z^{-1}_k = (\frac{m}{fb}d’ +(Z^{-1}_0 + \frac {n}{fb})) $$<br>通过测得(x,y,d’)可以映射到世界坐标系的(X,Y,Z)</p>
<p>4.加入颜色元素</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><strong>Author</strong>:Kourosh Khoshelham and Sander Oude Elberink</p>
<p><strong>Abstract</strong>: Discuss the calibration of Kinect sensor and analysis it based on the mathematical model of depth measurement from disparity.<br>]]>
    
    </summary>
    
      <category term="paper" scheme="https://csrjtan.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[论文阅读《Enhanced Computer Vision with Microsoft Kinect Sensor:A Review》]]></title>
    <link href="https://csrjtan.github.io/2015/08/18/paper-reading-20150816/"/>
    <id>https://csrjtan.github.io/2015/08/18/paper-reading-20150816/</id>
    <published>2015-08-18T07:26:29.000Z</published>
    <updated>2015-10-28T14:40:03.000Z</updated>
    <content type="html"><![CDATA[<p>原文：<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6547194" target="_blank" rel="external">Click</a><br>作者：Jungong Han,Ling Shao</p>
<h3 id="u8FD9_u4E2A_u8BBA_u6587_u4E3B_u8981_u7B80_u4ECB_u4E00_u4E9B_u57FA_u4E8EKinect_u7684_u8BA1_u7B97_u673A_u89C6_u89C9_u7B97_u6CD5_u548C_u5E94_u7528_uFF0C_u6DB5_u76D6_u4E86_u5305_u62EC_u6DF1_u5EA6_u4FE1_u606F_u7684_u9884_u5904_u7406_uFF0CKinect_u7684_u7CBE_u786E_u6807_u5B9A_uFF1B_u7269_u4F53_u8DDF_u8E2A_u548C_u8BC6_u522B_uFF1B_u4EBA_u7C7B_u6D3B_u52A8_u5206_u6790_u548C_u624B_u52BF_u5206_u6790_u4EE5_u53CA_u5BA4_u51853D_u5339_u914D_u3002"><a href="#u8FD9_u4E2A_u8BBA_u6587_u4E3B_u8981_u7B80_u4ECB_u4E00_u4E9B_u57FA_u4E8EKinect_u7684_u8BA1_u7B97_u673A_u89C6_u89C9_u7B97_u6CD5_u548C_u5E94_u7528_uFF0C_u6DB5_u76D6_u4E86_u5305_u62EC_u6DF1_u5EA6_u4FE1_u606F_u7684_u9884_u5904_u7406_uFF0CKinect_u7684_u7CBE_u786E_u6807_u5B9A_uFF1B_u7269_u4F53_u8DDF_u8E2A_u548C_u8BC6_u522B_uFF1B_u4EBA_u7C7B_u6D3B_u52A8_u5206_u6790_u548C_u624B_u52BF_u5206_u6790_u4EE5_u53CA_u5BA4_u51853D_u5339_u914D_u3002" class="headerlink" title="这个论文主要简介一些基于Kinect的计算机视觉算法和应用，涵盖了包括深度信息的预处理，Kinect的精确标定；物体跟踪和识别；人类活动分析和手势分析以及室内3D匹配。"></a>这个论文主要简介一些基于Kinect的计算机视觉算法和应用，涵盖了包括深度信息的预处理，Kinect的精确标定；物体跟踪和识别；人类活动分析和手势分析以及室内3D匹配。</h3><a id="more"></a>
<p>这是一篇全面介绍RGB-D 技术进行CV研究的综述，里面可以挖掘很多可以细抠的论文。<br>主要用于入门延伸阅读所用。</p>
<p>介绍Kinect实现和技术原理的论文<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6190806" target="_blank" rel="external">《Microsoft Kinect Sensor and Its Effect》</a></p>
<p>关于深度信息的光线三维成像原理查看论文<a href="https://www.osapublishing.org/aop/fulltext.cfm?uri=aop-3-2-128&amp;id=211561" target="_blank" rel="external">《Structured-Light 3D Surface Imaging: A Tutorial, Advances in Optics and Photonics》</a></p>
<pre><code>- 硬件：RGB Camera,3D Depth Sensor,Motorized Tilt(调整角度)
- 软件：OpenNI, Microsoft Kinect SDK,OpenKinect
</code></pre><p>1.预处理：</p>
<ul>
<li><p>Kinect重标定,广泛的基本方法《Accurate and Practical Cali-<br>bration of A Depth and Color Camera Pair》</p>
</li>
<li><p>深度信息的过滤,《Joint Denoising and Interpolation of Depth Maps for MS Kinect Sensors》通过在物体边界寻找空间对应点，物体在彩图分割转移到深度图中。<br>  《 Structure Guided Fusion for Depth Map Inpainting, Pattern Recognition Letters 》</p>
</li>
</ul>
<p>2.物体跟踪和识别</p>
<ul>
<li>DETECTION:《Leveraging RGB-D Data: Adaptive Fusion and Domain Adaptation for Object Detection》</li>
<li>室内切割《 Indoor Segmentation and Support Inference from RGBD Images》 其中包含RGBD数据集</li>
<li>行为识别 《Accurate 3D Pose Estimation from A Single Depth Image》<img src="http://7xl4js.com1.z0.glb.clouddn.com/2015-08-20activity_recog.png" alt="Basic processing pipeline of Activity Recognition"></li>
<li>场景识别  《RGB-(D) Scene Labeling: Features and Algorithms》使用加入深度信息的FEATURE有利于提高COMPACITY，抵挡灯光影响和杂乱影响。然而，计算量加大难以实时Real-time</li>
</ul>
<p>3.手势识别和分析 4.手势动作（然而这个我并不关心，谢谢）</p>
<h2 id="5-_u5BA4_u51853D_u5339_u914D"><a href="#5-_u5BA4_u51853D_u5339_u914D" class="headerlink" title="5.室内3D匹配"></a><strong>5.室内3D匹配</strong></h2><p>  AIM：对室内环境的数字化表达，室内三维重建<br>  研究包含两部分：1.数据和特征的抽取 2.循环检测并全局优化</p>
<h3 id="A-_u7A00_u758F_uFF08sparse_uFF09_u7279_u5F81_u5339_u914D"><a href="#A-_u7A00_u758F_uFF08sparse_uFF09_u7279_u5F81_u5339_u914D" class="headerlink" title="A.稀疏（sparse）特征匹配"></a>A.稀疏（sparse）特征匹配</h3><p>  典型《RGB-D Mapping: Using Depth Cameras for Dense 3D Modeling of Indoor Environments》 优化结合appearance and shape（依赖深度图所得） 通过Iterative Closest Point(ICP)算法。</p>
<h3 id="B-_u5BC6_u96C6_u70B9_u4E91_u5339_u914D_uFF08Dense_points_tracking_between_two_frames_uFF09"><a href="#B-_u5BC6_u96C6_u70B9_u4E91_u5339_u914D_uFF08Dense_points_tracking_between_two_frames_uFF09" class="headerlink" title="B.密集点云匹配（Dense points tracking between two frames）"></a>B.密集点云匹配（Dense points tracking between two frames）</h3><p>  计算量大，不易实时，但有成功的论文《KinectFusion: Real-Time Dense Surface Mapping and Tracking》</p>
<p>  针对特定问题下的论文：<br>  LARGE LIGHTING VARIATIONS《Dense RGB-D Mapping for Real-Time Localisation and Navigation》<br>  CAMERA ROAM FREE《Moving Volume KinectFusion》<br>  室内基准实验数据集 《A Benchmark for the Evaluation of RGB-D SLAM Systems》</p>
<p>6.问题&amp;展望&amp;总结</p>
<ul>
<li>REAL-WORLD应用</li>
<li>有效算法模型</li>
<li>RGB-D信息高效整合</li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<p>原文：<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6547194">Click</a><br>作者：Jungong Han,Ling Shao</p>
<h3 id="u8FD9_u4E2A_u8BBA_u6587_u4E3B_u8981_u7B80_u4ECB_u4E00_u4E9B_u57FA_u4E8EKinect_u7684_u8BA1_u7B97_u673A_u89C6_u89C9_u7B97_u6CD5_u548C_u5E94_u7528_uFF0C_u6DB5_u76D6_u4E86_u5305_u62EC_u6DF1_u5EA6_u4FE1_u606F_u7684_u9884_u5904_u7406_uFF0CKinect_u7684_u7CBE_u786E_u6807_u5B9A_uFF1B_u7269_u4F53_u8DDF_u8E2A_u548C_u8BC6_u522B_uFF1B_u4EBA_u7C7B_u6D3B_u52A8_u5206_u6790_u548C_u624B_u52BF_u5206_u6790_u4EE5_u53CA_u5BA4_u51853D_u5339_u914D_u3002"><a href="#u8FD9_u4E2A_u8BBA_u6587_u4E3B_u8981_u7B80_u4ECB_u4E00_u4E9B_u57FA_u4E8EKinect_u7684_u8BA1_u7B97_u673A_u89C6_u89C9_u7B97_u6CD5_u548C_u5E94_u7528_uFF0C_u6DB5_u76D6_u4E86_u5305_u62EC_u6DF1_u5EA6_u4FE1_u606F_u7684_u9884_u5904_u7406_uFF0CKinect_u7684_u7CBE_u786E_u6807_u5B9A_uFF1B_u7269_u4F53_u8DDF_u8E2A_u548C_u8BC6_u522B_uFF1B_u4EBA_u7C7B_u6D3B_u52A8_u5206_u6790_u548C_u624B_u52BF_u5206_u6790_u4EE5_u53CA_u5BA4_u51853D_u5339_u914D_u3002" class="headerlink" title="这个论文主要简介一些基于Kinect的计算机视觉算法和应用，涵盖了包括深度信息的预处理，Kinect的精确标定；物体跟踪和识别；人类活动分析和手势分析以及室内3D匹配。"></a>这个论文主要简介一些基于Kinect的计算机视觉算法和应用，涵盖了包括深度信息的预处理，Kinect的精确标定；物体跟踪和识别；人类活动分析和手势分析以及室内3D匹配。</h3>]]>
    
    </summary>
    
      <category term="paper" scheme="https://csrjtan.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Paper_Reading 《Rapid Object Detection using a Boosted Cascade of Simple Features》]]></title>
    <link href="https://csrjtan.github.io/2015/08/16/paper-research-01/"/>
    <id>https://csrjtan.github.io/2015/08/16/paper-research-01/</id>
    <published>2015-08-16T13:12:12.000Z</published>
    <updated>2015-10-28T14:39:49.000Z</updated>
    <content type="html"><![CDATA[<p><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=990517" target="_blank" rel="external">Paper Link</a></p>
<h3 id="Author_3APaul_Viola_2CMichael_Jones"><a href="#Author_3APaul_Viola_2CMichael_Jones" class="headerlink" title="Author:Paul Viola,Michael Jones"></a>Author:Paul Viola,Michael Jones</h3><blockquote>
<h3 id="Key_Contributions_3A"><a href="#Key_Contributions_3A" class="headerlink" title="Key Contributions:"></a>Key Contributions:</h3><h3 id="1-Integral_Image"><a href="#1-Integral_Image" class="headerlink" title="1.Integral Image"></a>1.Integral Image</h3><h3 id="2-Learning_algorithm_based_on_AdaBoost"><a href="#2-Learning_algorithm_based_on_AdaBoost" class="headerlink" title="2.Learning algorithm based on AdaBoost"></a>2.Learning algorithm based on AdaBoost</h3><h3 id="3-Combine_Classifiers_in_Cascade"><a href="#3-Combine_Classifiers_in_Cascade" class="headerlink" title="3.Combine Classifiers in Cascade"></a>3.Combine Classifiers in Cascade</h3></blockquote>
<h3 id="Methods_3A"><a href="#Methods_3A" class="headerlink" title="Methods:"></a>Methods:</h3><ul>
<li>Represent and classify images based on simple features rather than pixels directly(operates <strong>faster</strong> and easily encode <strong>ad-hoc</strong> domain knowledge)<a id="more"></a></li>
<li><p><strong>Three</strong> kinds of simple features are used in the paper</p>
<ol>
<li>Two-Rectangles features (A and B)</li>
<li>Three-Rectangles features (C)</li>
<li><p>Four-Rectangles features (D)<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/sceneryfeatures.jpg" alt="Three-types-of-Features"></p>
<p>The feature value calculation:<br>$$\sum{pixel\ values\ in\ white}-\sum{pixel\ values\ in\ gray}$$</p>
</li>
</ol>
</li>
<li><p><strong>Contributions 1:Integral Image</strong><br><img src="http://7xl4js.com1.z0.glb.clouddn.com/scenery3.jpg" alt=""><br><img src="http://7xl4js.com1.z0.glb.clouddn.com/scenery1.jpg" alt=""></p>
</li>
<li><p>Contributions 2:Learning Algorithm based on AdaBoost</p>
<blockquote>
<p><strong>The advantages of AdaBoost:</strong><br>1.Used for <strong>feature selection</strong> and <strong>classifier training</strong><br>2.Selecting small set of <strong>good features</strong> from large feature set<br>3.Used a set of <strong>weak learners</strong> to form a strong one<br>4.Guarantees the training error of strong classifier very <strong>low</strong></p>
</blockquote>
<p><strong>The Pesudo of Adaboost:</strong><br><img src="http://7xl4js.com1.z0.glb.clouddn.com/2015-08-20pesudo.png" alt="The pesudo"></p>
</li>
<li><p>Contributions 3:Combine Classifiers in Cascade</p>
<ul>
<li>Building cascade of classifiers(Increase Performance &amp; Reduce computation)</li>
<li>Simpler classifiers apply early to <strong>reject majority</strong> of sub windows and apply complex classifiers to achieve low false positive</li>
<li>Subsequent classifiers are trained using examples,which pass through all the previous stages<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/scenery4.JPG" alt="Cascade Model"></li>
</ul>
</li>
<li><p>How to use</p>
<ol>
<li>User selects maximun FPR(False Prediction Rate) and minimun acceptable DR(Detection Rate) per each stage</li>
<li>User selects target</li>
<li>Each stage is trained by adding features until the target DR and FPRs are met</li>
<li>Stages are added until the overall target for DR and FPR are met</li>
</ol>
</li>
<li><p>Further optimization</p>
<ol>
<li>Number of classifier stages</li>
<li>Number of features in each stage</li>
<li>Threshold of each stage</li>
<li>Minimun number of features that achieved accuracy</li>
</ol>
</li>
<li><p>Conclusion</p>
<ol>
<li>Solution achieves the goal of real time object detection</li>
<li>Conjunction of simple rectangle features and integral image gives a efficient feature representation</li>
<li>AdaBoost is used for the feature selection and classifier training</li>
<li>Cascade of classifiers allows to quickly discard background regions and concentrate more on ojbect-like regions</li>
</ol>
</li>
<li><p>Approximately Performance<br> Accuracy:Front-Face around 68.8%,Profile around 33%<br> Time:every picture with 67 millisecond</p>
</li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=990517">Paper Link</a></p>
<h3 id="Author_3APaul_Viola_2CMichael_Jones"><a href="#Author_3APaul_Viola_2CMichael_Jones" class="headerlink" title="Author:Paul Viola,Michael Jones"></a>Author:Paul Viola,Michael Jones</h3><blockquote>
<h3 id="Key_Contributions_3A"><a href="#Key_Contributions_3A" class="headerlink" title="Key Contributions:"></a>Key Contributions:</h3><h3 id="1-Integral_Image"><a href="#1-Integral_Image" class="headerlink" title="1.Integral Image"></a>1.Integral Image</h3><h3 id="2-Learning_algorithm_based_on_AdaBoost"><a href="#2-Learning_algorithm_based_on_AdaBoost" class="headerlink" title="2.Learning algorithm based on AdaBoost"></a>2.Learning algorithm based on AdaBoost</h3><h3 id="3-Combine_Classifiers_in_Cascade"><a href="#3-Combine_Classifiers_in_Cascade" class="headerlink" title="3.Combine Classifiers in Cascade"></a>3.Combine Classifiers in Cascade</h3></blockquote>
<h3 id="Methods_3A"><a href="#Methods_3A" class="headerlink" title="Methods:"></a>Methods:</h3><ul>
<li>Represent and classify images based on simple features rather than pixels directly(operates <strong>faster</strong> and easily encode <strong>ad-hoc</strong> domain knowledge)]]>
    
    </summary>
    
      <category term="paper" scheme="https://csrjtan.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Research_For_UAVs]]></title>
    <link href="https://csrjtan.github.io/2015/08/16/object-detection-research/"/>
    <id>https://csrjtan.github.io/2015/08/16/object-detection-research/</id>
    <published>2015-08-16T09:42:10.000Z</published>
    <updated>2015-10-28T14:40:07.000Z</updated>
    <content type="html"><![CDATA[<pre><code>目的：调研无人机网络项目的相关资料。
TOPICS: Efficient methods for object detection,object recognition and event/scene understanding,
including dangerous objects/events like fire and heavy smoke
要求：2015-8-18下午前，英文整理文档，简要文字加上直观图片说明相关领域内代表性的方法以及最好的方法，包含至少15篇参考文献。
</code></pre><a id="more"></a>
<h1 id="OBJECT_DETECTION__26amp_3B_RECOGNITION"><a href="#OBJECT_DETECTION__26amp_3B_RECOGNITION" class="headerlink" title="OBJECT DETECTION &amp; RECOGNITION"></a>OBJECT DETECTION &amp; RECOGNITION</h1><p> <strong>1.HISTORY&amp;OVERVIEW</strong> <a href="http://www.cs.unc.edu/~lazebnik/spring10/lec16_recognition_intro.pdf" target="_blank" rel="external">[1]</a> <a href="http://www.researchgate.net/publication/257484936_50_Years_of_object_recognition_Directions_forward" target="_blank" rel="external">[2]</a></p>
<ul>
<li><p>Targets:There are about 10,000 to 30,000 visual object categories. Including Scene categorization(city, outdoor), Image-level annotaion(are there people), Object detection(where are the people) and Image parsing(people building).</p>
<p>P.Perona[3] discerns <strong>five levels</strong> of tasks of increasing difficulty in the <strong>recognition problem</strong>:<br>1.Verification: Is a particular item present in an image patch?<br>2.Detection and Localization: Given a complex image, decide if a particular exemplar object is located some-where in this image, and provide accurate location information on this object.<br>3.Classification: Given an image patch, decide which of the multiple possible categories are present in that patch.<br>4.Naming: Given a large complex image (instead of an image patch as in the classification problem) determine the location and labels of the objects present in that image.<br>5.Description: Given a complex image, name all the objects present in the image, and describe the actions and re-lationships of the various objects within the context of this image. As the author indicates, this is also sometimes referred to as scene understanding.</p>
</li>
<li><p>The <strong>components</strong> used in a typical object recognition system:The feature extraction, followed by feature grouping, followed by object hypothesis generation, followed by an object verification stage.But nowadays methods have <strong>blurred</strong> the distinction between the mentioned component.<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/2015-08-20typical_recognition_system.png" alt="Classical Recognition System"></p>
</li>
<li><p><strong>Timeline</strong> of recognition</p>
<ul>
<li>Late 1980s:Alignment,Geometric Primitives</li>
<li>Early 1990s:Invariants,Appearance-based Methods</li>
<li>Mid-late 1990s:Sliding Windows Approaches</li>
<li>Late 1990s:Feature-based Methods</li>
<li>Early 2000s:Parts-and-shape Models</li>
<li>2003-late 2000s:Bags of Features</li>
<li>Present Trends:Machine Learning,Deep Learning,Combination of local and global Methods,Modeling Context,Emphasis on “Image Parsing”<br>(you can see the detail of methods in the <a href="http://www.cs.unc.edu/~lazebnik/spring10/lec16_recognition_intro.pdf" target="_blank" rel="external">Slides</a>)</li>
</ul>
<p><strong>2.Efficient Methods &amp; Relative Papers</strong></p>
</li>
<li><h3 id="Alignment__26amp_3B_Geometric_Primitives"><a href="#Alignment__26amp_3B_Geometric_Primitives" class="headerlink" title="Alignment &amp; Geometric Primitives"></a>Alignment &amp; Geometric Primitives</h3><p> <strong>1.Alignment:</strong>Transformation between pairs of features matches in two images<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/2015-08-20alignment.png" alt="Transformation"><br>e.g.《Object Recognition Using Alignment》<a href="http://www.cse.unr.edu/~bebis/CS773C/ObjectRecognition/Papers/Huttenlocher87.pdf" target="_blank" rel="external">[4]</a> based on the assumption and the method that the position, orientation and scale of an object in three-space can be determined from three pairs of corresponding model and image points.</p>
<p> <strong>2.Geometric Primitives:</strong>Decribed model-based system with <strong>Volumn Models</strong><br><img src="http://7xl4js.com1.z0.glb.clouddn.com/2015-08-20geomeric_primitive.png" alt="Volumn Models"><br><img src="http://7xl4js.com1.z0.glb.clouddn.com/2015-08-20geometric.png" alt="Model-Based"><br>e.g.《Symbolic reasoning among 3-D models and 2-D images》<a href="http://www.sciencedirect.com/science/article/pii/000437028190028X" target="_blank" rel="external">[5]</a> Describe <strong>model-based</strong> systems in models,prediction of image features,description of image features and interpretation which relates image features to models.</p>
</li>
<li><h3 id="Invariants__26amp_3B_Appearance-based"><a href="#Invariants__26amp_3B_Appearance-based" class="headerlink" title="Invariants &amp; Appearance-based"></a>Invariants &amp; Appearance-based</h3><p> <strong>1.Geometric invariants:</strong>Used to probide an efficient indexing mechanism for object recognition system.<br> e.g.《Geometric hashing: an overview》<a href="http://www.computer.org/csdl/mags/cs/1997/04/c4010.pdf" target="_blank" rel="external">[6]</a> Typical deformations discussed in the literature include 2D translations,rotations and scalings.</p>
<p> <strong>Limits</strong>:The above method only suit for <strong>monocular</strong> viewpoint invariants.</p>
<p> <strong>2.Appearance-based:</strong>Including Eigenfaces,Color Histograms and appearance manifolds.<br> e.g.《Face Recognition Using Eigenfaces 》<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=139758" target="_blank" rel="external">[7]</a> treats face recognition as a two-dimensional recognition problem and makes that the face images are projected onto a <strong>feature space</strong> which best encodes the variation among known face images.(<a href="http://blog.csdn.net/feirose/article/details/39552887" target="_blank" rel="external">实现原理</a>)</p>
<p> e.g.《Color Indexing》<a href="http://link.springer.com/article/10.1007/BF00130487" target="_blank" rel="external">[8]</a> demonstrates that <strong>color histograms</strong> of multicolored objects provide a robust,efficient cue for indexing into a large database of models</p>
<p> e.g. 《Visual learning and recognition of 3d objects from appearance》<a href="http://link.springer.com/article/10.1007/BF01421486" target="_blank" rel="external">[9]</a> used the <strong>manifolds</strong> for object detection.<br> <img src="http://7xl4js.com1.z0.glb.clouddn.com/2015-08-20manifold.png" alt="manifold"><br> <strong>Limits</strong>:<br> 1.Require global registration of patterns<br> 2.Not robust to clutter,occlusion,geometric transformations</p>
</li>
<li><h3 id="Sliding_Window_Approaches"><a href="#Sliding_Window_Approaches" class="headerlink" title="Sliding Window Approaches"></a>Sliding Window Approaches</h3><p> e.g. 《Rapid Object Detection using a Boosted Cascade of Simple Features》<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=990517" target="_blank" rel="external">[10]</a> It is <strong>prominent</strong> and milestone in face detection，more than 11500 citations and widely used solution for the real-time Object Detection.The very detailed of the method can click on <a href="/2015/08/16/paper-research-01/">this</a> (Strongly Recommend)<br> <strong>Limits</strong>:Can not handle clutter and occlusion well</p>
</li>
<li><h3 id="Feature-based_Methods"><a href="#Feature-based_Methods" class="headerlink" title="Feature-based Methods"></a>Feature-based Methods</h3><p> e.g. 《Distinctive Image Features from Scale-Invariant Keypoints》<a href="http://download.springer.com/static/pdf/941/art%253A10.1023%252FB%253AVISI.0000029664.99615.94.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1023%2FB%3AVISI.0000029664.99615.94&amp;token2=exp=1439832722~acl=%2Fstatic%2Fpdf%2F941%2Fart%25253A10.1023%25252FB%25253AVISI.0000029664.99615.94.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Farticle%252F10.1023%252FB%253AVISI.0000029664.99615.94*~hmac=8324543a7075217178c5cd6bc912ca31a21f7476c9e9c3037381dad9019bc3fb" target="_blank" rel="external">[11]</a> the sift feature by Lowe,object detection via the feature points matching.The keypoints have been shown to be invariant to image rotation and scale and robust across a substantial range of affine distortion,addition of noise, and change in illumination.<br> <img src="http://7xl4js.com1.z0.glb.clouddn.com/2015-08-20sift.png" alt="SIFT Feature"><br> <strong>Limits</strong>:Can not real-time with large computation</p>
</li>
<li><h3 id="Part-based_Methods"><a href="#Part-based_Methods" class="headerlink" title="Part-based Methods"></a><a href="http://cs.nyu.edu/~fergus/teaching/vision_2012/11_parts_models.pdf" target="_blank" rel="external">Part-based Methods</a></h3><p><img src="http://7xl4js.com1.z0.glb.clouddn.com/2015-08-20part_model.png" alt="Part-based Model"></p>
<ul>
<li>Object as a set of parts</li>
<li>Relative locations between parts</li>
<li>Appearance of part<br>e.g. 《Object Detection with Discriminatively Trained Part Based Model》<a href="http://lear.inrialpes.fr/~oneata/reading_group/dpm.pdf" target="_blank" rel="external">[12]</a> use Hog Features,Part Model and Latent SVM to work.<br><img src="http://7xl4js.com1.z0.glb.clouddn.com/2015-08-20object_hypothesis.png" alt="Object hypohesis with component part"><br><img src="http://7xl4js.com1.z0.glb.clouddn.com/2015-08-20dpm.png" alt="Detected with part-based method"></li>
</ul>
</li>
</ul>
<ul>
<li><h3 id="Bag-of-features_Models"><a href="#Bag-of-features_Models" class="headerlink" title="Bag-of-features Models"></a>Bag-of-features Models</h3><p><img src="http://7xl4js.com1.z0.glb.clouddn.com/2015-08-20bag_of_words.png" alt="Bag of words"><br>e.g. 《Local features and kernels for classification of texture<br>and object categories: A comprehensive study》<a href="http://lear.inrialpes.fr/pubs/2007/ZMLS07/ZhangMarszalekLazebnikSchmid-IJCV07-ClassificationStudy.pdf" target="_blank" rel="external">[13]</a> achieved very impressive result in the <a href="http://host.robots.ox.ac.uk/pascal/VOC/" target="_blank" rel="external">PASCAL Visual Object Classes Challenge</a></p>
<p><strong>Limits</strong>:Ignore the spatial relationships among the patches</p>
</li>
<li><h3 id="Neural-network_models"><a href="#Neural-network_models" class="headerlink" title="Neural-network models"></a>Neural-network models</h3><p>e.g. 《ImageNet Classification with Deep Convolutional<br>Neural Networks》<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="external">[14]</a><br><img src="http://7xl4js.com1.z0.glb.clouddn.com/2015-08-20CNN.png" alt="Convolution Neutral Network"></p>
<p>e.g. 《Rich feature hierarchies for accurate object detection and semantic segmentation》<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6909475" target="_blank" rel="external">[15]</a><br><img src="http://7xl4js.com1.z0.glb.clouddn.com/2015-08-20RCNN.png" alt="RCNN,some relevant methods:fast RCNN,faster RCNN,SPP"></p>
</li>
<li><h3 id="Scene/Event_Recognition"><a href="#Scene/Event_Recognition" class="headerlink" title="Scene/Event Recognition"></a>Scene/Event Recognition</h3><p><strong>Scene</strong>: 《Modeling the shape of the scene: a holistic representation of the spatial envelope》<a href="http://people.csail.mit.edu/torralba/code/spatialenvelope/" target="_blank" rel="external">[16]</a> performs good at scene recognition.<br><strong>Event</strong>: 《Video-based event recognition:activity representation and probabilistic recognition methods》<a href="http://ac.els-cdn.com/S1077314204000712/1-s2.0-S1077314204000712-main.pdf?_tid=bf1117ce-4572-11e5-9fbb-00000aab0f27&amp;acdnat=1439879663_064adeae1298edd050e4715f458da8cb" target="_blank" rel="external">[17]</a><br><img src="http://7xl4js.com1.z0.glb.clouddn.com/2015-08-20event_recog.png" alt="Event Recognition System"></p>
</li>
</ul>
<p> <strong>3.Applications for nowadays</strong></p>
<p>  <img src="http://7xl4js.com1.z0.glb.clouddn.com/2015-08-20word_recog.png" alt="Reading license plates,zip codes,checks "></p>
<p>  <img src="http://7xl4js.com1.z0.glb.clouddn.com/2015-08-20finger_recog.png" alt="Fingerprint recognition"></p>
<p>  <img src="http://7xl4js.com1.z0.glb.clouddn.com/2015-08-20face_recog.jpeg" alt="Face detection"></p>
<p>  <img src="http://7xl4js.com1.z0.glb.clouddn.com/2015-08-20cover_recog.png" alt="Recognition of flat textured objects(Covers)"></p>
<p><strong>4.References</strong><br>[1] Fei-Fei Li, Rob Fergus, Antonio Torralba, and Jean Ponce. “Object Recognition:History and Overview.” CS.UNC.EDU , 2011<br>[2] Andreopoulos, Alexander, and John K. Tsotsos. “50 Years of object recognition: Directions forward.” Computer Vision and Image Understanding 117.8 (2013): 827-891.<br>[3] P. Perona, “Object Categorization: Computer and Human Perspectives, chap.” Visual Recognition circa 2008, Cambridge University Press,<br>55–68, 2009.<br>[4] Huttenlocher, Daniel P., and Shimon Ullman. “Object recognition using alignment.” Proc. ICCV. Vol. 87. 1987.<br>[5] Brooks, Rodney A. “Symbolic reasoning among 3-D models and 2-D images.” Artificial intelligence 17.1 (1981): 285-348.<br>[6] Wolfson, Haim J., and Isidore Rigoutsos. “Geometric hashing: An overview.” Computing in Science &amp; Engineering 4 (1997): 10-21.<br>[7] Turk, Matthew, and Alex P. Pentland. “Face recognition using eigenfaces.” Computer Vision and Pattern Recognition, 1991. Proceedings CVPR’91., IEEE Computer Society Conference on. IEEE, 1991.<br>[8] Swain, Michael J., and Dana H. Ballard. “Color indexing.” International journal of computer vision 7.1 (1991): 11-32.<br>[9] Murase, Hiroshi, and Shree K. Nayar. “Visual learning and recognition of 3-D objects from appearance.” International journal of computer vision 14.1 (1995): 5-24.<br>[10] Viola, Paul, and Michael Jones. “Rapid object detection using a boosted cascade of simple features.” Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on. Vol. 1. IEEE, 2001.<br>[11] Lowe, David G. “Distinctive image features from scale-invariant keypoints.” International journal of computer vision 60.2 (2004): 91-110.<br>[12] Felzenszwalb, Pedro F., et al. “Object detection with discriminatively trained part-based models.” Pattern Analysis and Machine Intelligence, IEEE Transactions on 32.9 (2010): 1627-1645.<br>[13] Zhang, Jianguo, et al. “Local features and kernels for classification of texture and object categories: A comprehensive study.” International journal of computer vision 73.2 (2007): 213-238.<br>[14] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “Imagenet classification with deep convolutional neural networks.” Advances in neural information processing systems. 2012.<br>[15] Girshick, Ross, et al. “Rich feature hierarchies for accurate object detection and semantic segmentation.” Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE, 2014.<br>[16] Oliva, Aude, and Antonio Torralba. “Modeling the shape of the scene: A holistic representation of the spatial envelope.” International journal of computer vision 42.3 (2001): 145-175.<br>[17] Hongeng, Somboon, Ram Nevatia, and Francois Bremond. “Video-based event recognition: activity representation and probabilistic recognition methods.” Computer Vision and Image Understanding 96.2 (2004): 129-162.</p>
]]></content>
    <summary type="html">
    <![CDATA[<pre><code>目的：调研无人机网络项目的相关资料。
TOPICS: Efficient methods for object detection,object recognition and event/scene understanding,
including dangerous objects/events like fire and heavy smoke
要求：2015-8-18下午前，英文整理文档，简要文字加上直观图片说明相关领域内代表性的方法以及最好的方法，包含至少15篇参考文献。
</code></pre>]]>
    
    </summary>
    
      <category term="paper" scheme="https://csrjtan.github.io/tags/paper/"/>
    
  </entry>
  
</feed>
