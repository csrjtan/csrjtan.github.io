<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>CSRJTAN</title>
  <subtitle>Keep Moving</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://csrjtan.github.io/"/>
  <updated>2016-11-08T12:41:48.000Z</updated>
  <id>https://csrjtan.github.io/</id>
  
  <author>
    <name>CsrjTan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>认识教会的朋友</title>
    <link href="https://csrjtan.github.io/2016/11/08/%E8%AE%A4%E8%AF%86%E6%95%99%E4%BC%9A%E7%9A%84%E6%9C%8B%E5%8F%8B/"/>
    <id>https://csrjtan.github.io/2016/11/08/认识教会的朋友/</id>
    <published>2016-11-08T12:21:20.000Z</published>
    <updated>2016-11-08T12:41:48.000Z</updated>
    
    <content type="html">&lt;p&gt;这一段时间生活确实比较枯燥，感觉自己把自己给困住了，总结一下最近三个月的生活日常。&lt;/p&gt;
&lt;p&gt;首先今年宪源小朋友来科大读Msc了，也是学习机器学习内容，可以充实忙碌地学习工作了。来了之后组织女朋友和他情侣俩一块来到城门水塘，由于起床比较晚，中间不大会路，去到大概已经3点多了。从西铁线去到荃湾站，然后坐82号巴士到公园。&lt;/p&gt;
&lt;p&gt;城门水塘整个是一个麦理浩径的一段，所以十分的长，要走完感觉一天也勉强，一般就直接进去看看猴子，到了水塘附近找个美景拍拍照。这里确实风景优美，空气清新，是在香港这个钢铁森林里一个极好的郊野公园，以后有机会还要再来拍风景和人物美照。&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/2016110801.png&quot; alt=&quot;离开水塘前的合照&quot;&gt;&lt;/p&gt;
&lt;p&gt;之后，珊珊突然周末希望去教会参加崇拜。受到了Windy的邀请，我们来到了九龙维景酒店对面的窝福堂，认识了普通话团契的伙伴们，大家都热切殷勤地欢迎了我俩。第一次参加他们的团契活动就是Outing了，我们先坐尖沙咀的邮轮来到中环，然后步行去到中环的一个比较文艺的旅游景点，有情侣石阶，壁画街，还参观了香港旧警署宿舍改造的设计工坊。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/2016110802.png&quot; alt=&quot;一起坐邮轮&quot;&gt;&lt;/p&gt;
&lt;p&gt;结果，我们渐渐地周末从没有活动而去一起学习，变成了参加团契的活动，虽然有时候开支比较大，但也很珍惜和热心的朋友在一起的光阴。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/2016110803.png&quot; alt=&quot;壁画街的幸福照&quot;&gt;&lt;/p&gt;
&lt;p&gt;后来的一周参加桌游活动，在观塘附近找到一家很赞的酒楼，性价比也是很好的。只是感概在香港开个桌游房都人均100，确实略贵，广州同类甚至娱乐性更强的才人均25~。~，玩完之后大家去了一个那边超级大的商场对面的草坪，参加了跳蚤市场。我俩在商场里的一家书局买了好久的手账，不过看到珊爷满载而归，还是十分值得的。&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/2016110804.png&quot; alt=&quot;桌游活动&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;这一段时间生活确实比较枯燥，感觉自己把自己给困住了，总结一下最近三个月的生活日常。&lt;/p&gt;
&lt;p&gt;首先今年宪源小朋友来科大读Msc了，也是学习机器学习内容，可以充实忙碌地学习工作了。来了之后组织女朋友和他情侣俩一块来到城门水塘，由于起床比较晚，中间不大会路，去到大概已经3点
    
    </summary>
    
      <category term="Life" scheme="https://csrjtan.github.io/categories/Life/"/>
    
    
      <category term="教会 朋友" scheme="https://csrjtan.github.io/tags/%E6%95%99%E4%BC%9A-%E6%9C%8B%E5%8F%8B/"/>
    
  </entry>
  
  <entry>
    <title>CDM using inter-channel correlation and NSS</title>
    <link href="https://csrjtan.github.io/2016/11/08/paper-reading-20161108/"/>
    <id>https://csrjtan.github.io/2016/11/08/paper-reading-20161108/</id>
    <published>2016-11-08T09:14:05.000Z</published>
    <updated>2016-11-08T12:54:19.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;a href=&quot;http://ac.els-cdn.com/S092359651500168X/1-s2.0-S092359651500168X-main.pdf?_tid=1a82a922-a596-11e6-b1c6-00000aab0f02&amp;amp;acdnat=1478597659_28a15da42ec23144e3bfab16a0639039&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;原文链接&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;Title-CDM-with-ICC-and-NSS-nonlocal-self-similarity&quot;&gt;&lt;a href=&quot;#Title-CDM-with-ICC-and-NSS-nonlocal-self-similarity&quot; class=&quot;headerlink&quot; title=&quot;Title: CDM with ICC and NSS(nonlocal self-similarity)&quot;&gt;&lt;/a&gt;Title: CDM with ICC and NSS(nonlocal self-similarity)&lt;/h3&gt;&lt;p&gt;摘要：提出了novel modeling strategy and efficient algorithm. 在梯度上，使用inter-channel correlation。在约束项上，利用了Nonlocal self-similarity. 在提出了joint-model之后，将minimize problem分解成两个子问题，并迭代求解（凸优化里面的问题）。&lt;/p&gt;
&lt;h4 id=&quot;Introduction-将CDM分成了四类&quot;&gt;&lt;a href=&quot;#Introduction-将CDM分成了四类&quot; class=&quot;headerlink&quot; title=&quot;Introduction:将CDM分成了四类&quot;&gt;&lt;/a&gt;Introduction:将CDM分成了四类&lt;/h4&gt;&lt;p&gt;一.inter-channel correlation: 1.最早的color difference方法，2.Gunturk AP使用了wavelet basis假定图像在high-frequency subbands are high correlated &lt;a href=&quot;http://3.Li&quot; class=&quot;test test-url&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;3.Li&lt;/a&gt;认为图像经过gamma correction之后，再使用color difference is better 4.Jaiswal提出low-pass filter(LPF)结合（color difference）CD的方法 5.Kiku使用temtative residual interpolation rather than the color difference maps.&lt;/p&gt;
&lt;p&gt;二.locally estiamte a better gradient direction:&lt;br&gt;1.Hamilton and Adams使用了二阶梯度&lt;br&gt;2.Hirakwa采用了local homogenity&lt;br&gt;3.Zhang优化了directional filter of G-R and G-B by LMMSE&lt;/p&gt;
&lt;p&gt;三.Nonlocal Self-simialrity&lt;br&gt;1.Buades验证使用NSS可以reduce artfacts&lt;br&gt;2.Zhang使用LDI-NAT，用nonlocal adaptive thresholding来suppress artifact&lt;/p&gt;
&lt;p&gt;四.用prior knowledge来解决Ill-posed problem&lt;br&gt;1.Compress sensing with sparse coding&lt;br&gt;&lt;a href=&quot;http://2.Total&quot; class=&quot;test test-url&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;2.Total&lt;/a&gt; variation regularization: the smaller TV energy function gets a better PSNR in denoise&lt;br&gt;3.Gao提出了约束项，使用拉普拉斯操作后，图像的color difference are sparse.变得稀疏了，这篇论文的建模也用到了这个理论&lt;br&gt;4.基于字典学习（dictionary learning),Mairal认为natrual image服从sparse representation.&lt;/p&gt;
&lt;h4 id=&quot;Methodology&quot;&gt;&lt;a href=&quot;#Methodology&quot; class=&quot;headerlink&quot; title=&quot;Methodology&quot;&gt;&lt;/a&gt;Methodology&lt;/h4&gt;&lt;p&gt;1.先用初始化方法得到init_demosaicked images.&lt;br&gt;2.加入Inter-channel correlation的约束项：&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/20161108_01.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;3.根据l2-distance来查找similar patch，组成matrix. 由于patches都是相似的，所以matrix应该是sparse的，但由于噪声或者纹理丰富的原因，这里文章采用一个近似的方法，更好地满足稀疏特点&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/20161108_02.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;$$L+S = M $$&lt;br&gt;这里D指水平和垂直的finite difference operators,M是patches组成的矩阵，S表示outliers,假设Frobenius norm(二范数) of S is small,L是满足稀疏的矩阵。从而得到construct L如上式所示。&lt;br&gt;4.得到最后的求解模型&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/20161108_03.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;5.求解过程是分解成两个子问题，分别迭代最优化求解X和L。&lt;/p&gt;
&lt;h4 id=&quot;Experiment&quot;&gt;&lt;a href=&quot;#Experiment&quot; class=&quot;headerlink&quot; title=&quot;Experiment&quot;&gt;&lt;/a&gt;Experiment&lt;/h4&gt;&lt;p&gt;1.采用MLRI作为初始化方法&lt;br&gt;2.分为全图和Block-based NSS&lt;br&gt;3.包含窗口大小，patch大小，以及trade off hyperparameters的设置&lt;br&gt;4.在Kodak有41.06,McMaster有37.55，效果还不错&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://ac.els-cdn.com/S092359651500168X/1-s2.0-S092359651500168X-main.pdf?_tid=1a82a922-a596-11e6-b1c6-00000aab0f02&amp;amp;acdnat=1
    
    </summary>
    
      <category term="Tech" scheme="https://csrjtan.github.io/categories/Tech/"/>
    
    
      <category term="paper" scheme="https://csrjtan.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>CS231n_总结</title>
    <link href="https://csrjtan.github.io/2016/08/30/CS231n-%E6%80%BB%E7%BB%93/"/>
    <id>https://csrjtan.github.io/2016/08/30/CS231n-总结/</id>
    <published>2016-08-30T05:42:05.000Z</published>
    <updated>2016-08-30T06:11:50.000Z</updated>
    
    <content type="html">&lt;h3 id=&quot;结束CS231N课程笔记&quot;&gt;&lt;a href=&quot;#结束CS231N课程笔记&quot; class=&quot;headerlink&quot; title=&quot;结束CS231N课程笔记&quot;&gt;&lt;/a&gt;结束CS231N课程笔记&lt;/h3&gt;&lt;p&gt;CS231N课程的总结，到了后面关于RNN的用法以及detection和sequential学习都是新颖领域。&lt;/p&gt;
&lt;p&gt;(知乎专栏的翻译版)[&lt;a href=&quot;https://zhuanlan.zhihu.com/p/21930884?refer=intelligentunit&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://zhuanlan.zhihu.com/p/21930884?refer=intelligentunit&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;1.首先是PYTHON的教学，NUMPY使用。&lt;br&gt;2.图像分类方法：线性分类，SOFTMAX,SVM&lt;br&gt;3.优化方法：梯度下降、最优化方法&lt;br&gt;4.BP的具体原理和实现细节&lt;br&gt;5.CNN的学习&lt;br&gt;6.如何设置参数以及学习率&lt;br&gt;7.现在RNN发展和DETECTION&lt;/p&gt;
&lt;h3 id=&quot;回到香港，最后一年的准备和学习&quot;&gt;&lt;a href=&quot;#回到香港，最后一年的准备和学习&quot; class=&quot;headerlink&quot; title=&quot;回到香港，最后一年的准备和学习&quot;&gt;&lt;/a&gt;回到香港，最后一年的准备和学习&lt;/h3&gt;&lt;p&gt;经过了一年的放养，每天度假似的过得战战兢兢，如履薄冰。遗憾于自己不抓紧，没有自学到很多东西之余，拖延着使得任务也一点一点的搁浅，让自己和周围的朋友都失望，实在是愧对自己。如何展现自己的能力和让自己得到价值的证明是首要思考的。&lt;/p&gt;
&lt;p&gt;保持看书、锻炼的习惯，养成早起，日子过得很滋润，也得让自己抓紧有所贡献。就像王健林所说的，先划一个一个小目标，比如说先赚他一个亿。只是内心对于走科研的路子和决心越来越不能肯定了，还是只是希望自己混点小日子就过活，虽然明明知道自己过小日子也不会幸福快活，奈何对自己没有充足的信任。找到自己的才华和发力点，兴趣点是关键。热爱音乐，艺术，游戏的我，能在年轻的路上走得多远，我们拭目以待。当然英语我也是不排斥的，趁着年轻，可以逼着自己做些不喜欢的事情，考验自己的能力和内心承受。&lt;/p&gt;
&lt;h3 id=&quot;关于人生方向和目标&quot;&gt;&lt;a href=&quot;#关于人生方向和目标&quot; class=&quot;headerlink&quot; title=&quot;关于人生方向和目标&quot;&gt;&lt;/a&gt;关于人生方向和目标&lt;/h3&gt;&lt;p&gt;这里设立五年的目标：1.经济上：能负起首付，买上小车，自给自足，一步一步过上满意的生活。&lt;br&gt;2.事业上：拿到PHD学位或者获得良好的工作岗位并得到升职的空间 3.家庭上：帮爸妈过上小康的生活，与另一半稳定温暖的生活 4.兴趣上：吉他和健身的造诣得到提升，还可以培养更多的兴趣，可以研发一下游戏周边有意义的活动 5.身心上：身体要锻炼，心灵要熏陶，眼界要开阔，精力要丰富，做成更多有意义的事情，建立自信。&lt;/p&gt;
&lt;p&gt;一步一步的小目标：1.坚持早睡早起 2.坚持跑步和健身 3.坚持背单词和写代码 4.坚持看论文和看书 5.定期总结&lt;/p&gt;
&lt;p&gt;设定了小目标之后，需要及时检验，以我对自己的了解，我喜欢玩游戏，所以这种及时和即时的反馈是十分重要。在获得正面反馈之后，必须及时定下新的任务，一步一步的挑战，下副本和打boss。这样的能力提高过程，伴随能力的提高以及自信的建立，重要的是建立量化指标和衡量能力的有效途径，对于经济，明显就是投资和金钱；对于事业，一方面是学位和职位，另一方面就是掌握的知识结构；对于家庭和生活，情感是最难以衡量的，只能尽自己的心情地去关怀和付出了；对于兴趣，吉他的熟练和记谱，健身的体重与肌肉，跑步的速度与长度；对于身心，每天多对自己微笑和加油，生活是自己的，每个人的世界都是由自己所决定的；&lt;/p&gt;
&lt;p&gt;对于小目标来说：1.坚持早睡早起30天，50天，100天，300天 2.跑步和健身需要错开，一周保持3~4次，每次不少于30分钟，还需要量化统计自己的健身，看到自己的进步 3.单词和代码也是很好量化统计的 4.论文和看书也不例外。  这里建议使用特定的软件和本子来统计自己的进度和进步，用电脑或者笔记来总结和记录自己的进步吧，关键在于精力差的时候能走出破坏习惯的怪圈。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;结束CS231N课程笔记&quot;&gt;&lt;a href=&quot;#结束CS231N课程笔记&quot; class=&quot;headerlink&quot; title=&quot;结束CS231N课程笔记&quot;&gt;&lt;/a&gt;结束CS231N课程笔记&lt;/h3&gt;&lt;p&gt;CS231N课程的总结，到了后面关于RNN的用法以及dete
    
    </summary>
    
      <category term="Read" scheme="https://csrjtan.github.io/categories/Read/"/>
    
    
  </entry>
  
  <entry>
    <title>GoodMood</title>
    <link href="https://csrjtan.github.io/2016/07/28/GoodMood/"/>
    <id>https://csrjtan.github.io/2016/07/28/GoodMood/</id>
    <published>2016-07-28T08:52:33.000Z</published>
    <updated>2016-07-28T09:25:07.000Z</updated>
    
    <content type="html">&lt;p&gt;刚完成了Confirmation,压力好大，缓不过气，每天都要充实的学习做实验和生活。阅读、记录、代码、健身、音乐，早睡早起，多交朋友，保持健康。&lt;/p&gt;
&lt;p&gt;这里搬运一些让自己好心情的方子：&lt;/p&gt;
&lt;h4 id=&quot;14条交友原则&quot;&gt;&lt;a href=&quot;#14条交友原则&quot; class=&quot;headerlink&quot; title=&quot;14条交友原则&quot;&gt;&lt;/a&gt;14条交友原则&lt;/h4&gt;&lt;p&gt;1.保证&lt;strong&gt;充足睡眠&lt;/strong&gt;情况下，早起让每天多出几个小时&lt;br&gt;2.做事情留出至少10分钟的余地，不要&lt;strong&gt;赶时间&lt;/strong&gt;&lt;br&gt;3.&lt;strong&gt;和喜欢的人说笑&lt;/strong&gt;&lt;br&gt;4.&lt;strong&gt;日事日毕&lt;/strong&gt;，尽量8小时内解决问题，不要把工作带入生活&lt;br&gt;5.每两天来一次酣畅淋漓的&lt;strong&gt;运动&lt;/strong&gt;&lt;br&gt;6.对不重要的事情&lt;strong&gt;不争抢不较劲&lt;/strong&gt;，如开车&lt;br&gt;7.多&lt;strong&gt;接触新事物&lt;/strong&gt;带来新鲜感，多&lt;strong&gt;学习新技能&lt;/strong&gt;带来成就感&lt;br&gt;8.&lt;strong&gt;改造环境&lt;/strong&gt;，比如装饰房间&lt;br&gt;9.仔细读&lt;strong&gt;好书&lt;/strong&gt;，让心灵成熟、自由，有更多创造快乐的方法&lt;br&gt;10.&lt;strong&gt;偶尔放松&lt;/strong&gt;，通宵看球，和妹子开黑，吃零食看剧，躺床上看一天小说什么的。&lt;br&gt;11.每隔一段时间就换环境呆一段时间，&lt;strong&gt;度个假，旅个游&lt;/strong&gt;&lt;br&gt;12.每天给自己&lt;strong&gt;独处&lt;/strong&gt;的时间&lt;br&gt;13.&lt;strong&gt;早睡&lt;/strong&gt;，心里不会有负罪感，第二天精力充沛&lt;br&gt;14.&lt;strong&gt;不和不重要的人撕逼&lt;/strong&gt;&lt;br&gt;15.&lt;strong&gt;远离傻逼&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;交友原则&quot;&gt;&lt;a href=&quot;#交友原则&quot; class=&quot;headerlink&quot; title=&quot;交友原则&quot;&gt;&lt;/a&gt;交友原则&lt;/h4&gt;&lt;p&gt;1.不要非和某人做朋友，不喜欢就保持距离&lt;br&gt;2.合得来的人要好好相处，不计较地乐于助人&lt;br&gt;3.不要胡思乱想，顺势而为&lt;br&gt;4.管理好自己的形象，衣着配饰得体，运动整洁&lt;br&gt;5.尽最大努力去赚钱，理清现实后不妄想得不到的东西&lt;br&gt;6.爱，不苟求。‘与有情人做快乐事，不理是劫还是缘’&lt;br&gt;7.温柔的对待父母&lt;br&gt;8.不买，可有可无的东西；必需品则买经典耐用的&lt;br&gt;9.有话就说，不知道怎么说才好就照直了说，恶言恶语当场反驳&lt;br&gt;10.别给自己的行为贴不好的标签，内向就是内向，不是抑郁&lt;br&gt;11.爱情本身不会影响生活，对爱情犹豫不决才会，为爱情忽略自我才会&lt;br&gt;12.找机会与看似快乐的陌生人玩耍，和爱自己的亲人吃饭。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/Beauty.jpg&quot; alt=&quot;美女们&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;刚完成了Confirmation,压力好大，缓不过气，每天都要充实的学习做实验和生活。阅读、记录、代码、健身、音乐，早睡早起，多交朋友，保持健康。&lt;/p&gt;
&lt;p&gt;这里搬运一些让自己好心情的方子：&lt;/p&gt;
&lt;h4 id=&quot;14条交友原则&quot;&gt;&lt;a href=&quot;#14条交友原则&quot;
    
    </summary>
    
      <category term="Life" scheme="https://csrjtan.github.io/categories/Life/"/>
    
    
      <category term="health" scheme="https://csrjtan.github.io/tags/health/"/>
    
  </entry>
  
  <entry>
    <title>matconvnet配置</title>
    <link href="https://csrjtan.github.io/2016/06/26/matconvnet%E9%85%8D%E7%BD%AE/"/>
    <id>https://csrjtan.github.io/2016/06/26/matconvnet配置/</id>
    <published>2016-06-26T08:12:44.000Z</published>
    <updated>2016-06-26T08:35:55.000Z</updated>
    
    <content type="html">&lt;p&gt;&lt;a href=&quot;http://www.vlfeat.org/matconvnet/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;matconvnet&lt;/a&gt;,使用CNN做Image Demosaicking，由于组内的师兄主要使用这个框架，率先对这个框架进行快速上手使用和学习，CS231N也不能落下，之后可以考虑对CAFFE进行学习。&lt;/p&gt;
&lt;h4 id=&quot;配置&quot;&gt;&lt;a href=&quot;#配置&quot; class=&quot;headerlink&quot; title=&quot;配置&quot;&gt;&lt;/a&gt;配置&lt;/h4&gt;&lt;p&gt;首选由于选择软件的版本上也做了点无用功，亲测之下使用一下组合是有效的。&lt;strong&gt;win8.1+VS2013+MatlabR2014a+CUDA7.5+Cudnn-rc5&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;由于系统和Matlab的兼容性问题，感觉如此的组合是最为高效可靠的。&lt;br&gt;首先是mex -setup的问题，MatlabR2014a尚不支持VS2015的，为此我还体验了一会儿WIN10和VS2015.但考虑到后期开发和兼容性的问题，使用稳定版本是最为可靠的！对于下载后的源码，mex -setup设置成功后，直接跑vl_compilenn，则通过编译CPU的版本。这里着重说一下GPU版本，因为编译执行了很多次，不是编译就是运行代码时候遇到问题了，这里说一下自己的理解和亲测的过程。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.voidcn.com/blog/Hungryof/article/p-5047632.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;matconvnet gpu的方法&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;首先使用了CUDA5.5的版本，但由于在编译时遇到了点问题，意识到应该使用7.5版本，而且无论caffe还是最近来说，7.5版本是比较稳定，而且适应我当前使用的TITAN X的显卡设备。然后使用vl_complinn(‘enableGpu’,true)则可实现普通GPU的版本。然而一直卡在使用cuDnn包的时候编译或者运行就出错。后来发现是用的rc-4的包不正确，应当使用官方的包，直接下了rc-5的版本。[建议任何的框架和库都使用官方版本！]最后终于成功了，跑cifar的example,1500Hz for gpu; cuDnn成功后，达到6700Hz!&lt;/p&gt;
&lt;h4 id=&quot;学习&quot;&gt;&lt;a href=&quot;#学习&quot; class=&quot;headerlink&quot; title=&quot;学习&quot;&gt;&lt;/a&gt;学习&lt;/h4&gt;&lt;p&gt;这俩天认真的学习了一下MatconvNet,主要看官网的所有资料，以及example例子，然后就是VGG的&lt;a href=&quot;http://www.robots.ox.ac.uk/~vgg/practicals/cnn/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;tutorial&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;顺带有空把后面每一个函数的注释看一下，学会用法；针对于使用CNN，可以参考一下过程：&lt;br&gt;&lt;strong&gt;&lt;br&gt;1.Prepare Data&lt;br&gt;2.Set up Training parameters&lt;br&gt;&lt;a href=&quot;http://3.Training&quot; class=&quot;test test-url&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;3.Training&lt;/a&gt; Process:&lt;br&gt;    for num = 1:nEpoch&lt;br&gt;        train model on training set&lt;br&gt;        test on validation set&lt;br&gt;    end&lt;br&gt;4.Test on fianl test data
&lt;/strong&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://www.vlfeat.org/matconvnet/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;matconvnet&lt;/a&gt;,使用CNN做Image Demosaicking，由于组内的师兄主要使用这个框架，率先对这个框
    
    </summary>
    
      <category term="Tech" scheme="https://csrjtan.github.io/categories/Tech/"/>
    
    
      <category term="matconvNet" scheme="https://csrjtan.github.io/tags/matconvNet/"/>
    
  </entry>
  
  <entry>
    <title>CS231n_8_9</title>
    <link href="https://csrjtan.github.io/2016/06/21/CS231n-8/"/>
    <id>https://csrjtan.github.io/2016/06/21/CS231n-8/</id>
    <published>2016-06-21T11:16:47.000Z</published>
    <updated>2016-06-21T12:47:49.000Z</updated>
    
    <content type="html">&lt;h3 id=&quot;Lec8&quot;&gt;&lt;a href=&quot;#Lec8&quot; class=&quot;headerlink&quot; title=&quot;Lec8&quot;&gt;&lt;/a&gt;Lec8&lt;/h3&gt;&lt;p&gt;Spatial Localization and Detection&lt;/p&gt;
&lt;h4 id=&quot;Tasks&quot;&gt;&lt;a href=&quot;#Tasks&quot; class=&quot;headerlink&quot; title=&quot;Tasks&quot;&gt;&lt;/a&gt;Tasks&lt;/h4&gt;&lt;p&gt;Classification, Classificatin+Localization, Object Detection, Instance Segmentation.&lt;/p&gt;
&lt;p&gt;Localization, I:Image O:Box in the image(x,y,w,h) E:Intersection over Union.(IoU)&lt;/p&gt;
&lt;h4 id=&quot;Claissification-Localization&quot;&gt;&lt;a href=&quot;#Claissification-Localization&quot; class=&quot;headerlink&quot; title=&quot;Claissification+Localization&quot;&gt;&lt;/a&gt;Claissification+Localization&lt;/h4&gt;&lt;p&gt;Output: Single Label and Bounding Box&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Idea one&lt;/strong&gt;: Two task head&lt;br&gt;1.Train a CNN&lt;br&gt;2.Attach new fully-connected “regression head” to the network(FC)&lt;br&gt;  2.1 Classification Head&lt;br&gt;  2.2 Regression Head&lt;br&gt;3.Train the regression head only with SGD and L2 loss&lt;br&gt;&lt;a href=&quot;http://4.At&quot; class=&quot;test test-url&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;4.At&lt;/a&gt; test time use both heads&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Idea two&lt;/strong&gt;: Sliding Window&lt;br&gt;Input: Bounding Box&lt;br&gt;Iteratively refine the BB into a optimal size and place.&lt;/p&gt;
&lt;h4 id=&quot;Objects-Detection&quot;&gt;&lt;a href=&quot;#Objects-Detection&quot; class=&quot;headerlink&quot; title=&quot;Objects Detection&quot;&gt;&lt;/a&gt;Objects Detection&lt;/h4&gt;&lt;p&gt;Output: all the exist labels and BBs&lt;/p&gt;
&lt;p&gt;Problem: Need to test many positions and scales, use computationally demanding classifier&lt;/p&gt;
&lt;p&gt;Solution: Only look at a tiny subset of possible positions&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Region Proposal&lt;/strong&gt;:Bottom-up segmentation.&lt;/p&gt;
&lt;p&gt;RCNN: 1. Train a classification model on ImageNet&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Fine-tune model for detection(Throw finaly FC rathter than 20 Objects and one background, that is 4096*21 for the last layer.)&lt;/li&gt;
&lt;li&gt;Extract Features: Extract region proposals for all images, save the pool5 features to disk.&lt;/li&gt;
&lt;li&gt;Train one binary SVM per class to claissify region features.&lt;/li&gt;
&lt;li&gt;Bbox regression: Train a linear model to fine-grain the bbox&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Fast-RCNN: Build an end-to-end framework, much faster than RCNN.&lt;/p&gt;
&lt;h4 id=&quot;Summary&quot;&gt;&lt;a href=&quot;#Summary&quot; class=&quot;headerlink&quot; title=&quot;Summary&quot;&gt;&lt;/a&gt;Summary&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/0621classification.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;Lec9-Understanding-and-Visualizing&quot;&gt;&lt;a href=&quot;#Lec9-Understanding-and-Visualizing&quot; class=&quot;headerlink&quot; title=&quot;Lec9 Understanding and Visualizing&quot;&gt;&lt;/a&gt;Lec9 Understanding and Visualizing&lt;/h3&gt;&lt;p&gt;Visualizing the weights, t-SNE visualization&lt;/p&gt;
&lt;p&gt;Deconv Approaches:&lt;br&gt;1.Feed image into net&lt;br&gt;2.pick a layer, set gradients of the score vector to [0 0 1 .. 0], then bp to image&lt;br&gt;&lt;a href=&quot;http://3.Do&quot; class=&quot;test test-url&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;3.Do&lt;/a&gt; a small “Image Update”&lt;br&gt;4.Forward the Image&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Go to step 2&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/0621deconv.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;Deconv&quot;&gt;&lt;a href=&quot;#Deconv&quot; class=&quot;headerlink&quot; title=&quot;Deconv&quot;&gt;&lt;/a&gt;Deconv&lt;/h4&gt;&lt;p&gt;Learn to visualize the weights, also deconv to reconstruct an larger size output.&lt;/p&gt;
&lt;p&gt;Deconv: reverse the convolution filter&lt;br&gt;DePool: record the position and set other be zero.&lt;br&gt;DeReLU: The same as the ReLU.&lt;/p&gt;
&lt;h4 id=&quot;Neural-Style&quot;&gt;&lt;a href=&quot;#Neural-Style&quot; class=&quot;headerlink&quot; title=&quot;Neural Style&quot;&gt;&lt;/a&gt;Neural Style&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;extract content targets&lt;/li&gt;
&lt;li&gt;extract style targets&lt;/li&gt;
&lt;li&gt;Optimize over image &lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&quot;BackPropping-is-powerful&quot;&gt;&lt;a href=&quot;#BackPropping-is-powerful&quot; class=&quot;headerlink&quot; title=&quot;BackPropping is powerful&quot;&gt;&lt;/a&gt;BackPropping is powerful&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Understanding&lt;/li&gt;
&lt;li&gt;Segmenting Objects in the Image&lt;/li&gt;
&lt;li&gt;Inverting codes and introducing privacy concerns&lt;/li&gt;
&lt;li&gt;Fun(NeuralStyle/DeepDream)&lt;/li&gt;
&lt;li&gt;Confusion and chaos(Adversarial Examples)&lt;/li&gt;
&lt;/ul&gt;
</content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Lec8&quot;&gt;&lt;a href=&quot;#Lec8&quot; class=&quot;headerlink&quot; title=&quot;Lec8&quot;&gt;&lt;/a&gt;Lec8&lt;/h3&gt;&lt;p&gt;Spatial Localization and Detection&lt;/p&gt;
&lt;h4 id=&quot;Tasks&quot;&gt;&lt;a href=
    
    </summary>
    
      <category term="Read" scheme="https://csrjtan.github.io/categories/Read/"/>
    
    
      <category term="CNN 公开课" scheme="https://csrjtan.github.io/tags/CNN-%E5%85%AC%E5%BC%80%E8%AF%BE/"/>
    
  </entry>
  
  <entry>
    <title>Bilateral_Guided_Filter</title>
    <link href="https://csrjtan.github.io/2016/06/13/Bilateral-Guided-Filter/"/>
    <id>https://csrjtan.github.io/2016/06/13/Bilateral-Guided-Filter/</id>
    <published>2016-06-13T02:24:43.000Z</published>
    <updated>2016-06-13T04:09:51.000Z</updated>
    
    <content type="html">&lt;h4 id=&quot;Bilateral-Filter&quot;&gt;&lt;a href=&quot;#Bilateral-Filter&quot; class=&quot;headerlink&quot; title=&quot;Bilateral Filter&quot;&gt;&lt;/a&gt;Bilateral Filter&lt;/h4&gt;&lt;p&gt;双边滤波主要考虑了邻域里的像素加权，权重受几何距离和色彩距离相关。&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/0613bilateral.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;双边滤波具有Non-linear, edge-preseving and noise-reducing smoothing的特性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/0613bi1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;Guided-Filter&quot;&gt;&lt;a href=&quot;#Guided-Filter&quot; class=&quot;headerlink&quot; title=&quot;Guided Filter&quot;&gt;&lt;/a&gt;Guided Filter&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/0613guided.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;加入了引导图像的抽象，可以不一定沿着自身而做bilateral filter的。但这里guided filter还是用自身的保持边缘滤波来先理解。&lt;/p&gt;
&lt;p&gt;与bilateral的区别在于不是简单地利用spatial和range,而是建立一个局部的线性关系，在当前更新pixel的局部窗口k中，建立一个局部线性关系。&lt;br&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/0613linear.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;然后根据线性回归，对每个像素求解出其对应的参数$a_k,b_k$,再分“High variance”和”Flat patch”进行处理。&lt;br&gt;如果a-&amp;gt;1,b-&amp;gt;0:High Variance,保持值不变&lt;br&gt;如果a-&amp;gt;0,b-&amp;gt;$\mu_k$:Flat patch,使用临近像素平均&lt;/p&gt;
&lt;h4 id=&quot;具体数学分析&quot;&gt;&lt;a href=&quot;#具体数学分析&quot; class=&quot;headerlink&quot; title=&quot;具体数学分析&quot;&gt;&lt;/a&gt;具体数学分析&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/0613gui1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/0613gui2.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;保留了Bilateral filter的优点，同时克服了缺点，使得平滑之余达到边沿保留。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;Bilateral-Filter&quot;&gt;&lt;a href=&quot;#Bilateral-Filter&quot; class=&quot;headerlink&quot; title=&quot;Bilateral Filter&quot;&gt;&lt;/a&gt;Bilateral Filter&lt;/h4&gt;&lt;p&gt;双边滤波主要考虑了邻域里的像
    
    </summary>
    
      <category term="Tech" scheme="https://csrjtan.github.io/categories/Tech/"/>
    
    
  </entry>
  
  <entry>
    <title>时间简史</title>
    <link href="https://csrjtan.github.io/2016/06/12/%E6%97%B6%E9%97%B4%E7%AE%80%E5%8F%B2/"/>
    <id>https://csrjtan.github.io/2016/06/12/时间简史/</id>
    <published>2016-06-12T03:23:17.000Z</published>
    <updated>2016-06-12T03:45:43.000Z</updated>
    
    <content type="html">&lt;h4 id=&quot;简述&quot;&gt;&lt;a href=&quot;#简述&quot; class=&quot;headerlink&quot; title=&quot;简述&quot;&gt;&lt;/a&gt;简述&lt;/h4&gt;&lt;p&gt;这是霍金在2010出版的书，讲述近代纯物理学的一些研究成果以及围绕宇宙学来展开的一系列科普理论。&lt;br&gt;这个评价很高也让我了解到物理学是怎么去做research的，然而无奈自己太多理论没有接触过，也没有上过《大学物理》这个课程，中间太多断层的知识唯有靠瞎蒙，无奈第一次十分不理解地看完了。想做点笔记，也只能围绕书本的结构顺序展开基本的描述，也参考了一下豆瓣上的笔记之类的，在这里总结一下，毕竟这个书前前后后读了2周，依然云里雾里。庆幸的是我对为何宇宙膨胀，黑洞、虫洞、空间曲率等有了更深的理解。因为我很喜欢关于宇宙题材的电影，包括《火星救援》、《星际穿越》、《星际迷航》等等。还是很值得一看，这是个重要的科学问题，也是一个哲学问题。&lt;/p&gt;
&lt;p&gt;最后，大部分的朋友也是认为此书看不懂是正常的，甚至连物理系的学生也可能不能完全读通，主要是一来物理学未成大体系，二来目前研究还很粗糙，霍金用个人的理解和总结来描述，总不免有未能顾及大众科普，三来对基本理论的掌握要求比较高，不然无法延伸出来这些观点。&lt;/p&gt;
&lt;h4 id=&quot;文章结构&quot;&gt;&lt;a href=&quot;#文章结构&quot; class=&quot;headerlink&quot; title=&quot;文章结构&quot;&gt;&lt;/a&gt;文章结构&lt;/h4&gt;&lt;p&gt;这里把看书的笔记列一下，比较杂乱:&lt;br&gt;人类发展以来的宇宙观和世界观：地方说-&amp;gt;地圆说（上帝说）-&amp;gt;日心说（哥白尼）-&amp;gt;椭圆轨道-&amp;gt;万有引力（牛顿）-&amp;gt;狭义相对论-&amp;gt;广义相对论-&amp;gt;宇宙大爆炸&lt;/p&gt;
&lt;p&gt;组成成分的认识： 四元素（亚里士多德）-&amp;gt;原子论（道尔）-&amp;gt;布朗运动-&amp;gt;电子（卢瑟福）-&amp;gt;中子（查德威克）-&amp;gt;夸克（加州理工1969）&lt;/p&gt;
&lt;p&gt;四种力：万有引力、电磁力、弱核力、强作用力&lt;/p&gt;
&lt;p&gt;量子物理（微观）：量子假设、不确定性原理、薛定谔的猫&lt;br&gt;热力学熵增（有序到无序）：热力学箭头，心理学箭头，宇宙学箭头&lt;br&gt;黑洞的“无毛定力”，称为“不能逃逸远处的时间集合”、弱人择原理、不完备定理、虫洞（空间扭曲）、宇宙弦（张力大的橡筋），光速C为常量且最快，粒子无法提速到99.99%*c,无论功率如何加大。还有光锥、高维空间、PST对称原理等。&lt;/p&gt;
&lt;h4 id=&quot;借鉴总结&quot;&gt;&lt;a href=&quot;#借鉴总结&quot; class=&quot;headerlink&quot; title=&quot;借鉴总结&quot;&gt;&lt;/a&gt;借鉴总结&lt;/h4&gt;&lt;p&gt;【引用自豆瓣 川贝】&lt;/p&gt;
&lt;p&gt;一、物质（Substance）&lt;br&gt;物质即一种存在。物质由一些基本粒子（自旋为1/2）构成。物质的绝对静止是不存在的，物质的绝对状态是不停运动变化的。质量和能量是描述物质状态的两个重要属性，两者皆满足广泛意义上的守恒定律，且可以互相转化。力（自旋为0，1，2的虚粒子）和波（自旋为0，1，2的实粒子）描述了物质间的相互作用及效果，自然界归纳出四种基本的力：引力、电磁力、强核力、弱核力。&lt;/p&gt;
&lt;p&gt;二、时空（Time&amp;amp;Space）&lt;br&gt;当我们跳出低维度的视角去思考这个世界，时间和空间是一个混合的概念，时空的本质是物质的散漫态，而时间只是物质状态变迁的一种度量。时间箭头是基于热力学方向（闭合系统中的熵总是随时间增加）、心理学方向（取决于热力学方向）、宇宙学方向（方向不定，但根据人择原理，现在它与前两个方向一致）的。关于广义相对论和时空曲率的一些论证及推论让我们看到通过旋转黑洞、虫洞进行时空旅行的可能性。&lt;/p&gt;
&lt;p&gt;三、科学（Science）&lt;br&gt;从最初亚里士多德的权威说法到牛顿的经典理论再到爱因斯坦的相对论和近代物理学的量子论，科学理论的提出、完善、应用乃至推翻，每每令人惊叹随即错愕。让我来回想一下部分基本理论及关键字：&lt;br&gt;1、电磁场理论：四个方程，似乎没什么好说的。做大一统工作的人总能博得满堂彩，向麦克斯韦致敬。也正是这个理论方程关于伽利略变换的不谐洽导致了洛伦兹变换和相对论的提出。&lt;br&gt;2、相对论：狭义相对论凭借两条简洁的假设，开拓了一个崭新的时空观，重新探讨了惯性系中物体的运动规律，由此衍生出来的一系列推论和预言近乎完美的解释并验证了诸多难题，继而广义相对论又对非惯性系中的物体运动规律做了进一步研究，赋予了引力场和惯性等物理概念以新的科学内涵，有力地推动了天文宇宙物理学的快速发展。&lt;br&gt;3、量子理论：光电效应、波谱研究、康普顿效应、德布罗意波-&amp;gt;量子化（从普朗克到波尔）！海森堡不确定性原理（俗称测不准原理）放弃了量子状态的精确测量、泡利不相容原理－&amp;gt;概率波、薛定谔方程，还有一大堆基本粒子及其量子状态描述（复杂从略），量子物理学作为研究物质微观机理的近代理论还在不断的完善中……&lt;br&gt;4、近代宇宙学：宇宙的起源及演化（热大爆炸学说，弗里德曼闭合宇宙，宇宙无边界设想）、黑洞理论（坍塌，高密度，大引力）、虫洞(时空负曲率)、人择原理（最莫名其妙的原理：“不要问为什么，因为这就是答案”）、暗物质（反粒子）、弦理论、……关于宇宙的理论与设想自古以来就没少过。&lt;br&gt;5、理论的统一：广义相对论用以描述宇宙的大尺度结构（几公里到1亿亿亿英里），量子力学用以处理极小尺度现象（百万亿分之一米），然而，可惜的是这两个理论不是互相协调的——它们不可能都对！现在科学家们正在思索并探寻一种被称为“量子引力论”的统一理论。自然界中的四种基本力中除引力外的其他三种力的统一已经在GUT（大统一理论）中初见端倪，然而离最终得到包含包括引力在内的所有力和普适物理规律的统一理论还有相当长的一段路要走。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;简述&quot;&gt;&lt;a href=&quot;#简述&quot; class=&quot;headerlink&quot; title=&quot;简述&quot;&gt;&lt;/a&gt;简述&lt;/h4&gt;&lt;p&gt;这是霍金在2010出版的书，讲述近代纯物理学的一些研究成果以及围绕宇宙学来展开的一系列科普理论。&lt;br&gt;这个评价很高也让我了解到物理学是怎么
    
    </summary>
    
      <category term="Tech" scheme="https://csrjtan.github.io/categories/Tech/"/>
    
    
      <category term="Read" scheme="https://csrjtan.github.io/tags/Read/"/>
    
  </entry>
  
  <entry>
    <title>CS231n-7</title>
    <link href="https://csrjtan.github.io/2016/06/09/CS231n-7/"/>
    <id>https://csrjtan.github.io/2016/06/09/CS231n-7/</id>
    <published>2016-06-09T06:33:36.000Z</published>
    <updated>2016-06-09T13:33:20.000Z</updated>
    
    <content type="html">&lt;h4 id=&quot;CNN&quot;&gt;&lt;a href=&quot;#CNN&quot; class=&quot;headerlink&quot; title=&quot;CNN&quot;&gt;&lt;/a&gt;CNN&lt;/h4&gt;&lt;p&gt;终于进入CNN的话题了，介绍一下CONV,POOL,FC层的做法，具体的结构、参数、运算量等。&lt;/p&gt;
&lt;p&gt;回顾一下，Mini-batch SGD&lt;br&gt;Loop: 1. &lt;strong&gt;Sample&lt;/strong&gt; a batch of data&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Forward&lt;/strong&gt; prop it through the graph, get loss&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Backprop&lt;/strong&gt; to calculate the gradients&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Update&lt;/strong&gt; the parameters using the gradient&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/0609CNN.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;卷积操作过程的参数计算公式图，趋向于用数量更多的小Filter,更深的网络。&lt;br&gt;ConV的卷积核深度总是和输入的立方Feature Map的深度一致，而Kernel的个数就是新的FeatureMap的Depth.&lt;/p&gt;
&lt;p&gt;一般来说：Max Pool with 2*2 filters and stride 2&lt;/p&gt;
&lt;p&gt;FC： Containes Neurons connect to the entire input volume&lt;/p&gt;
&lt;p&gt;ConV的参数取决于Filter,例如227&lt;em&gt;227&lt;/em&gt;3通道的Feature Map,用Stride为4的96个Kernel 11&lt;em&gt;11 Filters,则有(11&lt;/em&gt;11&lt;em&gt;3)&lt;/em&gt;96=35K， Output Volum [((227-11)/4+1)&lt;em&gt;55&lt;/em&gt;96]&lt;/p&gt;
&lt;p&gt;Pool的参数为0，FC的参数最多.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/0609ALEX.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;2012的ALEXNET的网络架构&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/0609VGG.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;2014的VGGNET网络架构和参数，主要占显存的是头几层的FeatureMap,而主要占用参数是FC层，VGG是初始化效果最佳的网络之一。&lt;br&gt;TOTAL MEMORY: 24M&lt;em&gt;4bytes ~= 93MB/image (Only forward!~&lt;/em&gt;2 for bwd)&lt;br&gt;TOTAL params: 138M parameters&lt;/p&gt;
&lt;p&gt;之后是GoogleNET(2014),6.7% for top5 error,12X less params than ALEXNET&lt;br&gt;然后是MSRA的RESNET（2015）， 3.6% top5 error, at runtime: faster than VGGNet&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xl4js.com1.z0.glb.clouddn.com/0609RES.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;特点：BN after every CONV, Xavier/2 for initialization, SGD+Momentum(0.9), Learning rate,0.1 and dived by 10 when validation error plateaus. Mini-batch size 256, Weight decay of 1e-5, No dropout&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;CNN&quot;&gt;&lt;a href=&quot;#CNN&quot; class=&quot;headerlink&quot; title=&quot;CNN&quot;&gt;&lt;/a&gt;CNN&lt;/h4&gt;&lt;p&gt;终于进入CNN的话题了，介绍一下CONV,POOL,FC层的做法，具体的结构、参数、运算量等。&lt;/p&gt;
&lt;p&gt;回顾一下，Mini-b
    
    </summary>
    
      <category term="Read" scheme="https://csrjtan.github.io/categories/Read/"/>
    
    
      <category term="公开课 CNN" scheme="https://csrjtan.github.io/tags/%E5%85%AC%E5%BC%80%E8%AF%BE-CNN/"/>
    
  </entry>
  
  <entry>
    <title>CS231n_6</title>
    <link href="https://csrjtan.github.io/2016/06/09/CS231n-6/"/>
    <id>https://csrjtan.github.io/2016/06/09/CS231n-6/</id>
    <published>2016-06-09T04:34:02.000Z</published>
    <updated>2016-06-09T06:33:55.000Z</updated>
    
    <content type="html">&lt;p&gt;开始新一课之前，先来把上一节的相关阅读材料的知识补充上来。&lt;/p&gt;
&lt;h3 id=&quot;Lecture-5-Notes2-amp-3&quot;&gt;&lt;a href=&quot;#Lecture-5-Notes2-amp-3&quot; class=&quot;headerlink&quot; title=&quot;Lecture 5 Notes2&amp;amp;3&quot;&gt;&lt;/a&gt;Lecture 5 Notes2&amp;amp;3&lt;/h3&gt;&lt;h4 id=&quot;Regularization&quot;&gt;&lt;a href=&quot;#Regularization&quot; class=&quot;headerlink&quot; title=&quot;Regularization&quot;&gt;&lt;/a&gt;Regularization&lt;/h4&gt;&lt;p&gt;L1,L2的Loss function 还有Max Norm constraints: 对于系数向量w,有$w^2 &amp;lt; c$ ，C一般为3或4.&lt;br&gt;Dropout的技术：一般采用P=0.5,每个神经元的激活概率为0.5，然后每个样本对应一个新的Mask之后的子网络进行训练，最后测试的时候开启全部神经元但得到的结果需要乘上P=0.5这个系数。这种技术直观的好处是：1.迫使网络学习冗余的表达能力  2.实现了大型的学习模型，具有共享参数特性 &lt;/p&gt;
&lt;p&gt;Practice: 使用single,global L2 Regularization(cross-validated)+ Dropout(p=0.5)&lt;/p&gt;
&lt;h4 id=&quot;Loss-Functions&quot;&gt;&lt;a href=&quot;#Loss-Functions&quot; class=&quot;headerlink&quot; title=&quot;Loss Functions&quot;&gt;&lt;/a&gt;Loss Functions&lt;/h4&gt;&lt;p&gt;针对classification的任务，使用Softmax或者SVM Loss. 针对类别多的情况，可以使用Hierarchical Softmax. &lt;/p&gt;
&lt;p&gt;Attribute Classification可以用Logistic regression classifier with two classes(0,1)&lt;/p&gt;
&lt;p&gt;Regression任务，一般使用L2或者L1。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;br&gt;When faced with a regression task, first consider if it is absolutely necessary. Instead, have a strong preference to discretizing your outputs to bins and perform classification over them whenever possible.
&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;Gradient-Checks&quot;&gt;&lt;a href=&quot;#Gradient-Checks&quot; class=&quot;headerlink&quot; title=&quot;Gradient Checks&quot;&gt;&lt;/a&gt;Gradient Checks&lt;/h4&gt;&lt;p&gt;Use centered formula,求梯度用左右方向的平均。$$\frac{df(x)}{dx} = \frac{f(x+h)-f(x-h)}{2h}$$&lt;/p&gt;
&lt;p&gt;Use relative error for the comparison, relative error&lt;br&gt;$$ \frac{ |f_a - f_n | }{max(f_a,f_n)} $$  . $f_a$ 为analytic gradient  , $f_n$ 为numberic gradient &lt;/p&gt;
&lt;p&gt;In practice:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;relative error &amp;gt; 1e-2 usually means the gradient is probably wrong&lt;/li&gt;
&lt;li&gt;1e-2&amp;gt;relative error&amp;gt;1e-4 should make you feel uncomfortable&lt;/li&gt;
&lt;li&gt;1e-4&amp;gt;relative error is usually okay for objectives with kinks, but if there are no kinks(such tanh nonlinearities and softmax) , then 1e-4 is too high.&lt;/li&gt;
&lt;li&gt;1e-7 and less you should be happy&lt;/li&gt;
&lt;li&gt;if too small like or than 1e-10, absolute value is worrying&lt;br&gt;越深的网络，relative errors越大，如果10层，则1e-2是可以接受的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Summary: Careful with step size h, Gradcheck important, Don’t let regularization overwhelm the data, turn off the dropout/augmentations when gradient check, check only few dimensions.&lt;/p&gt;
&lt;h3 id=&quot;Lecture-6&quot;&gt;&lt;a href=&quot;#Lecture-6&quot; class=&quot;headerlink&quot; title=&quot;Lecture 6&quot;&gt;&lt;/a&gt;Lecture 6&lt;/h3&gt;&lt;h4 id=&quot;How-to-do-parameter-Updates&quot;&gt;&lt;a href=&quot;#How-to-do-parameter-Updates&quot; class=&quot;headerlink&quot; title=&quot;How to do parameter Updates&quot;&gt;&lt;/a&gt;How to do parameter Updates&lt;/h4&gt;&lt;p&gt;1.SGD: $x+= -learning rate *dx$&lt;br&gt;2.Momentum:  allow velocity build up, velocity damped in steep due to changing sign&lt;/p&gt;
&lt;p&gt;v = mu &lt;em&gt; v - learning_rate &lt;/em&gt; dx&lt;br&gt;x += v&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Nestreov Momentum:&lt;br&gt;$v_t = \mu v_{t-1} - \epsilon \Delta f(\theta_{t-1} + \mu V_{t-1})$&lt;br&gt;$\theta_t = \theta_{t-1} + V_t$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;4.Adagrad: Equalization the steep and shallow direction&lt;br&gt;cache + = dx &lt;em&gt;*2&lt;br&gt;x += - learn_rate &lt;/em&gt; dx / (sqrt(cache)+1e-7)&lt;/p&gt;
&lt;p&gt;5.Adam: Great enough with bias correct&lt;br&gt;m = beta1 &lt;em&gt; m + (1-beta1)&lt;/em&gt;dx&lt;br&gt;v = beta2 &lt;em&gt; v + (1-beta2)&lt;/em&gt;(dx &lt;em&gt;*2)&lt;br&gt;x += -learn_rate &lt;/em&gt; m/(sqrt(v)+1e-7)&lt;br&gt;一般beta1和beta2可以设置为0.9和0.995&lt;/p&gt;
&lt;h4 id=&quot;Second-Order-Optimization&quot;&gt;&lt;a href=&quot;#Second-Order-Optimization&quot; class=&quot;headerlink&quot; title=&quot;Second Order Optimization&quot;&gt;&lt;/a&gt;Second Order Optimization&lt;/h4&gt;&lt;p&gt;1.泰勒展开&lt;br&gt;2.Newton Gradient: Jacobian H is too large with O(n^3),n is million&lt;br&gt;3.Quasi-Newton O(n^2)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;L-BFGS(Limited Memory BFGS):work well in full patch,but can not transfoer well to mini-batch search.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Practice: 1.Train Multiple Indeoendent model&lt;br&gt;&lt;a href=&quot;http://2.At&quot; class=&quot;test test-url&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;2.At&lt;/a&gt; test time average the results &lt;/p&gt;
&lt;p&gt;Fun tricks: get small boost from average multiple initilization model, keep track running average parametre vector.&lt;/p&gt;
&lt;h4 id=&quot;Annealing-learning-rate&quot;&gt;&lt;a href=&quot;#Annealing-learning-rate&quot; class=&quot;headerlink&quot; title=&quot;Annealing learning rate&quot;&gt;&lt;/a&gt;Annealing learning rate&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;step dcay: learning rate by half every t epochs&lt;/li&gt;
&lt;li&gt;Exponential decay $\alpha = \alpha_0 e^{-kt}$&lt;/li&gt;
&lt;li&gt;1/t decay: $\alpha = \alpha_0 /(1+kt)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;Hyperparameter-optimzation&quot;&gt;&lt;a href=&quot;#Hyperparameter-optimzation&quot; class=&quot;headerlink&quot; title=&quot;Hyperparameter optimzation&quot;&gt;&lt;/a&gt;Hyperparameter optimzation&lt;/h4&gt;&lt;p&gt;stage search from coarse to fine&lt;br&gt;bayesian hyperparameter optimization&lt;br&gt;Model ensemble, improve the performance of NN a few percent:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Same Model, Different Initializations&lt;/li&gt;
&lt;li&gt;Top models discovered during cross-validation&lt;/li&gt;
&lt;li&gt;Different Checkpoints of a single model: Training is very expensive, taking the different checkpoints of single network and using those to form an ensemble.(Cheap and practice，选取一些好的epoch模型)&lt;/li&gt;
&lt;li&gt;Running averatge of parameters during training(用训练过程模型中的均值，直观来看在碗状徘徊，均值更有利于接近底部)&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&quot;Summary&quot;&gt;&lt;a href=&quot;#Summary&quot; class=&quot;headerlink&quot; title=&quot;Summary&quot;&gt;&lt;/a&gt;Summary&lt;/h4&gt;&lt;p&gt;1.针对少量的样本，gradient check很重要，并注意正确的初始化&lt;br&gt;2.the magnitude of updates should be ~1e-3 in first-layer&lt;br&gt;3.推荐用SGD+Nesterov Momentum or Adam&lt;br&gt;4.Decay learning rate over the period of training&lt;br&gt;5.Search good hyperparameters with random search(not grid), stage coarse to fine&lt;br&gt;6.Form model ensembles for extra performance&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;开始新一课之前，先来把上一节的相关阅读材料的知识补充上来。&lt;/p&gt;
&lt;h3 id=&quot;Lecture-5-Notes2-amp-3&quot;&gt;&lt;a href=&quot;#Lecture-5-Notes2-amp-3&quot; class=&quot;headerlink&quot; title=&quot;Lecture 5 N
    
    </summary>
    
      <category term="Read" scheme="https://csrjtan.github.io/categories/Read/"/>
    
    
      <category term="公开课 CNN" scheme="https://csrjtan.github.io/tags/%E5%85%AC%E5%BC%80%E8%AF%BE-CNN/"/>
    
  </entry>
  
</feed>
